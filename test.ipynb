{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd69fa21",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Download & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac1a4b",
   "metadata": {},
   "source": [
    "## 1. Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ccxt pandas numpy pyarrow ta-lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c6e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show Ta-Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b019e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime, timedelta\n",
    "import talib as ta\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Default parameters\n",
    "SYMBOL = 'BTCUSDT'\n",
    "START_DATE = '2023-01-01'\n",
    "END_DATE = '2024-12-31'\n",
    "TIMEFRAMES = ['1m', '5m']\n",
    "DATA_DIR = Path('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5094e2",
   "metadata": {},
   "source": [
    "## 2. Setup Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(DATA_DIR / 'raw' / SYMBOL).mkdir(parents=True, exist_ok=True)\n",
    "(DATA_DIR / 'processed' / SYMBOL).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Directory structure created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74861c",
   "metadata": {},
   "source": [
    "## 3. Download Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_binance_data(symbol, timeframe, start_date, end_date):\n",
    "    \"\"\"Download OHLCV data from Binance\"\"\"\n",
    "    exchange = ccxt.binance({\n",
    "        'rateLimit': 1200,\n",
    "        'enableRateLimit': True,\n",
    "    })\n",
    "    \n",
    "    # Convert dates to timestamps\n",
    "    start_ts = exchange.parse8601(f'{start_date}T00:00:00Z')\n",
    "    end_ts = exchange.parse8601(f'{end_date}T23:59:59Z')\n",
    "    \n",
    "    all_ohlcv = []\n",
    "    since = start_ts\n",
    "    \n",
    "    print(f\"Downloading {symbol} {timeframe} from {start_date} to {end_date}...\")\n",
    "    \n",
    "    while since < end_ts:\n",
    "        try:\n",
    "            ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since, 1500)\n",
    "            \n",
    "            if not ohlcv:\n",
    "                break\n",
    "                \n",
    "            all_ohlcv.extend(ohlcv)\n",
    "            since = ohlcv[-1][0] + 1  # Next timestamp\n",
    "            \n",
    "            print(f\"Downloaded {len(all_ohlcv)} candles...\", end='\\r')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df = df.drop_duplicates(subset=['timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nCompleted: {len(df)} candles downloaded\")\n",
    "    return df\n",
    "\n",
    "# Download 1m and 5m data\n",
    "data_1m = download_binance_data(SYMBOL, '1m', START_DATE, END_DATE)\n",
    "data_5m = download_binance_data(SYMBOL, '5m', START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19414c63",
   "metadata": {},
   "source": [
    "## 4. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9e9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(df, timeframe):\n",
    "    \"\"\"Check for missing data and gaps\"\"\"\n",
    "    print(f\"\\n=== {timeframe} Data Quality Check ===\")\n",
    "    print(f\"Total records: {len(df)}\")\n",
    "    print(f\"Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    print(f\"Missing values:\\n{missing[missing > 0]}\")\n",
    "    \n",
    "    # Check for time gaps (example for 1m should be 60 seconds)\n",
    "    if timeframe == '1m':\n",
    "        expected_diff = 60000  # 1 minute in ms\n",
    "    elif timeframe == '5m':\n",
    "        expected_diff = 300000  # 5 minutes in ms\n",
    "    \n",
    "    df['time_diff'] = df['timestamp'].diff()\n",
    "    gaps = df[df['time_diff'] > expected_diff * 1.5]  # Allow 50% tolerance\n",
    "    \n",
    "    print(f\"Time gaps found: {len(gaps)}\")\n",
    "    if len(gaps) > 0:\n",
    "        print(\"Gap details:\")\n",
    "        print(gaps[['datetime', 'time_diff']].head())\n",
    "    \n",
    "    return len(gaps) == 0\n",
    "\n",
    "# Check data quality\n",
    "quality_1m = check_data_quality(data_1m, '1m')\n",
    "quality_5m = check_data_quality(data_5m, '5m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56de359",
   "metadata": {},
   "source": [
    "## 5. Add Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(df):\n",
    "    \"\"\"Add basic technical indicators\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert to numpy arrays for talib\n",
    "    close = df['close'].values.astype(float)\n",
    "    high = df['high'].values.astype(float)\n",
    "    low = df['low'].values.astype(float)\n",
    "    volume = df['volume'].values.astype(float)\n",
    "    \n",
    "    # Price indicators\n",
    "    df['sma_20'] = ta.SMA(close, timeperiod=20)\n",
    "    df['ema_12'] = ta.EMA(close, timeperiod=12)\n",
    "    df['ema_26'] = ta.EMA(close, timeperiod=26)\n",
    "    \n",
    "    # Momentum indicators\n",
    "    df['rsi'] = ta.RSI(close, timeperiod=14)\n",
    "    macd, macd_signal, macd_hist = ta.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    df['macd'] = macd\n",
    "    df['macd_signal'] = macd_signal\n",
    "    \n",
    "    # Volatility indicators\n",
    "    bb_upper, bb_middle, bb_lower = ta.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    df['bb_upper'] = bb_upper\n",
    "    df['bb_lower'] = bb_lower\n",
    "    df['bb_width'] = (bb_upper - bb_lower) / close\n",
    "    \n",
    "    # Volume indicators (simple moving average of volume)\n",
    "    df['volume_sma'] = ta.SMA(volume, timeperiod=20)\n",
    "    \n",
    "    # Price change\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['price_change'] = df['close'].diff()\n",
    "    \n",
    "    print(f\"Added technical indicators. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Add indicators to both timeframes\n",
    "data_1m_processed = add_technical_indicators(data_1m)\n",
    "data_5m_processed = add_technical_indicators(data_5m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256396e4",
   "metadata": {},
   "source": [
    "## 6. Save to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce04352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(df, symbol, timeframe, data_type='processed'):\n",
    "    \"\"\"Save DataFrame to Parquet format\"\"\"\n",
    "    filename = f\"{symbol.lower()}_{timeframe}_{data_type}.parquet\"\n",
    "    filepath = DATA_DIR / data_type / symbol / filename\n",
    "    \n",
    "    # Remove NaN values\n",
    "    df_clean = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # Save to parquet\n",
    "    df_clean.to_parquet(filepath, compression='snappy', index=False)\n",
    "    \n",
    "    print(f\"Saved {len(df_clean)} records to {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "# Save processed data\n",
    "file_1m = save_to_parquet(data_1m_processed, SYMBOL, '1m')\n",
    "file_5m = save_to_parquet(data_5m_processed, SYMBOL, '5m')\n",
    "\n",
    "print(f\"\\n=== Data Preparation Complete ===\")\n",
    "print(f\"1m data: {len(data_1m_processed)} records\")\n",
    "print(f\"5m data: {len(data_5m_processed)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456737e",
   "metadata": {},
   "source": [
    "## 7 Quick Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 5m Data Sample ===\")\n",
    "print(data_5m_processed[['datetime', 'open', 'high', 'low', 'close', 'volume', 'rsi', 'macd']].tail())\n",
    "\n",
    "print(\"\\n=== Data Info ===\")\n",
    "print(f\"1m file size: {file_1m.stat().st_size / (1024*1024):.2f} MB\")\n",
    "print(f\"5m file size: {file_5m.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debc6caa",
   "metadata": {},
   "source": [
    "# 2. Data Management & Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c1853",
   "metadata": {},
   "source": [
    "## 1. Import Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1c4896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Default parameters\n",
    "SYMBOL = 'BTCUSDT'\n",
    "DATA_DIR = Path('data')\n",
    "WINDOW_SIZE = 60  # 60 periods for sequence\n",
    "BATCH_SIZE = 128\n",
    "TRAIN_RATIO = 0.7\n",
    "VALIDATION_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df2e53",
   "metadata": {},
   "source": [
    "## 2. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fdab43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1m data: (1052967, 20)\n",
      "Loaded 5m data: (210967, 20)\n",
      "1m data range: 2023-01-01 00:33:00 to 2025-01-01 07:19:00\n",
      "5m data range: 2023-01-01 02:45:00 to 2025-01-02 16:35:00\n"
     ]
    }
   ],
   "source": [
    "def load_processed_data(symbol, timeframe):\n",
    "    \"\"\"Load processed parquet data\"\"\"\n",
    "    filename = f\"{symbol.lower()}_{timeframe}_processed.parquet\"\n",
    "    filepath = DATA_DIR / 'processed' / symbol / filename\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "    \n",
    "    df = pd.read_parquet(filepath)\n",
    "    print(f\"Loaded {timeframe} data: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Load both timeframes\n",
    "data_1m = load_processed_data(SYMBOL, '1m')\n",
    "data_5m = load_processed_data(SYMBOL, '5m')\n",
    "\n",
    "print(f\"1m data range: {data_1m['datetime'].min()} to {data_1m['datetime'].max()}\")\n",
    "print(f\"5m data range: {data_5m['datetime'].min()} to {data_5m['datetime'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f31c8c",
   "metadata": {},
   "source": [
    "## 3. Feature Selection & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9cf3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['open_log', 'high_log', 'low_log', 'close_log', 'volume_log', 'rsi', 'macd', 'macd_signal', 'bb_width', 'returns', 'price_change']\n",
      "Clean data shape: (210967, 13)\n"
     ]
    }
   ],
   "source": [
    "def prepare_features(df):\n",
    "    \"\"\"Select and prepare features for training\"\"\"\n",
    "    # Price features (log transform for better distribution)\n",
    "    price_features = ['open', 'high', 'low', 'close']\n",
    "    for col in price_features:\n",
    "        df[f'{col}_log'] = np.log(df[col])\n",
    "    \n",
    "    # Volume feature (log transform)\n",
    "    df['volume_log'] = np.log(df['volume'] + 1)  # +1 to avoid log(0)\n",
    "    \n",
    "    # Select final features\n",
    "    feature_columns = [\n",
    "        # Log transformed prices\n",
    "        'open_log', 'high_log', 'low_log', 'close_log', 'volume_log',\n",
    "        # Technical indicators\n",
    "        'rsi', 'macd', 'macd_signal', 'bb_width',\n",
    "        # Price changes\n",
    "        'returns', 'price_change'\n",
    "    ]\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    df_clean = df[feature_columns + ['datetime', 'close']].dropna()\n",
    "    \n",
    "    print(f\"Selected features: {feature_columns}\")\n",
    "    print(f\"Clean data shape: {df_clean.shape}\")\n",
    "    \n",
    "    return df_clean, feature_columns\n",
    "\n",
    "# Prepare features for 5m data (main timeframe)\n",
    "data_clean, feature_cols = prepare_features(data_5m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73c450",
   "metadata": {},
   "source": [
    "## 4. Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f06d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data normalization completed\n",
      "Normalized data shape: (210967, 13)\n"
     ]
    }
   ],
   "source": [
    "class DataNormalizer:\n",
    "    \"\"\"Handles data normalization and denormalization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def fit_transform(self, data, feature_columns):\n",
    "        \"\"\"Fit scalers and transform data\"\"\"\n",
    "        normalized_data = data.copy()\n",
    "        \n",
    "        for col in feature_columns:\n",
    "            scaler = StandardScaler()\n",
    "            normalized_data[col] = scaler.fit_transform(data[[col]])\n",
    "            self.scalers[col] = scaler\n",
    "            \n",
    "        return normalized_data\n",
    "    \n",
    "    def transform(self, data, feature_columns):\n",
    "        \"\"\"Transform data using fitted scalers\"\"\"\n",
    "        normalized_data = data.copy()\n",
    "        \n",
    "        for col in feature_columns:\n",
    "            if col in self.scalers:\n",
    "                normalized_data[col] = self.scalers[col].transform(data[[col]])\n",
    "                \n",
    "        return normalized_data\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save scalers to file\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self.scalers, f)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load scalers from file\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            self.scalers = pickle.load(f)\n",
    "\n",
    "# Normalize data\n",
    "normalizer = DataNormalizer()\n",
    "data_normalized = normalizer.fit_transform(data_clean, feature_cols)\n",
    "\n",
    "# Save normalizer\n",
    "normalizer_path = DATA_DIR / 'processed' / SYMBOL / 'normalizer.pkl'\n",
    "normalizer.save(normalizer_path)\n",
    "\n",
    "print(\"Data normalization completed\")\n",
    "print(f\"Normalized data shape: {data_normalized.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced4f70",
   "metadata": {},
   "source": [
    "## 5. Create Sliding Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a63ee9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sequences: X shape (210907, 60, 11), y shape (210907,)\n",
      "Input shape: (210907, 60, 11)\n",
      "Target shape: (210907,)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(data, feature_columns, window_size, target_column='close'):\n",
    "    \"\"\"Create sliding window sequences for time series\"\"\"\n",
    "    features = data[feature_columns].values\n",
    "    targets = data[target_column].values\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(window_size, len(data)):\n",
    "        # Feature sequence (window_size x num_features)\n",
    "        X.append(features[i-window_size:i])\n",
    "        # Target (next close price)\n",
    "        y.append(targets[i])\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    \n",
    "    print(f\"Created sequences: X shape {X.shape}, y shape {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "# Create sequences\n",
    "X_sequences, y_sequences = create_sequences(\n",
    "    data_normalized, feature_cols, WINDOW_SIZE, 'close'\n",
    ")\n",
    "\n",
    "print(f\"Input shape: {X_sequences.shape}\")  # (samples, window_size, features)\n",
    "print(f\"Target shape: {y_sequences.shape}\")  # (samples,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755052e1",
   "metadata": {},
   "source": [
    "## 6. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e00a6972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 147634 samples\n",
      "Validation: 31636 samples\n",
      "Test: 31637 samples\n"
     ]
    }
   ],
   "source": [
    "def split_data(X, y, train_ratio, val_ratio, test_ratio):\n",
    "    \"\"\"Split data chronologically\"\"\"\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    train_end = int(n_samples * train_ratio)\n",
    "    val_end = int(n_samples * (train_ratio + val_ratio))\n",
    "    \n",
    "    X_train = X[:train_end]\n",
    "    y_train = y[:train_end]\n",
    "    \n",
    "    X_val = X[train_end:val_end]\n",
    "    y_val = y[train_end:val_end]\n",
    "    \n",
    "    X_test = X[val_end:]\n",
    "    y_test = y[val_end:]\n",
    "    \n",
    "    print(f\"Train: {len(X_train)} samples\")\n",
    "    print(f\"Validation: {len(X_val)} samples\")\n",
    "    print(f\"Test: {len(X_test)} samples\")\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "# Split data\n",
    "train_data, val_data, test_data = split_data(\n",
    "    X_sequences, y_sequences, TRAIN_RATIO, VALIDATION_RATIO, TEST_RATIO\n",
    ")\n",
    "\n",
    "X_train, y_train = train_data\n",
    "X_val, y_val = val_data\n",
    "X_test, y_test = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f3560",
   "metadata": {},
   "source": [
    "## 7. Convert to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45c58495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch tensors created:\n",
      "Train tensors: torch.Size([147634, 60, 11]), torch.Size([147634])\n",
      "Validation tensors: torch.Size([31636, 60, 11]), torch.Size([31636])\n",
      "Test tensors: torch.Size([31637, 60, 11]), torch.Size([31637])\n"
     ]
    }
   ],
   "source": [
    "def create_torch_tensors(X, y, device):\n",
    "    \"\"\"Convert numpy arrays to PyTorch tensors\"\"\"\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32, device=device)\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor, y_train_tensor = create_torch_tensors(X_train, y_train, device)\n",
    "X_val_tensor, y_val_tensor = create_torch_tensors(X_val, y_val, device)\n",
    "X_test_tensor, y_test_tensor = create_torch_tensors(X_test, y_test, device)\n",
    "\n",
    "print(\"PyTorch tensors created:\")\n",
    "print(f\"Train tensors: {X_train_tensor.shape}, {y_train_tensor.shape}\")\n",
    "print(f\"Validation tensors: {X_val_tensor.shape}, {y_val_tensor.shape}\")\n",
    "print(f\"Test tensors: {X_test_tensor.shape}, {y_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e85f2",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2716a630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to data\\processed\\BTCUSDT\\tensor_data.pt\n",
      "File size: 531.81 MB\n",
      "\n",
      "=== Data Processing Summary ===\n",
      "Window size: 60\n",
      "Features: 11\n",
      "Total sequences: 210907\n",
      "Train/Val/Test split: 0.7/0.15/0.15\n"
     ]
    }
   ],
   "source": [
    "# Save tensor data\n",
    "processed_data = {\n",
    "    'X_train': X_train_tensor.cpu(),\n",
    "    'y_train': y_train_tensor.cpu(),\n",
    "    'X_val': X_val_tensor.cpu(),\n",
    "    'y_val': y_val_tensor.cpu(),\n",
    "    'X_test': X_test_tensor.cpu(),\n",
    "    'y_test': y_test_tensor.cpu(),\n",
    "    'feature_columns': feature_cols,\n",
    "    'data_info': {\n",
    "        'window_size': WINDOW_SIZE,\n",
    "        'n_features': len(feature_cols),\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "tensor_path = DATA_DIR / 'processed' / SYMBOL / 'tensor_data.pt'\n",
    "torch.save(processed_data, tensor_path)\n",
    "\n",
    "print(f\"Processed data saved to {tensor_path}\")\n",
    "print(f\"File size: {tensor_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n=== Data Processing Summary ===\")\n",
    "print(f\"Window size: {WINDOW_SIZE}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Total sequences: {len(X_sequences)}\")\n",
    "print(f\"Train/Val/Test split: {TRAIN_RATIO}/{VALIDATION_RATIO}/{TEST_RATIO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0404d",
   "metadata": {},
   "source": [
    "# 3. Trading Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319eef26",
   "metadata": {},
   "source": [
    "## 3.1. Import Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9f06a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trading Environment Setup - Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from enum import IntEnum\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# Default trading parameters\n",
    "SYMBOL = 'BTCUSDT'\n",
    "INITIAL_BALANCE = 100  # $100\n",
    "LEVERAGE = 3.0\n",
    "FEE_RATE = 0.0025  # 0.25%\n",
    "STOP_LOSS = 0.05   # 5%\n",
    "TAKE_PROFIT = 0.005  # 0.5%\n",
    "LIQUIDATION_THRESHOLD = 0.8  # 80% of margin\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Trading Environment Setup - Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e07b2",
   "metadata": {},
   "source": [
    "## 3.2. Trading Actions Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fad6ec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trading actions defined:\n",
      "NONE: 0\n",
      "LONG: 1\n",
      "SHORT: 2\n",
      "EXIT: 3\n"
     ]
    }
   ],
   "source": [
    "class TradingAction(IntEnum):\n",
    "    \"\"\"Trading actions enum\"\"\"\n",
    "    NONE = 0   # Do nothing\n",
    "    LONG = 1   # Open long position\n",
    "    SHORT = 2  # Open short position\n",
    "    EXIT = 3   # Close position\n",
    "\n",
    "class PositionType(IntEnum):\n",
    "    \"\"\"Position types enum\"\"\"\n",
    "    FLAT = 0   # No position\n",
    "    LONG = 1   # Long position\n",
    "    SHORT = 2  # Short position\n",
    "\n",
    "print(\"Trading actions defined:\")\n",
    "print(f\"NONE: {TradingAction.NONE}\")\n",
    "print(f\"LONG: {TradingAction.LONG}\")\n",
    "print(f\"SHORT: {TradingAction.SHORT}\")\n",
    "print(f\"EXIT: {TradingAction.EXIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b972107",
   "metadata": {},
   "source": [
    "## 3.3. Trading Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "772ff6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoTradingEnv(gym.Env):\n",
    "    \"\"\"Cryptocurrency trading environment with fixed position sizing\"\"\"\n",
    "    \n",
    "    def __init__(self, data, feature_columns, initial_balance=100, \n",
    "                 position_size=10, fee_rate=0.001):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Data\n",
    "        self.data = data  # (X, y) tuple\n",
    "        self.feature_columns = feature_columns\n",
    "        self.n_features = len(feature_columns)\n",
    "        \n",
    "        # Trading parameters\n",
    "        self.initial_balance = initial_balance\n",
    "        self.fixed_position_size = position_size  # Fixed 10 USDT\n",
    "        self.fee_rate = fee_rate\n",
    "        self.liquidation_threshold = 0.1  # 10% price movement\n",
    "        \n",
    "        # Action and observation spaces\n",
    "        self.action_space = spaces.Discrete(4)  # NONE, LONG, SHORT, EXIT\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, \n",
    "            shape=(self.n_features + 4,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.position_type = PositionType.FLAT\n",
    "        self.position_size = 0.0\n",
    "        self.entry_price = 0.0\n",
    "        \n",
    "        # Trading history\n",
    "        self.trades = []\n",
    "        self.balance_history = [self.balance]\n",
    "        \n",
    "        return self._get_observation()\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get current observation\"\"\"\n",
    "        if self.current_step >= len(self.data[0]):\n",
    "            return np.zeros(self.n_features + 4, dtype=np.float32)\n",
    "        \n",
    "        # Get current timestep features (last timestep from sequence)\n",
    "        current_sequence = self.data[0][self.current_step].cpu().numpy()  # Shape: (60, 11)\n",
    "        market_features = current_sequence[-1]  # Take only the last timestep: (11,)\n",
    "        \n",
    "        # Position information\n",
    "        position_info = np.array([\n",
    "            float(self.position_type),  # 0=FLAT, 1=LONG, 2=SHORT\n",
    "            self.position_size / self.initial_balance,  # Normalized position size\n",
    "            (self.balance / self.initial_balance) - 1.0,  # Balance change\n",
    "            1.0 if self.balance >= self.fixed_position_size + (self.fixed_position_size * self.fee_rate) else 0.0,  # Can trade\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return np.concatenate([market_features, position_info])\n",
    "    \n",
    "    def _get_current_prices(self):\n",
    "        \"\"\"Get OHLC prices for current step\"\"\"\n",
    "        if self.current_step >= len(self.data[0]):\n",
    "            return {'open': 0, 'high': 0, 'low': 0, 'close': 0}\n",
    "        \n",
    "        # Extract OHLC from features (assuming first 4 are log prices)\n",
    "        current_data = self.data[0][self.current_step].cpu().numpy()[-1]  # Last timestep\n",
    "        \n",
    "        # Convert from log prices back to actual prices (approximate)\n",
    "        return {\n",
    "            'open': float(np.exp(current_data[0])),\n",
    "            'high': float(np.exp(current_data[1])), \n",
    "            'low': float(np.exp(current_data[2])),\n",
    "            'close': float(np.exp(current_data[3]))\n",
    "        }\n",
    "    \n",
    "    def _check_liquidation(self, prices):\n",
    "        \"\"\"Check liquidation based on high/low prices vs entry\"\"\"\n",
    "        if self.position_type == PositionType.FLAT:\n",
    "            return False\n",
    "            \n",
    "        if self.position_type == PositionType.LONG:\n",
    "            # Long liquidated if low drops 10% below entry\n",
    "            return prices['low'] <= self.entry_price * (1 - self.liquidation_threshold)\n",
    "        else:  # SHORT\n",
    "            # Short liquidated if high rises 10% above entry  \n",
    "            return prices['high'] >= self.entry_price * (1 + self.liquidation_threshold)\n",
    "    \n",
    "    def _validate_action(self, action):\n",
    "        \"\"\"Validate if action is allowed given current state\"\"\"\n",
    "        if self.position_type == PositionType.FLAT:\n",
    "            # No position: can do NONE, LONG, SHORT\n",
    "            return action in [TradingAction.NONE, TradingAction.LONG, TradingAction.SHORT]\n",
    "        else:  # Has position\n",
    "            # Has position: can only do NONE, EXIT\n",
    "            return action in [TradingAction.NONE, TradingAction.EXIT]\n",
    "    \n",
    "    def _can_afford_trade(self):\n",
    "        \"\"\"Check if balance is sufficient for new position\"\"\"\n",
    "        required_margin = self.fixed_position_size + (self.fixed_position_size * self.fee_rate)\n",
    "        return self.balance >= required_margin\n",
    "    \n",
    "    def _open_position(self, action, price):\n",
    "        \"\"\"Open new position with fixed size\"\"\"\n",
    "        if not self._can_afford_trade():\n",
    "            return -0.5  # Heavy penalty for insufficient funds\n",
    "        \n",
    "        # Calculate required margin\n",
    "        required_margin = self.fixed_position_size + (self.fixed_position_size * self.fee_rate)\n",
    "        \n",
    "        # Deduct margin and fees\n",
    "        self.balance -= required_margin\n",
    "        self.position_type = PositionType.LONG if action == TradingAction.LONG else PositionType.SHORT\n",
    "        self.position_size = self.fixed_position_size\n",
    "        self.entry_price = price\n",
    "        \n",
    "        # Record trade\n",
    "        self.trades.append({\n",
    "            'action': 'OPEN',\n",
    "            'position_type': self.position_type,\n",
    "            'entry_price': price,\n",
    "            'size': self.fixed_position_size,\n",
    "            'fee': self.fixed_position_size * self.fee_rate\n",
    "        })\n",
    "        \n",
    "        return -0.01  # Small penalty for opening position (fees)\n",
    "    \n",
    "    def _close_position(self, price):\n",
    "        \"\"\"Close current position\"\"\"\n",
    "        # Calculate P&L\n",
    "        if self.position_type == PositionType.LONG:\n",
    "            pnl = self.position_size * (price - self.entry_price) / self.entry_price\n",
    "        else:  # SHORT\n",
    "            pnl = self.position_size * (self.entry_price - price) / self.entry_price\n",
    "        \n",
    "        # Calculate fees\n",
    "        fee = self.position_size * self.fee_rate\n",
    "        net_pnl = pnl - fee\n",
    "        \n",
    "        # Return capital + P&L\n",
    "        self.balance += self.position_size + net_pnl\n",
    "        \n",
    "        # Record trade\n",
    "        self.trades.append({\n",
    "            'action': 'CLOSE',\n",
    "            'position_type': self.position_type,\n",
    "            'entry_price': self.entry_price,\n",
    "            'exit_price': price,\n",
    "            'size': self.position_size,\n",
    "            'pnl': pnl,\n",
    "            'fee': fee,\n",
    "            'net_pnl': net_pnl\n",
    "        })\n",
    "        \n",
    "        # Reset position\n",
    "        self.position_type = PositionType.FLAT\n",
    "        self.position_size = 0.0\n",
    "        self.entry_price = 0.0\n",
    "        \n",
    "        return np.clip(net_pnl / self.initial_balance, -1.0, 1.0)  # Normalized reward\n",
    "    \n",
    "    def _liquidate_position(self):\n",
    "        \"\"\"Handle liquidation - lose entire position\"\"\"\n",
    "        loss = -self.position_size  # Lose entire position value\n",
    "        \n",
    "        # Record liquidation\n",
    "        self.trades.append({\n",
    "            'action': 'LIQUIDATION',\n",
    "            'position_type': self.position_type,\n",
    "            'entry_price': self.entry_price,\n",
    "            'size': self.position_size,\n",
    "            'net_pnl': loss,\n",
    "            'liquidated': True\n",
    "        })\n",
    "        \n",
    "        # Reset position (no money returned)\n",
    "        self.position_type = PositionType.FLAT\n",
    "        self.position_size = 0.0\n",
    "        self.entry_price = 0.0\n",
    "        \n",
    "        return -1.0  # Maximum penalty for liquidation\n",
    "    \n",
    "    @property\n",
    "    def equity(self):\n",
    "        \"\"\"Calculate current equity (balance + unrealized P&L)\"\"\"\n",
    "        if self.position_type == PositionType.FLAT:\n",
    "            return self.balance\n",
    "        \n",
    "        # Get current price for unrealized P&L calculation\n",
    "        current_prices = self._get_current_prices()\n",
    "        current_price = current_prices['close']\n",
    "        \n",
    "        # Calculate unrealized P&L\n",
    "        if self.position_type == PositionType.LONG:\n",
    "            unrealized_pnl = self.position_size * (current_price - self.entry_price) / self.entry_price\n",
    "        else:  # SHORT\n",
    "            unrealized_pnl = self.position_size * (self.entry_price - current_price) / self.entry_price\n",
    "        \n",
    "        return self.balance + self.position_size + unrealized_pnl\n",
    "    \n",
    "    def _execute_trade(self, action, prices):\n",
    "        \"\"\"Execute trading action\"\"\"\n",
    "        # Validate action\n",
    "        if not self._validate_action(action):\n",
    "            return -0.1  # Penalty for invalid action\n",
    "        \n",
    "        if action == TradingAction.NONE:\n",
    "            return 0.0\n",
    "            \n",
    "        elif action == TradingAction.EXIT:\n",
    "            if self.position_type != PositionType.FLAT:\n",
    "                return self._close_position(prices['close'])\n",
    "                \n",
    "        elif action in [TradingAction.LONG, TradingAction.SHORT]:\n",
    "            if self.position_type == PositionType.FLAT:\n",
    "                return self._open_position(action, prices['close'])\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def _calculate_reward(self, action, prices, trade_result):\n",
    "        \"\"\"Calculate reward encouraging profitable trading\"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Base survival reward\n",
    "        reward += 0.01\n",
    "        \n",
    "        # Trading rewards\n",
    "        if action != TradingAction.NONE:\n",
    "            if trade_result > 0:\n",
    "                # Profitable trade\n",
    "                reward += trade_result * 10  # Amplify profit reward\n",
    "            elif trade_result < 0 and trade_result > -0.1:\n",
    "                # Small loss is acceptable\n",
    "                reward += trade_result * 2\n",
    "            else:\n",
    "                # Large loss penalty\n",
    "                reward += trade_result * 5\n",
    "        \n",
    "        # Position holding rewards\n",
    "        if self.position_type != PositionType.FLAT:\n",
    "            current_price = prices['close']\n",
    "            if self.position_type == PositionType.LONG:\n",
    "                unrealized_return = (current_price - self.entry_price) / self.entry_price\n",
    "            else:\n",
    "                unrealized_return = (self.entry_price - current_price) / self.entry_price\n",
    "            \n",
    "            # Reward for profitable positions\n",
    "            reward += unrealized_return * 0.5\n",
    "        \n",
    "        # Encourage exploration (prevent always doing NONE)\n",
    "        if action == TradingAction.NONE and self.position_type == PositionType.FLAT:\n",
    "            reward -= 0.005  # Small penalty for inaction\n",
    "        \n",
    "        return np.clip(reward, -2.0, 2.0)\n",
    "\n",
    "    # Update step method to use new reward calculation\n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one trading step\"\"\"\n",
    "        if self.current_step >= len(self.data[0]) - 1:\n",
    "            return self._get_observation(), 0, True, {}\n",
    "        \n",
    "        prices = self._get_current_prices()\n",
    "        \n",
    "        # Check liquidation first\n",
    "        if self._check_liquidation(prices):\n",
    "            trade_result = self._liquidate_position()\n",
    "        else:\n",
    "            trade_result = self._execute_trade(action, prices)\n",
    "        \n",
    "        # Calculate comprehensive reward\n",
    "        reward = self._calculate_reward(action, prices, trade_result)\n",
    "        \n",
    "        # Record history\n",
    "        self.balance_history.append(self.balance)\n",
    "        self.equity_history.append(self.equity)\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = (self.current_step >= len(self.data[0]) - 1) or (self.balance <= 0)\n",
    "        \n",
    "        info = {\n",
    "            'balance': self.balance,\n",
    "            'position_type': self.position_type,\n",
    "            'total_trades': len(self.trades),\n",
    "            'can_trade': self._can_afford_trade(),\n",
    "            'action_taken': action\n",
    "        }\n",
    "        \n",
    "        return self._get_observation(), reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52b067",
   "metadata": {},
   "source": [
    "## 3.4. Load Data and Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f2246e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: torch.Size([147634, 60, 11]), torch.Size([147634])\n",
      "Trading environment created successfully\n",
      "Observation space: (15,)\n",
      "Action space: 4\n"
     ]
    }
   ],
   "source": [
    "# Load processed tensor data\n",
    "DATA_DIR = Path('data')\n",
    "tensor_path = DATA_DIR / 'processed' / SYMBOL / 'tensor_data.pt'\n",
    "\n",
    "if not tensor_path.exists():\n",
    "    print(\"Error: Tensor data not found. Run Notebook 2 first.\")\n",
    "else:\n",
    "    # Load data\n",
    "    processed_data = torch.load(tensor_path, map_location=device)\n",
    "    \n",
    "    X_train = processed_data['X_train']\n",
    "    y_train = processed_data['y_train']\n",
    "    feature_columns = processed_data['feature_columns']\n",
    "    \n",
    "    print(f\"Loaded data: {X_train.shape}, {y_train.shape}\")\n",
    "    \n",
    "    # Create environment with training data\n",
    "    train_env = CryptoTradingEnv(\n",
    "        data=(X_train, y_train),\n",
    "        feature_columns=feature_columns,\n",
    "        initial_balance=100,\n",
    "        position_size=10,      # คงที่ 10 USDT\n",
    "        fee_rate=0.001\n",
    "    )\n",
    "    \n",
    "    print(f\"Trading environment created successfully\")\n",
    "    print(f\"Observation space: {train_env.observation_space.shape}\")\n",
    "    print(f\"Action space: {train_env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd25b35",
   "metadata": {},
   "source": [
    "## 3.5. Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66288956",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CryptoTradingEnv' object has no attribute 'equity_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Test the environment\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m test_env \u001b[38;5;241m=\u001b[39m \u001b[43mtest_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m, in \u001b[0;36mtest_environment\u001b[1;34m(env, n_steps)\u001b[0m\n\u001b[0;32m      9\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     10\u001b[0m actions_taken[action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 12\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[12], line 276\u001b[0m, in \u001b[0;36mCryptoTradingEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# Record history\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbalance_history\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbalance)\n\u001b[1;32m--> 276\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequity_history\u001b[49m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mequity)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# Move to next step\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CryptoTradingEnv' object has no attribute 'equity_history'"
     ]
    }
   ],
   "source": [
    "def test_environment(env, n_steps=1000):\n",
    "    \"\"\"Test environment with random actions\"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    actions_taken = {0: 0, 1: 0, 2: 0, 3: 0}  # Count each action\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        # Random action\n",
    "        action = env.action_space.sample()\n",
    "        actions_taken[action] += 1\n",
    "        \n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    print(f\"=== Environment Test Results ===\")\n",
    "    print(f\"Steps taken: {step + 1}\")\n",
    "    print(f\"Final balance: ${env.balance:.2f}\")\n",
    "    print(f\"Final equity: ${env.equity:.2f}\")\n",
    "    print(f\"Total return: {(env.equity / env.initial_balance - 1) * 100:.2f}%\")\n",
    "    print(f\"Total trades: {len(env.trades)}\")\n",
    "    print(f\"Total reward: {total_reward:.4f}\")\n",
    "    print(f\"Actions taken: {actions_taken}\")\n",
    "    \n",
    "    return env\n",
    "\n",
    "# Test the environment\n",
    "test_env = test_environment(train_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036bf97b",
   "metadata": {},
   "source": [
    "## 3.6. Environment Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trading_results(env):\n",
    "    \"\"\"Plot trading results\"\"\"\n",
    "    if len(env.balance_history) < 2:\n",
    "        print(\"No trading history to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Create equity history if missing\n",
    "    if not hasattr(env, 'equity_history'):\n",
    "        env.equity_history = [env.balance] * len(env.balance_history)\n",
    "    \n",
    "    # Ensure equal lengths\n",
    "    min_len = min(len(env.equity_history), len(env.balance_history))\n",
    "    steps = range(min_len)\n",
    "    \n",
    "    # Plot equity curve\n",
    "    ax1.plot(steps, env.equity_history[:min_len], label='Equity', color='blue')\n",
    "    ax1.plot(steps, env.balance_history[:min_len], label='Balance', color='green')\n",
    "    ax1.axhline(y=env.initial_balance, color='red', linestyle='--', label='Initial Balance')\n",
    "    ax1.set_ylabel('Value ($)')\n",
    "    ax1.set_title('Portfolio Performance')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot returns\n",
    "    returns = [(eq / env.initial_balance - 1) * 100 for eq in env.equity_history[:min_len]]\n",
    "    ax2.plot(steps, returns, color='purple')\n",
    "    ax2.axhline(y=0, color='red', linestyle='--')\n",
    "    ax2.set_xlabel('Steps')\n",
    "    ax2.set_ylabel('Return (%)')\n",
    "    ax2.set_title('Cumulative Returns')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Trade statistics\n",
    "    if env.trades:\n",
    "        trade_pnls = [trade.get('net_pnl', 0) for trade in env.trades if 'net_pnl' in trade]\n",
    "        if trade_pnls:\n",
    "            winning_trades = [pnl for pnl in trade_pnls if pnl > 0]\n",
    "            losing_trades = [pnl for pnl in trade_pnls if pnl < 0]\n",
    "            \n",
    "            print(f\"\\n=== Trade Statistics ===\")\n",
    "            print(f\"Total trades: {len(trade_pnls)}\")\n",
    "            print(f\"Winning trades: {len(winning_trades)} ({len(winning_trades)/len(trade_pnls)*100:.1f}%)\")\n",
    "            print(f\"Losing trades: {len(losing_trades)} ({len(losing_trades)/len(trade_pnls)*100:.1f}%)\")\n",
    "            print(f\"Average win: ${np.mean(winning_trades):.2f}\" if winning_trades else \"No winning trades\")\n",
    "            print(f\"Average loss: ${np.mean(losing_trades):.2f}\" if losing_trades else \"No losing trades\")\n",
    "            print(f\"Total PnL: ${sum(trade_pnls):.2f}\")\n",
    "    else:\n",
    "        print(\"\\n=== No Trades Executed ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ca1f6",
   "metadata": {},
   "source": [
    "## 3.7. Save Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0243f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save environment configuration\n",
    "env_config = {\n",
    "    'initial_balance': INITIAL_BALANCE,\n",
    "    'leverage': LEVERAGE,\n",
    "    'fee_rate': FEE_RATE,\n",
    "    'stop_loss': STOP_LOSS,\n",
    "    'take_profit': TAKE_PROFIT,\n",
    "    'liquidation_threshold': LIQUIDATION_THRESHOLD,\n",
    "    'observation_shape': train_env.observation_space.shape,\n",
    "    'action_space_size': train_env.action_space.n,\n",
    "    'n_features': train_env.n_features\n",
    "}\n",
    "\n",
    "import json\n",
    "config_path = DATA_DIR / 'processed' / SYMBOL / 'env_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(env_config, f, indent=2)\n",
    "\n",
    "print(f\"Environment configuration saved to {config_path}\")\n",
    "print(f\"\\n=== Environment Setup Complete ===\")\n",
    "print(f\"Observation space: {train_env.observation_space.shape}\")\n",
    "print(f\"Action space: {train_env.action_space.n} actions\")\n",
    "print(f\"Features: {train_env.n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265a00f",
   "metadata": {},
   "source": [
    "# 4. Deep Q-Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f59d4",
   "metadata": {},
   "source": [
    "## 4.1. Import Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"DQN Model Setup - Device: {device}\")\n",
    "\n",
    "# DQN Hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.95  # Discount factor\n",
    "EPS_START = 0.9  # Starting epsilon for exploration\n",
    "EPS_END = 0.05   # Minimum epsilon\n",
    "EPS_DECAY = 0.9995  # Epsilon decay rate\n",
    "TARGET_UPDATE = 10  # Update target network every N episodes\n",
    "MEMORY_SIZE = 100000  # Replay buffer size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddca62a",
   "metadata": {},
   "source": [
    "## 4.2. Deep Q-Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c9b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network with LSTM for sequence processing\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=256, lstm_layers=2, n_actions=4):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # LSTM for processing sequence data\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2 if lstm_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers for LSTM output\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, n_actions)\n",
    "        \n",
    "        # Simple FC layers for single observations (bypass LSTM)\n",
    "        self.simple_fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.simple_fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.simple_fc3 = nn.Linear(hidden_size // 2, n_actions)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Handle different input formats\n",
    "        if x.dim() == 2:\n",
    "            # Input is [batch, features] - use simple FC layers\n",
    "            x = F.relu(self.simple_fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.simple_fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.simple_fc3(x)\n",
    "            return x\n",
    "        elif x.dim() == 3:\n",
    "            # Input is [batch, sequence, features] - use LSTM\n",
    "            # Initialize LSTM hidden state\n",
    "            h0 = torch.zeros(self.lstm_layers, batch_size, self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.lstm_layers, batch_size, self.hidden_size).to(x.device)\n",
    "            \n",
    "            # LSTM forward pass\n",
    "            lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "            \n",
    "            # Take the last output from the sequence\n",
    "            lstm_out = lstm_out[:, -1, :]\n",
    "            \n",
    "            # Fully connected layers\n",
    "            x = F.relu(self.fc1(lstm_out))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            \n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input dimension: {x.dim()}. Expected 2D or 3D input.\")\n",
    "\n",
    "# Test DQN architecture\n",
    "input_size = 15  # Will be set based on actual data\n",
    "test_dqn = DQN(input_size=input_size)\n",
    "print(f\"DQN Architecture:\")\n",
    "print(test_dqn)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in test_dqn.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474d9a9",
   "metadata": {},
   "source": [
    "## 4.3. Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f613ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named tuple for storing experiences\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for storing and sampling transitions\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.buffer.append(Transition(state, action, next_state, reward, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions\"\"\"\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.tensor(batch.action, dtype=torch.long)\n",
    "        next_state_batch = torch.stack(batch.next_state)\n",
    "        reward_batch = torch.tensor(batch.reward, dtype=torch.float32)\n",
    "        done_batch = torch.tensor(batch.done, dtype=torch.bool)\n",
    "        \n",
    "        return state_batch, action_batch, next_state_batch, reward_batch, done_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Create replay buffer\n",
    "replay_buffer = ReplayBuffer(MEMORY_SIZE)\n",
    "print(f\"Replay buffer created with capacity: {MEMORY_SIZE:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac14ca",
   "metadata": {},
   "source": [
    "## 4.4. Double DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e039e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent:\n",
    "    \"\"\"Double DQN Agent for cryptocurrency trading\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, learning_rate=LEARNING_RATE):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Q-Networks\n",
    "        self.q_network = DQN(state_size, n_actions=action_size).to(device)\n",
    "        self.target_network = DQN(state_size, n_actions=action_size).to(device)\n",
    "        \n",
    "        # Copy weights to target network\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = EPS_START\n",
    "        \n",
    "        # Training metrics\n",
    "        self.loss_history = []\n",
    "        self.q_values_history = []\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from main network to target network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # Convert state to tensor\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "        \n",
    "        # Add batch dimension if needed: [features] -> [batch=1, features]\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        # Get Q-values\n",
    "        self.q_network.eval()\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "            self.q_values_history.append(q_values.cpu().numpy().flatten())\n",
    "    \n",
    "        self.q_network.train()\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def learn(self, replay_buffer, batch_size=BATCH_SIZE):\n",
    "        \"\"\"Train the network on a batch of experiences\"\"\"\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, next_states, rewards, dones = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Double DQN: use main network to select actions, target network to evaluate\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.q_network(next_states).argmax(1)\n",
    "            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1))\n",
    "            target_q_values = rewards.unsqueeze(1) + (GAMMA * next_q_values * (~dones).unsqueeze(1))\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update epsilon\n",
    "        if self.epsilon > EPS_END:\n",
    "            self.epsilon *= EPS_DECAY\n",
    "        \n",
    "        # Record loss\n",
    "        self.loss_history.append(loss.item())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model and training state\"\"\"\n",
    "        torch.save({\n",
    "            'q_network_state_dict': self.q_network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'loss_history': self.loss_history,\n",
    "            'state_size': self.state_size,\n",
    "            'action_size': self.action_size,\n",
    "            'learning_rate': self.learning_rate\n",
    "        }, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load model and training state\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.loss_history = checkpoint['loss_history']\n",
    "\n",
    "print(\"DoubleDQNAgent class defined successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4811272",
   "metadata": {},
   "source": [
    "## 4.5. Load Data and Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229f6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment configuration and data\n",
    "DATA_DIR = Path('data')\n",
    "config_path = DATA_DIR / 'processed' / SYMBOL / 'env_config.json'\n",
    "tensor_path = DATA_DIR / 'processed' / SYMBOL / 'tensor_data.pt'\n",
    "\n",
    "# Load environment config\n",
    "with open(config_path, 'r') as f:\n",
    "    env_config = json.load(f)\n",
    "\n",
    "# Load tensor data\n",
    "processed_data = torch.load(tensor_path, map_location=device)\n",
    "\n",
    "# Get dimensions\n",
    "observation_shape = env_config['observation_shape']\n",
    "action_size = env_config['action_space_size']\n",
    "state_size = observation_shape[0]\n",
    "\n",
    "print(f\"State size: {state_size}\")\n",
    "print(f\"Action size: {action_size}\")\n",
    "\n",
    "# Initialize DQN agent\n",
    "agent = DoubleDQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "print(f\"DQN Agent initialized\")\n",
    "print(f\"Q-Network parameters: {sum(p.numel() for p in agent.q_network.parameters()):,}\")\n",
    "print(f\"Target Network parameters: {sum(p.numel() for p in agent.target_network.parameters()):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0023c",
   "metadata": {},
   "source": [
    "## 4.6. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd82a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_performance(agent, env, n_episodes=5):\n",
    "    \"\"\"Test agent performance without training\"\"\"\n",
    "    total_rewards = []\n",
    "    final_balances = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        \n",
    "        while step < 1000:  # Limit steps\n",
    "            action = agent.act(state, training=False)  # No exploration\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            step += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        final_balances.append(env.equity)\n",
    "        \n",
    "        print(f\"Episode {episode + 1}: Reward {total_reward:.4f}, \"\n",
    "              f\"Final Equity ${env.equity:.2f}, Steps {step}\")\n",
    "    \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    avg_balance = np.mean(final_balances)\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Average Reward: {avg_reward:.4f}\")\n",
    "    print(f\"Average Final Equity: ${avg_balance:.2f}\")\n",
    "    print(f\"Average Return: {(avg_balance / env.initial_balance - 1) * 100:.2f}%\")\n",
    "    \n",
    "    return avg_reward, avg_balance\n",
    "\n",
    "def save_training_state(agent, episode, save_dir):\n",
    "    \"\"\"Save training state and metrics\"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = save_dir / f'dqn_model_episode_{episode}.pt'\n",
    "    agent.save(model_path)\n",
    "    \n",
    "    # Save training metrics\n",
    "    metrics = {\n",
    "        'episode': episode,\n",
    "        'epsilon': agent.epsilon,\n",
    "        'loss_history': agent.loss_history[-100:],  # Last 100 losses\n",
    "        'avg_loss': np.mean(agent.loss_history[-100:]) if agent.loss_history else 0,\n",
    "        'q_values_stats': {\n",
    "            'mean': np.mean(agent.q_values_history[-100:]) if agent.q_values_history else 0,\n",
    "            'std': np.std(agent.q_values_history[-100:]) if agent.q_values_history else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metrics_path = save_dir / f'training_metrics_episode_{episode}.json'\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"Training state saved to {save_dir}\")\n",
    "    return model_path\n",
    "\n",
    "print(\"Training utilities defined successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc2923",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcd58d39",
   "metadata": {},
   "source": [
    "## 4.7. Model Architecture Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23594464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model information\n",
    "print(\"=== Double DQN Model Summary ===\")\n",
    "print(f\"Architecture: LSTM + Fully Connected\")\n",
    "print(f\"Input size: {state_size}\")\n",
    "print(f\"Hidden size: 256\")\n",
    "print(f\"LSTM layers: 2\")\n",
    "print(f\"Output size: {action_size}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Replay buffer size: {MEMORY_SIZE:,}\")\n",
    "print(f\"Gamma (discount): {GAMMA}\")\n",
    "print(f\"Epsilon decay: {EPS_DECAY}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, 60, state_size).to(device)  # Batch, sequence, features\n",
    "with torch.no_grad():\n",
    "    output = agent.q_network(dummy_input)\n",
    "    print(f\"\\nTest forward pass:\")\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Q-values sample: {output.cpu().numpy().flatten()}\")\n",
    "\n",
    "# Create model save directory\n",
    "SYMBOL = 'BTCUSDT'\n",
    "model_save_dir = Path('models') / SYMBOL / 'double_dqn'\n",
    "model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nModel ready for training!\")\n",
    "print(f\"Save directory: {model_save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e23c7",
   "metadata": {},
   "source": [
    "# 5. Training & Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4411fee",
   "metadata": {},
   "source": [
    "## 5.1. Import & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Training parameters\n",
    "N_EPISODES = 2000\n",
    "MAX_STEPS_PER_EPISODE = 2000\n",
    "SAVE_EVERY = 50\n",
    "TEST_EVERY = 25\n",
    "\n",
    "SYMBOL = 'BTCUSDT'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training Setup - Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26b5ac",
   "metadata": {},
   "source": [
    "## 5.2. Load All Data & Create Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f3b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_DIR = Path('data')\n",
    "tensor_path = DATA_DIR / 'processed' / SYMBOL / 'tensor_data.pt'\n",
    "config_path = DATA_DIR / 'processed' / SYMBOL / 'env_config.json'\n",
    "\n",
    "processed_data = torch.load(tensor_path, map_location=device)\n",
    "with open(config_path, 'r') as f:\n",
    "    env_config = json.load(f)\n",
    "\n",
    "# Prepare data\n",
    "X_train, y_train = processed_data['X_train'], processed_data['y_train']\n",
    "X_val, y_val = processed_data['X_val'], processed_data['y_val']\n",
    "X_test, y_test = processed_data['X_test'], processed_data['y_test']\n",
    "feature_columns = processed_data['feature_columns']\n",
    "\n",
    "# Create environments\n",
    "train_env = CryptoTradingEnv(\n",
    "    data=(X_train, y_train), \n",
    "    feature_columns=feature_columns,\n",
    "    initial_balance=100, \n",
    "    position_size=10,  # Fixed position size\n",
    "    fee_rate=0.001\n",
    ")\n",
    "\n",
    "val_env = CryptoTradingEnv(\n",
    "    data=(X_val, y_val), \n",
    "    feature_columns=feature_columns,\n",
    "    initial_balance=100, \n",
    "    position_size=10,\n",
    "    fee_rate=0.001\n",
    ")\n",
    "\n",
    "test_env = CryptoTradingEnv(\n",
    "    data=(X_test, y_test), \n",
    "    feature_columns=feature_columns,\n",
    "    initial_balance=100, \n",
    "    position_size=10,\n",
    "    fee_rate=0.001\n",
    ")\n",
    "# Initialize agent\n",
    "agent = DoubleDQNAgent(\n",
    "    state_size=15,\n",
    "    action_size=env_config['action_space_size']\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer(100000)\n",
    "\n",
    "print(f\"Environments created: Train({len(X_train)}), Val({len(X_val)}), Test({len(X_test)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a6183",
   "metadata": {},
   "source": [
    "## 5.3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410da9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_agent(agent, train_env, val_env, replay_buffer, n_episodes=5000):\n",
    "    \"\"\"Main training loop\"\"\"\n",
    "    \n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    episode_losses = []\n",
    "    validation_scores = []\n",
    "    best_val_score = -float('inf')\n",
    "    \n",
    "    print(\"Starting DQN Training...\")\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # Training episode\n",
    "        state = train_env.reset()\n",
    "        total_reward = 0\n",
    "        episode_loss = []\n",
    "        \n",
    "        for step in range(MAX_STEPS_PER_EPISODE):\n",
    "            # Select action\n",
    "            action = agent.act(state, training=True)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done, info = train_env.step(action)\n",
    "            \n",
    "            # Store experience\n",
    "            state_tensor = torch.FloatTensor(state).to(device)\n",
    "            next_state_tensor = torch.FloatTensor(next_state).to(device)\n",
    "            \n",
    "            replay_buffer.push(\n",
    "                state_tensor, action, next_state_tensor, reward, done\n",
    "            )\n",
    "            \n",
    "            # Learn from experience\n",
    "            if len(replay_buffer) > BATCH_SIZE:\n",
    "                loss = agent.learn(replay_buffer)\n",
    "                if loss is not None:\n",
    "                    episode_loss.append(loss)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update target network\n",
    "        if episode % TARGET_UPDATE == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "        avg_loss = np.mean(episode_loss) if episode_loss else 0\n",
    "        episode_losses.append(avg_loss)\n",
    "        \n",
    "        # Validation\n",
    "        if episode % TEST_EVERY == 0:\n",
    "            val_reward, val_equity = test_agent_performance(agent, val_env, n_episodes=3)\n",
    "            validation_scores.append(val_equity)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_equity > best_val_score:\n",
    "                best_val_score = val_equity\n",
    "                best_model_path = Path('models') / SYMBOL / 'best_model.pt'\n",
    "                best_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                agent.save(best_model_path)\n",
    "            \n",
    "            print(f\"Episode {episode}: Reward {total_reward:.3f}, \"\n",
    "                  f\"Loss {avg_loss:.4f}, Val Equity ${val_equity:.2f}, \"\n",
    "                  f\"Epsilon {agent.epsilon:.3f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if episode % SAVE_EVERY == 0 and episode > 0:\n",
    "            checkpoint_path = Path('models') / SYMBOL / f'checkpoint_{episode}.pt'\n",
    "            checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            agent.save(checkpoint_path)\n",
    "    \n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_losses': episode_losses,\n",
    "        'validation_scores': validation_scores,\n",
    "        'best_val_score': best_val_score\n",
    "    }\n",
    "\n",
    "# Start training\n",
    "training_results = train_dqn_agent(agent, train_env, val_env, replay_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685eb1fd",
   "metadata": {},
   "source": [
    "## 5.4. Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(results):\n",
    "    \"\"\"Plot training metrics\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    episodes = range(len(results['episode_rewards']))\n",
    "    \n",
    "    # Episode rewards\n",
    "    ax1.plot(episodes, results['episode_rewards'])\n",
    "    ax1.set_title('Episode Rewards')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Moving average rewards\n",
    "    window = 20\n",
    "    if len(results['episode_rewards']) >= window:\n",
    "        ma_rewards = pd.Series(results['episode_rewards']).rolling(window).mean()\n",
    "        ax2.plot(episodes, results['episode_rewards'], alpha=0.3, label='Episode Reward')\n",
    "        ax2.plot(episodes, ma_rewards, label=f'{window}-Episode MA')\n",
    "        ax2.set_title('Smoothed Episode Rewards')\n",
    "        ax2.set_xlabel('Episode')\n",
    "        ax2.set_ylabel('Reward')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "    \n",
    "    # Training losses\n",
    "    ax3.plot(episodes, results['episode_losses'])\n",
    "    ax3.set_title('Training Loss')\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    # Validation scores\n",
    "    val_episodes = range(0, len(results['episode_rewards']), TEST_EVERY)\n",
    "    if results['validation_scores']:\n",
    "        ax4.plot(val_episodes[:len(results['validation_scores'])], results['validation_scores'], 'o-')\n",
    "        ax4.axhline(y=100, color='r', linestyle='--', label='Initial Balance')\n",
    "        ax4.set_title('Validation Performance')\n",
    "        ax4.set_xlabel('Episode')\n",
    "        ax4.set_ylabel('Final Equity ($)')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n=== Training Summary ===\")\n",
    "    print(f\"Total Episodes: {len(results['episode_rewards'])}\")\n",
    "    print(f\"Final Episode Reward: {results['episode_rewards'][-1]:.3f}\")\n",
    "    print(f\"Average Reward (last 50): {np.mean(results['episode_rewards'][-50:]):.3f}\")\n",
    "    print(f\"Best Validation Score: ${results['best_val_score']:.2f}\")\n",
    "    print(f\"Final Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "plot_training_results(training_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eca01bf",
   "metadata": {},
   "source": [
    "## 5.5. Backtest on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6193d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_backtest(agent, test_env, model_path=None):\n",
    "    \"\"\"Comprehensive backtest with detailed analysis\"\"\"\n",
    "    \n",
    "    if model_path:\n",
    "        agent.load(model_path)\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "    \n",
    "    # Run backtest\n",
    "    state = test_env.reset()\n",
    "    actions_history = []\n",
    "    rewards_history = []\n",
    "    equity_history = [test_env.equity]\n",
    "    positions_history = []\n",
    "    \n",
    "    for step in range(len(test_env.data[0]) - 1):\n",
    "        action = agent.act(state, training=False)\n",
    "        next_state, reward, done, info = test_env.step(action)\n",
    "        \n",
    "        actions_history.append(action)\n",
    "        rewards_history.append(reward)\n",
    "        equity_history.append(test_env.equity)\n",
    "        positions_history.append(test_env.position_type.value)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_return = (test_env.equity / test_env.initial_balance - 1) * 100\n",
    "    max_equity = max(equity_history)\n",
    "    min_equity = min(equity_history)\n",
    "    max_drawdown = (max_equity - min_equity) / max_equity * 100\n",
    "    \n",
    "    # Trade analysis\n",
    "    closed_trades = [t for t in test_env.trades if 'net_pnl' in t]\n",
    "    winning_trades = [t for t in closed_trades if t['net_pnl'] > 0]\n",
    "    losing_trades = [t for t in closed_trades if t['net_pnl'] < 0]\n",
    "    \n",
    "    win_rate = len(winning_trades) / len(closed_trades) * 100 if closed_trades else 0\n",
    "    avg_win = np.mean([t['net_pnl'] for t in winning_trades]) if winning_trades else 0\n",
    "    avg_loss = np.mean([t['net_pnl'] for t in losing_trades]) if losing_trades else 0\n",
    "    profit_factor = abs(sum(t['net_pnl'] for t in winning_trades) / \n",
    "                       sum(t['net_pnl'] for t in losing_trades)) if losing_trades else float('inf')\n",
    "    \n",
    "    # Results\n",
    "    results = {\n",
    "        'final_equity': test_env.equity,\n",
    "        'total_return': total_return,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'total_trades': len(closed_trades),\n",
    "        'win_rate': win_rate,\n",
    "        'avg_win': avg_win,\n",
    "        'avg_loss': avg_loss,\n",
    "        'profit_factor': profit_factor,\n",
    "        'equity_history': equity_history,\n",
    "        'actions_history': actions_history,\n",
    "        'positions_history': positions_history\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run backtest with best model\n",
    "best_model_path = Path('models') / SYMBOL / 'best_model.pt'\n",
    "backtest_results = detailed_backtest(agent, test_env, best_model_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7dee4b",
   "metadata": {},
   "source": [
    "## 5.6. Backtest Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_backtest_results(results):\n",
    "    \"\"\"Plot comprehensive backtest analysis\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    steps = range(len(results['equity_history']))\n",
    "    \n",
    "    # Equity curve\n",
    "    ax1.plot(steps, results['equity_history'], linewidth=2)\n",
    "    ax1.axhline(y=100, color='r', linestyle='--', alpha=0.7, label='Initial Balance')\n",
    "    ax1.fill_between(steps, results['equity_history'], 100, \n",
    "                     where=np.array(results['equity_history']) >= 100, \n",
    "                     alpha=0.3, color='green', label='Profit')\n",
    "    ax1.fill_between(steps, results['equity_history'], 100, \n",
    "                     where=np.array(results['equity_history']) < 100, \n",
    "                     alpha=0.3, color='red', label='Loss')\n",
    "    ax1.set_title('Portfolio Equity Curve')\n",
    "    ax1.set_xlabel('Steps')\n",
    "    ax1.set_ylabel('Equity ($)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Returns\n",
    "    returns = [(eq/100 - 1) * 100 for eq in results['equity_history']]\n",
    "    ax2.plot(steps, returns, color='purple')\n",
    "    ax2.axhline(y=0, color='r', linestyle='--', alpha=0.7)\n",
    "    ax2.set_title('Cumulative Returns (%)')\n",
    "    ax2.set_xlabel('Steps')\n",
    "    ax2.set_ylabel('Return (%)')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Action distribution\n",
    "    action_names = ['NONE', 'LONG', 'SHORT', 'EXIT']\n",
    "    action_counts = [results['actions_history'].count(i) for i in range(4)]\n",
    "    ax3.bar(action_names, action_counts)\n",
    "    ax3.set_title('Action Distribution')\n",
    "    ax3.set_ylabel('Count')\n",
    "    \n",
    "    # Position timeline\n",
    "    ax4.plot(steps[:len(results['positions_history'])], results['positions_history'])\n",
    "    ax4.set_title('Position Timeline (0=FLAT, 1=LONG, 2=SHORT)')\n",
    "    ax4.set_xlabel('Steps')\n",
    "    ax4.set_ylabel('Position Type')\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_backtest_summary(results):\n",
    "    \"\"\"Print detailed backtest summary\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"BACKTEST RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Initial Balance:    ${100:.2f}\")\n",
    "    print(f\"Final Equity:       ${results['final_equity']:.2f}\")\n",
    "    print(f\"Total Return:       {results['total_return']:.2f}%\")\n",
    "    print(f\"Max Drawdown:       {results['max_drawdown']:.2f}%\")\n",
    "    print(f\"Total Trades:       {results['total_trades']}\")\n",
    "    print(f\"Win Rate:           {results['win_rate']:.1f}%\")\n",
    "    print(f\"Average Win:        ${results['avg_win']:.2f}\")\n",
    "    print(f\"Average Loss:       ${results['avg_loss']:.2f}\")\n",
    "    print(f\"Profit Factor:      {results['profit_factor']:.2f}\")\n",
    "    \n",
    "    # Performance rating\n",
    "    if results['total_return'] > 20:\n",
    "        rating = \"Excellent\"\n",
    "    elif results['total_return'] > 10:\n",
    "        rating = \"Good\"\n",
    "    elif results['total_return'] > 0:\n",
    "        rating = \"Positive\"\n",
    "    else:\n",
    "        rating = \"Negative\"\n",
    "    \n",
    "    print(f\"Performance Rating: {rating}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Display results\n",
    "plot_backtest_results(backtest_results)\n",
    "print_backtest_summary(backtest_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476b38e",
   "metadata": {},
   "source": [
    "## 5.7. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d17d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model and results\n",
    "final_results = {\n",
    "    'training_results': training_results,\n",
    "    'backtest_results': {k: v for k, v in backtest_results.items() \n",
    "                        if k not in ['equity_history', 'actions_history', 'positions_history']},\n",
    "    'model_config': {\n",
    "        'symbol': SYMBOL,\n",
    "        'episodes_trained': N_EPISODES,\n",
    "        'final_epsilon': agent.epsilon,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'gamma': GAMMA\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save results\n",
    "results_dir = Path('results') / SYMBOL\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results_path = results_dir / 'training_backtest_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "# Save final model\n",
    "final_model_path = results_dir / 'final_model.pt'\n",
    "agent.save(final_model_path)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(f\"\\n=== CRYPPO Training & Backtesting Complete ===\")\n",
    "print(f\"Total Return: {backtest_results['total_return']:.2f}%\")\n",
    "print(f\"Win Rate: {backtest_results['win_rate']:.1f}%\")\n",
    "print(f\"Total Trades: {backtest_results['total_trades']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "def plot_candlestick_data(data, n_candles=1000):\n",
    "    \"\"\"Plot candlestick chart from training data\"\"\"\n",
    "    \n",
    "    # Extract price data from first n samples\n",
    "    X_data = data[0][:n_candles].cpu().numpy()  # Shape: (n, 60, 11)\n",
    "    \n",
    "    # Take last timestep of each sequence for OHLC\n",
    "    price_data = X_data[:, -1, :4]  # Last timestep, first 4 features (log prices)\n",
    "    \n",
    "    # Convert from log prices to actual prices\n",
    "    ohlc_data = np.exp(price_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(ohlc_data, columns=['Open', 'High', 'Low', 'Close'])\n",
    "    df['Date'] = pd.date_range(start='2023-01-01', periods=len(df), freq='5T')\n",
    "    \n",
    "    # Create candlestick chart\n",
    "    fig = go.Figure(data=go.Candlestick(\n",
    "        x=df['Date'],\n",
    "        open=df['Open'],\n",
    "        high=df['High'], \n",
    "        low=df['Low'],\n",
    "        close=df['Close'],\n",
    "        name='BTCUSDT'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='BTCUSDT Candlestick Chart (5min)',\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Price (USDT)',\n",
    "        height=600,\n",
    "        xaxis_rangeslider_visible=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Print price statistics\n",
    "    print(f\"Price Range: ${df['Low'].min():.2f} - ${df['High'].max():.2f}\")\n",
    "    print(f\"Average Close: ${df['Close'].mean():.2f}\")\n",
    "    print(f\"Price Volatility: {df['Close'].std():.2f}\")\n",
    "\n",
    "# Plot the data\n",
    "plot_candlestick_data((X_train, y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
