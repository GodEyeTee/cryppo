{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54b4fcb6",
   "metadata": {},
   "source": [
    "# Multi-Asset Cryptocurrency Trading Bot with Enhanced Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb687c0",
   "metadata": {},
   "source": [
    "# Improvements:\n",
    "# - Multi-asset training (BTC, ETH, XRP)\n",
    "# - Date range: 2019-01-01 to present\n",
    "# - Enhanced risk management\n",
    "# - Better position sizing\n",
    "# - Improved win rate strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687353fb",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba6c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import requests\n",
    "from scipy import stats\n",
    "import talib\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98217457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a752ab",
   "metadata": {},
   "source": [
    "## 2. Enhanced Data Collection for Multiple Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_crypto_data_with_date(symbol='BTC/USDT', timeframe='1h', start_date='2019-01-01'):\n",
    "    \"\"\"Download crypto data from specific date to present - FIXED for full data\"\"\"\n",
    "    exchange = ccxt.binance({\n",
    "        'rateLimit': 1200,\n",
    "        'enableRateLimit': True,\n",
    "    })\n",
    "    \n",
    "    # Convert start date to timestamp\n",
    "    start_timestamp = exchange.parse8601(start_date + 'T00:00:00Z')\n",
    "    end_timestamp = exchange.milliseconds()\n",
    "    \n",
    "    print(f\"Downloading {symbol} {timeframe} data from {start_date}...\")\n",
    "    all_ohlcv = []\n",
    "    \n",
    "    current_timestamp = start_timestamp\n",
    "    batch_count = 0\n",
    "    \n",
    "    while current_timestamp < end_timestamp:\n",
    "        try:\n",
    "            # Fetch batch\n",
    "            ohlcv = exchange.fetch_ohlcv(symbol, timeframe, current_timestamp, limit=1000)\n",
    "            \n",
    "            if not ohlcv:\n",
    "                break\n",
    "                \n",
    "            # Add to results\n",
    "            all_ohlcv.extend(ohlcv)\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Update timestamp for next batch\n",
    "            last_timestamp = ohlcv[-1][0]\n",
    "            \n",
    "            # Check if we've reached the end\n",
    "            if last_timestamp >= end_timestamp or len(ohlcv) < 1000:\n",
    "                break\n",
    "                \n",
    "            # Move to next batch (add 1ms to avoid duplicate)\n",
    "            current_timestamp = last_timestamp + 1\n",
    "            \n",
    "            # Progress update\n",
    "            print(f\"Downloaded {len(all_ohlcv)} candles (batch {batch_count})...\", end='\\r')\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(exchange.rateLimit / 1000)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError downloading batch {batch_count}: {e}\")\n",
    "            time.sleep(5)\n",
    "            # Retry from last successful timestamp\n",
    "            if all_ohlcv:\n",
    "                current_timestamp = all_ohlcv[-1][0] + 1\n",
    "            else:\n",
    "                # If first batch failed, wait and retry\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df = df.drop_duplicates(subset=['timestamp']).reset_index(drop=True)\n",
    "    df['symbol'] = symbol\n",
    "    \n",
    "    # Calculate total days of data\n",
    "    if len(df) > 0:\n",
    "        days_of_data = (df['datetime'].max() - df['datetime'].min()).days\n",
    "        print(f\"\\nDownloaded {len(df)} candles for {symbol} ({days_of_data} days of data)\")\n",
    "    else:\n",
    "        print(f\"\\nNo data downloaded for {symbol}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad346218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_multiple_assets(symbols=['BTC/USDT', 'ETH/USDT', 'XRP/USDT'], \n",
    "                           timeframe='1h', start_date='2019-01-01'):\n",
    "    \"\"\"Download data for multiple cryptocurrency pairs\"\"\"\n",
    "    all_data = {}\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        try:\n",
    "            data = download_crypto_data_with_date(symbol, timeframe, start_date)\n",
    "            all_data[symbol] = data\n",
    "            time.sleep(2)  # Be nice to the API\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {symbol}: {e}\")\n",
    "            \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c7d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data for multiple assets\n",
    "print(\"Downloading multi-asset data...\")\n",
    "symbols = ['BTC/USDT', 'ETH/USDT', 'XRP/USDT']\n",
    "multi_asset_data = download_multiple_assets(symbols, timeframe='1h', start_date='2019-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670221a",
   "metadata": {},
   "source": [
    "## 3. Enhanced Feature Engineering for Multi-Asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e0222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advanced_features(df, asset_name='BTC'):\n",
    "    \"\"\"Calculate all features including crypto-specific ones with asset normalization\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # === Basic Price Features ===\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "    df['close_to_high'] = (df['high'] - df['close']) / df['high']\n",
    "    df['close_to_low'] = (df['close'] - df['low']) / df['low'].replace(0, 1e-8)\n",
    "    \n",
    "    # === Volatility Features ===\n",
    "    df['volatility_20'] = df['returns'].rolling(20).std()\n",
    "    df['volatility_50'] = df['returns'].rolling(50).std()\n",
    "    df['atr'] = talib.ATR(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "    df['natr'] = talib.NATR(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "    \n",
    "    # === Moving Averages & Trends ===\n",
    "    for period in [7, 14, 20, 50, 100, 200]:\n",
    "        df[f'sma_{period}'] = talib.SMA(df['close'], timeperiod=period)\n",
    "        df[f'ema_{period}'] = talib.EMA(df['close'], timeperiod=period)\n",
    "    \n",
    "    # Price position relative to MAs\n",
    "    df['price_to_sma20'] = (df['close'] - df['sma_20']) / df['sma_20']\n",
    "    df['price_to_sma50'] = (df['close'] - df['sma_50']) / df['sma_50']\n",
    "    \n",
    "    # MA Cross signals\n",
    "    df['ma_cross_bullish'] = ((df['sma_20'] > df['sma_50']) & \n",
    "                              (df['sma_20'].shift(1) <= df['sma_50'].shift(1))).astype(int)\n",
    "    df['ma_cross_bearish'] = ((df['sma_20'] < df['sma_50']) & \n",
    "                              (df['sma_20'].shift(1) >= df['sma_50'].shift(1))).astype(int)\n",
    "    \n",
    "    # Trend indicators\n",
    "    df['trend_20'] = np.where(df['close'] > df['close'].shift(20), 1, -1)\n",
    "    df['trend_50'] = np.where(df['close'] > df['close'].shift(50), 1, -1)\n",
    "    \n",
    "    # === Momentum Indicators ===\n",
    "    df['rsi'] = talib.RSI(df['close'], timeperiod=14)\n",
    "    df['rsi_ma'] = df['rsi'].rolling(10).mean()\n",
    "    \n",
    "    # Stochastic RSI\n",
    "    df['stochrsi_k'], df['stochrsi_d'] = talib.STOCHRSI(df['close'], \n",
    "                                                         timeperiod=14, \n",
    "                                                         fastk_period=3, \n",
    "                                                         fastd_period=3)\n",
    "    \n",
    "    # MACD\n",
    "    df['macd'], df['macd_signal'], df['macd_hist'] = talib.MACD(df['close'], \n",
    "                                                                 fastperiod=12, \n",
    "                                                                 slowperiod=26, \n",
    "                                                                 signalperiod=9)\n",
    "    \n",
    "    # === Bollinger Bands ===\n",
    "    df['bb_upper'], df['bb_middle'], df['bb_lower'] = talib.BBANDS(df['close'], \n",
    "                                                                    timeperiod=20, \n",
    "                                                                    nbdevup=2, \n",
    "                                                                    nbdevdn=2)\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']\n",
    "    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])\n",
    "    \n",
    "    # === Volume Analysis ===\n",
    "    df['volume_sma'] = df['volume'].rolling(20).mean()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_sma']\n",
    "    df['obv'] = talib.OBV(df['close'], df['volume'])\n",
    "    df['obv_ma'] = df['obv'].rolling(20).mean()\n",
    "    \n",
    "    # Volume-Weighted Average Price\n",
    "    df['vwap'] = (df['close'] * df['volume']).cumsum() / df['volume'].cumsum()\n",
    "    df['price_to_vwap'] = (df['close'] - df['vwap']) / df['vwap']\n",
    "    \n",
    "    # === Market Structure ===\n",
    "    # Support and Resistance levels\n",
    "    df['resistance'] = df['high'].rolling(20).max()\n",
    "    df['support'] = df['low'].rolling(20).min()\n",
    "    df['price_to_resistance'] = (df['close'] - df['resistance']) / df['resistance']\n",
    "    df['price_to_support'] = (df['close'] - df['support']) / df['support']\n",
    "    \n",
    "    # Higher Highs and Lower Lows\n",
    "    df['higher_high'] = (df['high'] > df['high'].rolling(20).max().shift(1)).astype(int)\n",
    "    df['lower_low'] = (df['low'] < df['low'].rolling(20).min().shift(1)).astype(int)\n",
    "    \n",
    "    # === Enhanced Features for Better Performance ===\n",
    "    # Momentum strength\n",
    "    df['momentum_strength'] = df['returns'].rolling(10).mean() / df['returns'].rolling(10).std()\n",
    "    \n",
    "    # Volume momentum\n",
    "    df['volume_momentum'] = df['volume'].rolling(10).mean() / df['volume'].rolling(50).mean()\n",
    "    \n",
    "    # Price acceleration\n",
    "    df['price_acceleration'] = df['returns'] - df['returns'].shift(1)\n",
    "    \n",
    "    # Support/Resistance strength\n",
    "    df['sr_strength'] = (df['resistance'] - df['support']) / df['close']\n",
    "    \n",
    "    # === Crypto-Specific Features ===\n",
    "    # Funding Rate simulation (in production, use real data)\n",
    "    # Different rates for different assets\n",
    "    base_rate = 0.0005 if asset_name == 'BTC' else 0.001\n",
    "    df['funding_rate'] = np.random.uniform(-base_rate, base_rate, len(df))\n",
    "    df['funding_rate_ma'] = df['funding_rate'].rolling(8).mean()\n",
    "    \n",
    "    # Extreme funding rate indicator\n",
    "    df['extreme_funding'] = (np.abs(df['funding_rate']) > base_rate).astype(int)\n",
    "    \n",
    "    # Market fear/greed proxy\n",
    "    df['fear_greed'] = (df['volatility_20'] / df['volatility_20'].rolling(100).mean()) * \\\n",
    "                       (df['volume_ratio'] / df['volume_ratio'].rolling(100).mean())\n",
    "    \n",
    "    # === Multi-Timeframe Features ===\n",
    "    df['returns_4h'] = df['close'].pct_change(4)\n",
    "    df['returns_24h'] = df['close'].pct_change(24)\n",
    "    df['high_24h'] = df['high'].rolling(24).max()\n",
    "    df['low_24h'] = df['low'].rolling(24).min()\n",
    "    df['range_24h'] = (df['high_24h'] - df['low_24h']) / df['close']\n",
    "    \n",
    "    # === Market Regime Detection ===\n",
    "    df['adx'] = talib.ADX(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "    df['trend_strength'] = df['adx'] / 100\n",
    "    \n",
    "    # Market regime\n",
    "    df['trending'] = (df['adx'] > 25).astype(int)\n",
    "    df['ranging'] = (df['adx'] < 20).astype(int)\n",
    "    \n",
    "    # === Asset-Specific Normalization ===\n",
    "    # Add asset identifier as feature\n",
    "    asset_map = {'BTC': 0, 'ETH': 1, 'XRP': 2}\n",
    "    df['asset_id'] = asset_map.get(asset_name.split('/')[0], 0)\n",
    "    \n",
    "    # Drop NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Replace any remaining NaN or inf values\n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering to all assets\n",
    "for symbol, data in multi_asset_data.items():\n",
    "    asset_name = symbol.split('/')[0]\n",
    "    multi_asset_data[symbol] = calculate_advanced_features(data, asset_name)\n",
    "    print(f\"Features calculated for {symbol}: {multi_asset_data[symbol].shape}\")\n",
    "\n",
    "# Combine all assets for training\n",
    "combined_data = pd.concat(list(multi_asset_data.values()), ignore_index=True)\n",
    "combined_data = combined_data.sort_values('datetime').reset_index(drop=True)\n",
    "print(f\"Combined data shape: {combined_data.shape}\")\n",
    "\n",
    "# Enhanced feature columns\n",
    "feature_columns = [\n",
    "    # Price action\n",
    "    'returns', 'log_returns', 'high_low_ratio', 'close_to_high', 'close_to_low',\n",
    "    # Volatility\n",
    "    'volatility_20', 'volatility_50', 'atr', 'natr',\n",
    "    # Trend\n",
    "    'price_to_sma20', 'price_to_sma50', 'ma_cross_bullish', 'ma_cross_bearish',\n",
    "    'trend_20', 'trend_50',\n",
    "    # Momentum\n",
    "    'rsi', 'rsi_ma', 'stochrsi_k', 'stochrsi_d', 'macd', 'macd_signal', 'macd_hist',\n",
    "    'momentum_strength',\n",
    "    # Bollinger Bands\n",
    "    'bb_width', 'bb_position',\n",
    "    # Volume\n",
    "    'volume_ratio', 'price_to_vwap', 'volume_momentum',\n",
    "    # Market structure\n",
    "    'price_to_resistance', 'price_to_support', 'higher_high', 'lower_low',\n",
    "    'sr_strength',\n",
    "    # Crypto-specific\n",
    "    'funding_rate', 'funding_rate_ma', 'extreme_funding', 'fear_greed',\n",
    "    # Multi-timeframe\n",
    "    'returns_4h', 'returns_24h', 'range_24h',\n",
    "    # Market regime\n",
    "    'adx', 'trend_strength', 'trending', 'ranging',\n",
    "    # Asset identifier\n",
    "    'asset_id',\n",
    "    # New features\n",
    "    'price_acceleration'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfee132",
   "metadata": {},
   "source": [
    "## 4. Enhanced XGBoost Direction Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedDirectionPredictor:\n",
    "    \"\"\"Enhanced XGBoost model with better feature engineering\"\"\"\n",
    "    \n",
    "    def __init__(self, lookback=24, prediction_horizon=4):\n",
    "        self.lookback = lookback\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.feature_importance = None\n",
    "        self.best_threshold = 0.5\n",
    "        \n",
    "    def prepare_data(self, df, feature_cols):\n",
    "        \"\"\"Prepare data for XGBoost with enhanced targets\"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(self.lookback, len(df) - self.prediction_horizon):\n",
    "            # Features: past lookback periods\n",
    "            X.append(df[feature_cols].iloc[i-self.lookback:i].values.flatten())\n",
    "            \n",
    "            # Enhanced target: consider both direction and magnitude\n",
    "            future_return = (df['close'].iloc[i + self.prediction_horizon] - \n",
    "                           df['close'].iloc[i]) / df['close'].iloc[i]\n",
    "            \n",
    "            # Use dynamic threshold based on volatility\n",
    "            vol = df['volatility_20'].iloc[i]\n",
    "            threshold = max(0.001, vol * 0.5)  # At least 0.1%, up to half volatility\n",
    "            \n",
    "            y.append(1 if future_return > threshold else 0)\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def train_ensemble(self, X_train, y_train, X_val, y_val, n_models=5):\n",
    "        \"\"\"Train ensemble with validation-based threshold selection\"\"\"\n",
    "        print(\"Training Enhanced XGBoost ensemble...\")\n",
    "        \n",
    "        # Find optimal threshold using validation set\n",
    "        best_score = 0\n",
    "        for thresh in [0.45, 0.5, 0.55, 0.6, 0.65]:\n",
    "            temp_score = 0\n",
    "            \n",
    "            for i in range(n_models):\n",
    "                params = {\n",
    "                    'objective': 'binary:logistic',\n",
    "                    'max_depth': np.random.randint(4, 10),\n",
    "                    'learning_rate': np.random.uniform(0.02, 0.15),\n",
    "                    'n_estimators': np.random.randint(150, 400),\n",
    "                    'subsample': np.random.uniform(0.7, 0.95),\n",
    "                    'colsample_bytree': np.random.uniform(0.7, 0.95),\n",
    "                    'min_child_weight': np.random.randint(1, 5),\n",
    "                    'random_state': i * 42,\n",
    "                    'use_label_encoder': False,\n",
    "                    'eval_metric': 'logloss'\n",
    "                }\n",
    "                \n",
    "                model = xgb.XGBClassifier(**params)\n",
    "                # Simple fit without early stopping for threshold selection\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "                pred = (pred_proba > thresh).astype(int)\n",
    "                temp_score += np.mean(pred == y_val)\n",
    "            \n",
    "            avg_score = temp_score / n_models\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                self.best_threshold = thresh\n",
    "        \n",
    "        print(f\"Best threshold: {self.best_threshold} with score: {best_score:.2%}\")\n",
    "        \n",
    "        # Train final ensemble with best parameters\n",
    "        for i in range(n_models):\n",
    "            print(f\"Training model {i+1}/{n_models}\")\n",
    "            \n",
    "            params = {\n",
    "                'objective': 'binary:logistic',\n",
    "                'max_depth': np.random.randint(4, 10),\n",
    "                'learning_rate': np.random.uniform(0.02, 0.15),\n",
    "                'n_estimators': np.random.randint(150, 400),\n",
    "                'subsample': np.random.uniform(0.7, 0.95),\n",
    "                'colsample_bytree': np.random.uniform(0.7, 0.95),\n",
    "                'min_child_weight': np.random.randint(1, 5),\n",
    "                'random_state': i * 42,\n",
    "                'use_label_encoder': False,\n",
    "                'eval_metric': 'logloss'\n",
    "            }\n",
    "            \n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            model.fit(X_train, y_train)\n",
    "            self.models[f'model_{i}'] = model\n",
    "        \n",
    "        # Get feature importance\n",
    "        self.feature_importance = self.models['model_0'].feature_importances_\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get ensemble predictions\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models.values():\n",
    "            pred = model.predict_proba(X)[:, 1]\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Weighted average with more weight on recent predictions\n",
    "        weights = np.linspace(0.8, 1.2, len(predictions))\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        weighted_pred = np.average(predictions, axis=0, weights=weights)\n",
    "        return weighted_pred\n",
    "    \n",
    "    def predict(self, X, custom_threshold=None):\n",
    "        \"\"\"Get binary predictions with dynamic threshold\"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        threshold = custom_threshold or self.best_threshold\n",
    "        \n",
    "        # Enhanced prediction logic\n",
    "        predictions = np.zeros(len(proba))\n",
    "        \n",
    "        # Strong signals only\n",
    "        predictions[proba > threshold] = 1  # Strong bullish\n",
    "        predictions[proba < (1 - threshold)] = -1  # Strong bearish\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7208f85b",
   "metadata": {},
   "source": [
    "## 5. Enhanced PPO Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a459652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedPPOAgent:\n",
    "    \"\"\"Enhanced PPO with better exploration and learning\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4):\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Networks\n",
    "        self.network = ActorCriticNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        # Enhanced PPO parameters\n",
    "        self.gamma = 0.995  # Increased for longer-term thinking\n",
    "        self.gae_lambda = 0.97\n",
    "        self.clip_epsilon = 0.2\n",
    "        self.value_coef = 0.5\n",
    "        self.entropy_coef = 0.02  # Increased for more exploration\n",
    "        self.max_grad_norm = 0.5\n",
    "        \n",
    "        # Adaptive learning rate\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='max', factor=0.5, patience=10\n",
    "        )\n",
    "        \n",
    "        # Memory\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.episode_returns = []\n",
    "        \n",
    "    def act(self, state, xgb_signal=None, valid_actions=None, explore_rate=0.1):\n",
    "        \"\"\"Select action with enhanced exploration\"\"\"\n",
    "        # Validate state\n",
    "        state = np.nan_to_num(state, 0.0)\n",
    "        state = np.clip(state, -10, 10)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_probs, value = self.network(state_tensor)\n",
    "        \n",
    "        # Ensure valid probabilities\n",
    "        action_probs = action_probs.cpu()\n",
    "        \n",
    "        if valid_actions is None:\n",
    "            valid_actions = list(range(self.action_dim))\n",
    "        \n",
    "        # Mask invalid actions\n",
    "        mask = torch.zeros(1, self.action_dim)\n",
    "        for action in valid_actions:\n",
    "            mask[0, action] = 1\n",
    "        \n",
    "        # Apply mask\n",
    "        action_probs = action_probs * mask\n",
    "        \n",
    "        # Renormalize\n",
    "        if action_probs.sum() > 0:\n",
    "            action_probs = action_probs / action_probs.sum(dim=-1, keepdim=True)\n",
    "        else:\n",
    "            action_probs = mask / mask.sum()\n",
    "        \n",
    "        # Enhanced XGBoost signal incorporation\n",
    "        if xgb_signal is not None and not np.isnan(xgb_signal):\n",
    "            signal_strength = min(abs(xgb_signal), 1.0)  # Cap signal strength\n",
    "            \n",
    "            if xgb_signal > 0 and 1 in valid_actions:  # Bullish\n",
    "                action_probs[0, 1] *= (1 + 0.5 * signal_strength)\n",
    "                if 2 in valid_actions:\n",
    "                    action_probs[0, 2] *= (1 - 0.3 * signal_strength)\n",
    "            elif xgb_signal < 0 and 2 in valid_actions:  # Bearish\n",
    "                if 1 in valid_actions:\n",
    "                    action_probs[0, 1] *= (1 - 0.3 * signal_strength)\n",
    "                action_probs[0, 2] *= (1 + 0.5 * signal_strength)\n",
    "            \n",
    "            # Renormalize\n",
    "            action_probs = action_probs / action_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Exploration vs exploitation\n",
    "        if np.random.random() < explore_rate:\n",
    "            # Random valid action\n",
    "            action = np.random.choice(valid_actions)\n",
    "        else:\n",
    "            # Sample from distribution\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample().item()\n",
    "        \n",
    "        # Store for training\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        if value.dim() == 0:\n",
    "            self.values.append(value.cpu().item())\n",
    "        else:\n",
    "            self.values.append(value[0].cpu().item() if len(value) > 0 else 0.0)\n",
    "        \n",
    "        # Calculate log prob for the taken action\n",
    "        dist = Categorical(action_probs)\n",
    "        self.log_probs.append(dist.log_prob(torch.tensor(action)).item())\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update(self, epochs=10):\n",
    "        \"\"\"Update with adaptive learning\"\"\"\n",
    "        if len(self.states) < 64:  # Increased minimum batch\n",
    "            return\n",
    "        \n",
    "        # Compute advantages and returns\n",
    "        advantages, returns = self.compute_gae()\n",
    "        \n",
    "        if advantages.numel() == 0:\n",
    "            return\n",
    "        \n",
    "        # Calculate average return for scheduler\n",
    "        avg_return = np.mean(self.rewards[-100:]) if len(self.rewards) >= 100 else 0\n",
    "        self.scheduler.step(avg_return)\n",
    "        \n",
    "        # Rest of update logic remains the same...\n",
    "        # [Previous update code here]\n",
    "        \n",
    "    def compute_gae(self):\n",
    "        \"\"\"Compute Generalized Advantage Estimation\"\"\"\n",
    "        # Ensure all lists have same length\n",
    "        min_length = min(len(self.rewards), len(self.values), len(self.dones))\n",
    "        \n",
    "        if min_length == 0:\n",
    "            return torch.FloatTensor([]).to(device), torch.FloatTensor([]).to(device)\n",
    "        \n",
    "        # Trim to same length\n",
    "        rewards = self.rewards[:min_length]\n",
    "        values = self.values[:min_length]\n",
    "        dones = self.dones[:min_length]\n",
    "        \n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "            advantage = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * advantage\n",
    "            advantages.insert(0, advantage)\n",
    "        \n",
    "        advantages = torch.FloatTensor(advantages).to(device)\n",
    "        returns = advantages + torch.FloatTensor(values).to(device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        if advantages.numel() > 0:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        return advantages, returns\n",
    "    \n",
    "    def store_reward(self, reward, done):\n",
    "        \"\"\"Store reward and done flag\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    \"\"\"Enhanced Actor-Critic network\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=512):  # Larger network\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared layers with more capacity\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        self.ln3 = nn.LayerNorm(hidden_dim // 2)\n",
    "        \n",
    "        # Actor head\n",
    "        self.actor_fc = nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "        self.actor_out = nn.Linear(hidden_dim // 4, action_dim)\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic_fc = nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "        self.critic_out = nn.Linear(hidden_dim // 4, 1)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights properly\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        # Check for NaN in input\n",
    "        if torch.isnan(state).any():\n",
    "            state = torch.nan_to_num(state, 0.0)\n",
    "        \n",
    "        # Shared network with residual connection\n",
    "        x = F.relu(self.ln1(self.fc1(state)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x2 = F.relu(self.ln2(self.fc2(x)))\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        x3 = F.relu(self.ln3(self.fc3(x2)))\n",
    "        x3 = self.dropout(x3)\n",
    "        \n",
    "        # Actor\n",
    "        actor = F.relu(self.actor_fc(x3))\n",
    "        action_logits = self.actor_out(actor)\n",
    "        action_probs = F.softmax(action_logits, dim=-1) + 1e-8\n",
    "        \n",
    "        # Critic\n",
    "        critic = F.relu(self.critic_fc(x3))\n",
    "        value = self.critic_out(critic).squeeze(-1)\n",
    "        \n",
    "        return action_probs, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab96c03",
   "metadata": {},
   "source": [
    "## 6. Enhanced Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53fcc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCryptoFuturesEnv:\n",
    "    \"\"\"Enhanced trading environment with better risk management\"\"\"\n",
    "    \n",
    "    def __init__(self, data, initial_balance=10000, fee_rate=0.0005, \n",
    "                 max_leverage=10, liquidation_margin=0.05):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.initial_balance = initial_balance\n",
    "        self.fee_rate = fee_rate\n",
    "        self.max_leverage = max_leverage\n",
    "        self.liquidation_margin = liquidation_margin\n",
    "        \n",
    "        # Enhanced position sizing\n",
    "        self.min_position_pct = 0.01   # 1% minimum\n",
    "        self.max_position_pct = 0.15    # 15% maximum  \n",
    "        self.base_position_pct = 0.03  # 3% base\n",
    "        \n",
    "        # Tighter risk management\n",
    "        self.stop_loss_pct = 0.015     # 1.5% stop loss\n",
    "        self.take_profit_pct = 0.025   # 2.5% take profit\n",
    "        self.trailing_stop_pct = 0.01  # 1% trailing stop\n",
    "        self.max_drawdown_pct = 0.15   # 15% max drawdown\n",
    "        \n",
    "        # Position management\n",
    "        self.max_positions = 1  # Focus on quality over quantity\n",
    "        self.position_timeout = 48  # Max hours in position\n",
    "        \n",
    "        # Features\n",
    "        self.feature_columns = feature_columns\n",
    "        self.action_space = 4  # HOLD, LONG, SHORT, EXIT\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.high_water_mark = initial_balance\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _calculate_position_size(self, action):\n",
    "        \"\"\"Enhanced position sizing based on market conditions\"\"\"\n",
    "        # Base size\n",
    "        size_pct = self.base_position_pct\n",
    "        \n",
    "        # Get current market conditions\n",
    "        current_vol = self.data['volatility_20'].iloc[self.current_step]\n",
    "        avg_vol = self.data['volatility_20'].rolling(100).mean().iloc[self.current_step]\n",
    "        rsi = self.data['rsi'].iloc[self.current_step]\n",
    "        trend_strength = self.data['trend_strength'].iloc[self.current_step]\n",
    "        \n",
    "        # Volatility adjustment\n",
    "        if pd.notna(current_vol) and pd.notna(avg_vol) and current_vol > 0 and avg_vol > 0:\n",
    "            vol_ratio = avg_vol / current_vol\n",
    "            vol_adjustment = np.clip(vol_ratio, 0.5, 1.5)\n",
    "            size_pct *= vol_adjustment\n",
    "        \n",
    "        # RSI extremes adjustment\n",
    "        if rsi < 25 or rsi > 75:\n",
    "            size_pct *= 1.2  # Increase size at extremes\n",
    "        elif 40 < rsi < 60:\n",
    "            size_pct *= 0.8  # Decrease size in neutral zone\n",
    "        \n",
    "        # Trend strength adjustment\n",
    "        if trend_strength > 0.4:\n",
    "            size_pct *= 1.2\n",
    "        elif trend_strength < 0.2:\n",
    "            size_pct *= 0.8\n",
    "        \n",
    "        # Win/loss streak adjustment\n",
    "        if self.winning_streak >= 2:\n",
    "            size_pct *= 1.1\n",
    "        elif self.consecutive_losses >= 2:\n",
    "            size_pct *= 0.7\n",
    "        \n",
    "        # Clamp to limits\n",
    "        size_pct = np.clip(size_pct, self.min_position_pct, self.max_position_pct)\n",
    "        \n",
    "        # Dynamic leverage based on confidence\n",
    "        adx = self.data['adx'].iloc[self.current_step]\n",
    "        if pd.notna(adx):\n",
    "            if adx > 40 and trend_strength > 0.5:  # Very strong trend\n",
    "                self.leverage = min(5, self.max_leverage)\n",
    "            elif adx > 25:  # Moderate trend\n",
    "                self.leverage = 3\n",
    "            else:  # Weak trend\n",
    "                self.leverage = 2\n",
    "        else:\n",
    "            self.leverage = 2\n",
    "        \n",
    "        return self.balance * size_pct\n",
    "    \n",
    "    def _check_exit_conditions(self):\n",
    "        \"\"\"Enhanced exit conditions with trailing stop\"\"\"\n",
    "        if self.position is None:\n",
    "            return False, None\n",
    "        \n",
    "        current_price = self.data['close'].iloc[self.current_step]\n",
    "        \n",
    "        # Calculate P&L\n",
    "        if self.position == 'long':\n",
    "            pnl_pct = (current_price - self.entry_price) / self.entry_price\n",
    "        else:\n",
    "            pnl_pct = (self.entry_price - current_price) / self.entry_price\n",
    "        \n",
    "        # Update trailing stop\n",
    "        if hasattr(self, 'trailing_stop_price'):\n",
    "            if self.position == 'long':\n",
    "                self.trailing_stop_price = max(\n",
    "                    self.trailing_stop_price,\n",
    "                    current_price * (1 - self.trailing_stop_pct)\n",
    "                )\n",
    "                if current_price <= self.trailing_stop_price:\n",
    "                    return True, 'trailing_stop'\n",
    "            else:  # short\n",
    "                self.trailing_stop_price = min(\n",
    "                    self.trailing_stop_price,\n",
    "                    current_price * (1 + self.trailing_stop_pct)\n",
    "                )\n",
    "                if current_price >= self.trailing_stop_price:\n",
    "                    return True, 'trailing_stop'\n",
    "        \n",
    "        # Check liquidation\n",
    "        leveraged_pnl = pnl_pct * self.leverage\n",
    "        if leveraged_pnl <= -self.liquidation_margin:\n",
    "            return True, 'liquidation'\n",
    "        \n",
    "        # Check stop loss\n",
    "        if pnl_pct <= -self.stop_loss_pct:\n",
    "            return True, 'stop_loss'\n",
    "        \n",
    "        # Check take profit\n",
    "        if pnl_pct >= self.take_profit_pct:\n",
    "            # Initialize trailing stop\n",
    "            if not hasattr(self, 'trailing_stop_price'):\n",
    "                if self.position == 'long':\n",
    "                    self.trailing_stop_price = current_price * (1 - self.trailing_stop_pct)\n",
    "                else:\n",
    "                    self.trailing_stop_price = current_price * (1 + self.trailing_stop_pct)\n",
    "            return False, None  # Don't exit immediately, let trailing stop work\n",
    "        \n",
    "        # Check timeout\n",
    "        if self.current_step - self.entry_step > self.position_timeout:\n",
    "            return True, 'timeout'\n",
    "        \n",
    "        return False, None\n",
    "    \n",
    "    def _execute_action(self, action, exit_reason=None):\n",
    "        \"\"\"Execute trading action with enhanced rewards - FIXED P&L calculation\"\"\"\n",
    "        current_price = self.data['close'].iloc[self.current_step]\n",
    "        reward = 0\n",
    "        \n",
    "        # Double check valid actions\n",
    "        valid_actions = self.get_valid_actions()\n",
    "        if action not in valid_actions:\n",
    "            return -0.1  # Invalid action penalty\n",
    "        \n",
    "        if action == 0:  # HOLD\n",
    "            if self.position is None:\n",
    "                reward = -0.0005  # Small penalty for not trading\n",
    "            else:\n",
    "                # Reward for holding profitable positions\n",
    "                if self.position == 'long':\n",
    "                    pnl_pct = (current_price - self.entry_price) / self.entry_price\n",
    "                else:\n",
    "                    pnl_pct = (self.entry_price - current_price) / self.entry_price\n",
    "                \n",
    "                # Progressive rewards for holding winners\n",
    "                if pnl_pct > 0.02:  # 2% profit\n",
    "                    reward = 0.05\n",
    "                elif pnl_pct > 0.01:  # 1% profit\n",
    "                    reward = 0.02\n",
    "                elif pnl_pct > 0:\n",
    "                    reward = 0.01\n",
    "                else:\n",
    "                    reward = -0.002  # Penalty for holding losers\n",
    "        \n",
    "        elif action in [1, 2] and self.position is None:  # Open position\n",
    "            # Calculate position size\n",
    "            self.position_size = self._calculate_position_size(action)\n",
    "            \n",
    "            # Pay fees\n",
    "            fee = self.position_size * self.fee_rate\n",
    "            \n",
    "            # Check if we have enough balance\n",
    "            if self.position_size + fee <= self.balance:\n",
    "                self.balance -= fee  # Only deduct fee, not position size\n",
    "                self.position = 'long' if action == 1 else 'short'\n",
    "                self.entry_price = current_price\n",
    "                self.entry_step = self.current_step\n",
    "                \n",
    "                # Reset trailing stop\n",
    "                if hasattr(self, 'trailing_stop_price'):\n",
    "                    delattr(self, 'trailing_stop_price')\n",
    "                \n",
    "                # Enhanced entry rewards based on technical conditions\n",
    "                reward = 0.01  # Base reward\n",
    "                \n",
    "                rsi = self.data['rsi'].iloc[self.current_step]\n",
    "                bb_position = self.data['bb_position'].iloc[self.current_step]\n",
    "                trend = self.data['trend_20'].iloc[self.current_step]\n",
    "                \n",
    "                if action == 1:  # LONG\n",
    "                    if rsi < 30:  # Oversold\n",
    "                        reward += 0.02\n",
    "                    if bb_position < 0.2:  # Near lower band\n",
    "                        reward += 0.01\n",
    "                    if trend > 0:  # Following trend\n",
    "                        reward += 0.01\n",
    "                else:  # SHORT\n",
    "                    if rsi > 70:  # Overbought\n",
    "                        reward += 0.02\n",
    "                    if bb_position > 0.8:  # Near upper band\n",
    "                        reward += 0.01\n",
    "                    if trend < 0:  # Following trend\n",
    "                        reward += 0.01\n",
    "        \n",
    "        elif action == 3 and self.position is not None:  # Exit position\n",
    "            # Calculate P&L percentage\n",
    "            if self.position == 'long':\n",
    "                pnl_pct = (current_price - self.entry_price) / self.entry_price\n",
    "            else:\n",
    "                pnl_pct = (self.entry_price - current_price) / self.entry_price\n",
    "            \n",
    "            # Apply leverage - FIXED calculation\n",
    "            leveraged_pnl_pct = pnl_pct * self.leverage\n",
    "            \n",
    "            # Calculate P&L amount correctly\n",
    "            pnl_amount = self.position_size * pnl_pct * self.leverage\n",
    "            \n",
    "            # Exit fee\n",
    "            exit_fee = self.position_size * self.fee_rate\n",
    "            \n",
    "            # Net P&L after fees\n",
    "            net_pnl = pnl_amount - exit_fee\n",
    "            \n",
    "            # Update balance - just add the net P&L\n",
    "            self.balance += net_pnl\n",
    "            \n",
    "            # Update streaks\n",
    "            if net_pnl > 0:\n",
    "                self.winning_streak += 1\n",
    "                self.consecutive_losses = 0\n",
    "            else:\n",
    "                self.consecutive_losses += 1\n",
    "                self.winning_streak = 0\n",
    "            \n",
    "            # Record trade\n",
    "            self.trades.append({\n",
    "                'entry_price': self.entry_price,\n",
    "                'exit_price': current_price,\n",
    "                'position': self.position,\n",
    "                'pnl': net_pnl,\n",
    "                'pnl_pct': pnl_pct,\n",
    "                'leveraged_pnl_pct': leveraged_pnl_pct,\n",
    "                'exit_reason': exit_reason,\n",
    "                'leverage': self.leverage,\n",
    "                'entry_step': self.entry_step,\n",
    "                'exit_step': self.current_step,\n",
    "                'bars_held': self.current_step - self.entry_step\n",
    "            })\n",
    "            \n",
    "            # Enhanced exit rewards\n",
    "            if exit_reason == 'liquidation':\n",
    "                reward = -2.0  # Severe penalty\n",
    "            elif exit_reason == 'stop_loss':\n",
    "                reward = -0.5  # Moderate penalty\n",
    "            elif exit_reason == 'trailing_stop':\n",
    "                reward = 0.5 + min(leveraged_pnl_pct * 5, 1.0)  # Good exit\n",
    "            elif exit_reason == 'take_profit':\n",
    "                reward = 1.0 + min(leveraged_pnl_pct * 5, 1.0)  # Great exit\n",
    "            elif exit_reason == 'timeout':\n",
    "                reward = net_pnl / self.initial_balance * 10  # Based on P&L\n",
    "            else:\n",
    "                # Manual exit - reward based on P&L and holding time\n",
    "                time_factor = min(self.current_step - self.entry_step, 24) / 24\n",
    "                reward = np.tanh(leveraged_pnl_pct * 20) * (1 + time_factor * 0.5)\n",
    "            \n",
    "            # Reset position\n",
    "            self.position = None\n",
    "            self.position_size = 0\n",
    "            self.leverage = 1\n",
    "            if hasattr(self, 'trailing_stop_price'):\n",
    "                delattr(self, 'trailing_stop_price')\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    # Other methods remain the same but inherit from parent class\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment\"\"\"\n",
    "        self.current_step = max(200, int(0.1 * len(self.data)))\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = None\n",
    "        self.position_size = 0\n",
    "        self.entry_price = 0\n",
    "        self.entry_step = 0\n",
    "        self.leverage = 1\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.trades = []\n",
    "        self.equity_curve = [self.balance]\n",
    "        self.peak_equity = self.balance\n",
    "        self.consecutive_losses = 0\n",
    "        self.winning_streak = 0\n",
    "        \n",
    "        return self._get_observation()\n",
    "    \n",
    "    def _calculate_equity(self):\n",
    "        \"\"\"Calculate current account equity\"\"\"\n",
    "        if self.position is None:\n",
    "            return self.balance\n",
    "        \n",
    "        current_price = self.data['close'].iloc[self.current_step]\n",
    "        \n",
    "        # Calculate P&L percentage\n",
    "        if self.position == 'long':\n",
    "            pnl_pct = (current_price - self.entry_price) / self.entry_price\n",
    "        else:\n",
    "            pnl_pct = (self.entry_price - current_price) / self.entry_price\n",
    "        \n",
    "        # Apply leverage to P&L percentage\n",
    "        leveraged_pnl_pct = pnl_pct * self.leverage\n",
    "        \n",
    "        # Calculate unrealized P&L\n",
    "        unrealized_pnl = self.position_size * leveraged_pnl_pct\n",
    "        \n",
    "        # Just add unrealized P&L to current balance\n",
    "        return self.balance + unrealized_pnl\n",
    "    \n",
    "    def _calculate_win_rate(self):\n",
    "        \"\"\"Calculate win rate of closed trades\"\"\"\n",
    "        if not self.trades:\n",
    "            return 0.5\n",
    "        \n",
    "        wins = sum(1 for t in self.trades if t.get('pnl', 0) > 0)\n",
    "        total = sum(1 for t in self.trades if 'pnl' in t)\n",
    "        \n",
    "        if total == 0:\n",
    "            return 0.5\n",
    "        \n",
    "        return wins / total\n",
    "    \n",
    "    def _calculate_avg_win_loss_ratio(self):\n",
    "        \"\"\"Calculate average win/loss ratio\"\"\"\n",
    "        if not self.trades:\n",
    "            return 1.0\n",
    "        \n",
    "        wins = [t['pnl'] for t in self.trades if t.get('pnl', 0) > 0]\n",
    "        losses = [abs(t['pnl']) for t in self.trades if t.get('pnl', 0) < 0]\n",
    "        \n",
    "        avg_win = np.mean(wins) if wins else 0\n",
    "        avg_loss = np.mean(losses) if losses else 1\n",
    "        \n",
    "        return avg_win / max(avg_loss, 1e-6)\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        \"\"\"Get list of valid actions based on current position\"\"\"\n",
    "        if self.position is None:\n",
    "            # No position: can HOLD, LONG, or SHORT\n",
    "            return [0, 1, 2]\n",
    "        else:\n",
    "            # Have position: can only HOLD or EXIT\n",
    "            return [0, 3]\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get current state observation\"\"\"\n",
    "        # Market features\n",
    "        market_features = self.data[self.feature_columns].iloc[self.current_step].values\n",
    "        \n",
    "        # Replace NaN and inf values\n",
    "        market_features = np.nan_to_num(market_features, 0.0)\n",
    "        market_features = np.clip(market_features, -5, 5)\n",
    "        \n",
    "        # Position features\n",
    "        position_features = self._get_position_features()\n",
    "        \n",
    "        # Account features\n",
    "        account_features = self._get_account_features()\n",
    "        \n",
    "        # Concatenate all features\n",
    "        observation = np.concatenate([market_features, position_features, account_features])\n",
    "        \n",
    "        # Final validation\n",
    "        observation = np.nan_to_num(observation, 0.0)\n",
    "        observation = np.clip(observation, -10, 10)\n",
    "        \n",
    "        return observation.astype(np.float32)\n",
    "    \n",
    "    def _get_position_features(self):\n",
    "        \"\"\"Get position-specific features\"\"\"\n",
    "        if self.position is None:\n",
    "            return np.zeros(10)\n",
    "        \n",
    "        current_price = self.data['close'].iloc[self.current_step]\n",
    "        \n",
    "        # Basic position info\n",
    "        is_long = float(self.position == 'long')\n",
    "        is_short = float(self.position == 'short')\n",
    "        \n",
    "        # P&L calculation\n",
    "        if self.position == 'long':\n",
    "            pnl_pct = (current_price - self.entry_price) / self.entry_price\n",
    "        else:\n",
    "            pnl_pct = (self.entry_price - current_price) / self.entry_price\n",
    "        \n",
    "        # Leveraged P&L\n",
    "        leveraged_pnl = pnl_pct * self.leverage\n",
    "        \n",
    "        # Time in position\n",
    "        bars_in_position = self.current_step - self.entry_step\n",
    "        \n",
    "        # Distance to liquidation\n",
    "        liquidation_distance = abs(leveraged_pnl - self.liquidation_margin)\n",
    "        \n",
    "        # Distance to stop loss/take profit\n",
    "        distance_to_sl = abs(pnl_pct - (-self.stop_loss_pct))\n",
    "        distance_to_tp = abs(pnl_pct - self.take_profit_pct)\n",
    "        \n",
    "        return np.array([\n",
    "            is_long,\n",
    "            is_short,\n",
    "            self.position_size / self.initial_balance,\n",
    "            pnl_pct,\n",
    "            leveraged_pnl,\n",
    "            bars_in_position / 100,\n",
    "            self.leverage / self.max_leverage,\n",
    "            liquidation_distance,\n",
    "            distance_to_sl,\n",
    "            distance_to_tp\n",
    "        ])\n",
    "    \n",
    "    def _get_account_features(self):\n",
    "        \"\"\"Get account-specific features\"\"\"\n",
    "        current_equity = self._calculate_equity()\n",
    "        \n",
    "        # Performance metrics\n",
    "        total_return = (current_equity - self.initial_balance) / self.initial_balance\n",
    "        \n",
    "        # Prevent division by zero\n",
    "        if self.peak_equity > 0:\n",
    "            current_drawdown = (self.peak_equity - current_equity) / self.peak_equity\n",
    "        else:\n",
    "            current_drawdown = 0\n",
    "        \n",
    "        # Trade statistics\n",
    "        win_rate = self._calculate_win_rate()\n",
    "        avg_win_loss = self._calculate_avg_win_loss_ratio()\n",
    "        \n",
    "        # Streaks\n",
    "        normalized_winning_streak = min(self.winning_streak / 5, 1)\n",
    "        normalized_losing_streak = min(self.consecutive_losses / 5, 1)\n",
    "        \n",
    "        features = np.array([\n",
    "            self.balance / self.initial_balance,\n",
    "            current_equity / self.initial_balance,\n",
    "            total_return,\n",
    "            current_drawdown,\n",
    "            win_rate,\n",
    "            avg_win_loss,\n",
    "            normalized_winning_streak,\n",
    "            normalized_losing_streak\n",
    "        ])\n",
    "        \n",
    "        # Validate features\n",
    "        features = np.nan_to_num(features, 0.0)\n",
    "        features = np.clip(features, -10, 10)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute trading action\"\"\"\n",
    "        # Validate action\n",
    "        valid_actions = self.get_valid_actions()\n",
    "        if action not in valid_actions:\n",
    "            # Invalid action penalty\n",
    "            reward = -0.1\n",
    "            self.current_step += 1\n",
    "            done = self.current_step >= len(self.data) - 1\n",
    "            next_obs = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "            \n",
    "            info = {\n",
    "                'equity': self._calculate_equity(),\n",
    "                'position': self.position,\n",
    "                'trades': len(self.trades),\n",
    "                'win_rate': self._calculate_win_rate(),\n",
    "                'invalid_action': True\n",
    "            }\n",
    "            \n",
    "            return next_obs, reward, done, info\n",
    "        \n",
    "        current_price = self.data['close'].iloc[self.current_step]\n",
    "        \n",
    "        # Check exit conditions first\n",
    "        should_exit, exit_reason = self._check_exit_conditions()\n",
    "        if should_exit:\n",
    "            action = 3  # Force exit\n",
    "        \n",
    "        # Execute action\n",
    "        reward = self._execute_action(action, exit_reason)\n",
    "        \n",
    "        # Update equity tracking\n",
    "        current_equity = self._calculate_equity()\n",
    "        self.equity_curve.append(current_equity)\n",
    "        self.peak_equity = max(self.peak_equity, current_equity)\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = (self.current_step >= len(self.data) - 1 or \n",
    "                current_equity <= self.initial_balance * 0.5 or  # 50% loss\n",
    "                (self.peak_equity - current_equity) / self.peak_equity > self.max_drawdown_pct)\n",
    "        \n",
    "        # Get next observation\n",
    "        if not done:\n",
    "            next_obs = self._get_observation()\n",
    "        else:\n",
    "            next_obs = np.zeros_like(self._get_observation())\n",
    "        \n",
    "        info = {\n",
    "            'equity': current_equity,\n",
    "            'position': self.position,\n",
    "            'trades': len(self.trades),\n",
    "            'win_rate': self._calculate_win_rate()\n",
    "        }\n",
    "        \n",
    "        return next_obs, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd93b80",
   "metadata": {},
   "source": [
    "## 7. Enhanced Training System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80390696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enhanced_system(combined_data, episodes=500, save_models=True):\n",
    "    \"\"\"Train the enhanced multi-asset system\"\"\"\n",
    "    \n",
    "    # Validate data\n",
    "    print(\"Validating data...\")\n",
    "    if combined_data.isnull().any().any():\n",
    "        print(\"Warning: Found NaN values in data, cleaning...\")\n",
    "        combined_data = combined_data.ffill().bfill().fillna(0)\n",
    "    \n",
    "    # Check data range\n",
    "    print(f\"Data range: {combined_data['datetime'].min()} to {combined_data['datetime'].max()}\")\n",
    "    print(f\"Total samples: {len(combined_data)}\")\n",
    "    \n",
    "    # Split by time - ensure we have enough data\n",
    "    total_days = (combined_data['datetime'].max() - combined_data['datetime'].min()).days\n",
    "    \n",
    "    if total_days < 90:\n",
    "        print(f\"Warning: Only {total_days} days of data available. Need more data for proper training.\")\n",
    "        # Use 70/30 split for limited data\n",
    "        train_size = int(0.7 * len(combined_data))\n",
    "        train_data = combined_data[:train_size].copy()\n",
    "        test_data = combined_data[train_size:].copy()\n",
    "    else:\n",
    "        # Use date-based split for sufficient data\n",
    "        split_date = combined_data['datetime'].max() - pd.Timedelta(days=30)\n",
    "        train_data = combined_data[combined_data['datetime'] < split_date].copy()\n",
    "        test_data = combined_data[combined_data['datetime'] >= split_date].copy()\n",
    "    \n",
    "    # Ensure we have data\n",
    "    if len(train_data) < 1000:\n",
    "        print(\"Error: Not enough training data. Please download more historical data.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"Train data: {len(train_data)} samples ({train_data['datetime'].min()} to {train_data['datetime'].max()})\")\n",
    "    print(f\"Test data: {len(test_data)} samples ({test_data['datetime'].min() if len(test_data) > 0 else 'None'} to {test_data['datetime'].max() if len(test_data) > 0 else 'None'})\")\n",
    "    \n",
    "    # === Step 1: Train Enhanced XGBoost ===\n",
    "    print(\"\\n=== Training Enhanced XGBoost Direction Predictor ===\")\n",
    "    \n",
    "    xgb_predictor = EnhancedDirectionPredictor(lookback=24, prediction_horizon=4)\n",
    "    \n",
    "    # Prepare XGBoost data\n",
    "    X, y = xgb_predictor.prepare_data(train_data, feature_columns)\n",
    "    \n",
    "    if len(X) < 100:\n",
    "        print(\"Error: Not enough samples for XGBoost training\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Train-validation split\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(f\"XGBoost train samples: {len(X_train)}\")\n",
    "    print(f\"XGBoost validation samples: {len(X_val)}\")\n",
    "    \n",
    "    # Train ensemble with validation\n",
    "    xgb_predictor.train_ensemble(X_train, y_train, X_val, y_val, n_models=7)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_predictions = xgb_predictor.predict(X_val)\n",
    "    val_accuracy = np.mean((val_predictions != 0))  # Non-zero predictions\n",
    "    print(f\"XGBoost validation signal rate: {val_accuracy:.2%}\")\n",
    "    \n",
    "    # === Step 2: Initialize Enhanced PPO Agent ===\n",
    "    print(\"\\n=== Initializing Enhanced PPO Agent ===\")\n",
    "    \n",
    "    # Create environment\n",
    "    train_env = EnhancedCryptoFuturesEnv(train_data)\n",
    "    \n",
    "    # Get state dimensions\n",
    "    initial_state = train_env.reset()\n",
    "    state_dim = initial_state.shape[0]\n",
    "    action_dim = train_env.action_space\n",
    "    \n",
    "    print(f\"State dimension: {state_dim}\")\n",
    "    print(f\"Action dimension: {action_dim}\")\n",
    "    \n",
    "    # Initialize PPO agent\n",
    "    ppo_agent = EnhancedPPOAgent(state_dim, action_dim, lr=3e-4)\n",
    "    \n",
    "    # === Step 3: Train PPO with Curriculum Learning ===\n",
    "    print(\"\\n=== Training Enhanced PPO Agent ===\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_profits = []\n",
    "    episode_trades = []\n",
    "    episode_win_rates = []\n",
    "    episode_sharpe_ratios = []\n",
    "    \n",
    "    best_sharpe = -np.inf\n",
    "    best_profit = -np.inf\n",
    "    patience = 0\n",
    "    max_patience = 75\n",
    "    \n",
    "    # Curriculum learning parameters\n",
    "    explore_rate = 0.3  # Start with high exploration\n",
    "    explore_decay = 0.995\n",
    "    min_explore = 0.05\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = train_env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Prepare XGBoost features for the episode\n",
    "        if train_env.current_step >= xgb_predictor.lookback:\n",
    "            xgb_features = []\n",
    "            for i in range(len(train_data) - train_env.current_step):\n",
    "                if train_env.current_step + i >= xgb_predictor.lookback:\n",
    "                    feat = train_data[feature_columns].iloc[\n",
    "                        train_env.current_step + i - xgb_predictor.lookback:\n",
    "                        train_env.current_step + i\n",
    "                    ].values.flatten()\n",
    "                    xgb_features.append(feat)\n",
    "            \n",
    "            if xgb_features:\n",
    "                xgb_features = np.array(xgb_features)\n",
    "                xgb_signals = xgb_predictor.predict(xgb_features)\n",
    "            else:\n",
    "                xgb_signals = None\n",
    "        else:\n",
    "            xgb_signals = None\n",
    "        \n",
    "        # Episode loop\n",
    "        step_count = 0\n",
    "        done = False\n",
    "        episode_returns = []\n",
    "        \n",
    "        while not done:\n",
    "            # Get XGBoost signal for current step\n",
    "            if xgb_signals is not None and step_count < len(xgb_signals):\n",
    "                current_xgb_signal = xgb_signals[step_count]\n",
    "            else:\n",
    "                current_xgb_signal = None\n",
    "            \n",
    "            # Get valid actions\n",
    "            valid_actions = train_env.get_valid_actions()\n",
    "            \n",
    "            # Select action with exploration\n",
    "            action = ppo_agent.act(state, current_xgb_signal, valid_actions, explore_rate)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done, info = train_env.step(action)\n",
    "            \n",
    "            # Store reward and done\n",
    "            ppo_agent.store_reward(reward, done)\n",
    "            \n",
    "            # Track returns for Sharpe ratio\n",
    "            if len(train_env.equity_curve) > 1:\n",
    "                ret = (train_env.equity_curve[-1] - train_env.equity_curve[-2]) / train_env.equity_curve[-2]\n",
    "                episode_returns.append(ret)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update PPO\n",
    "        ppo_agent.update(epochs=10)\n",
    "        \n",
    "        # Calculate episode metrics\n",
    "        final_equity = info['equity']\n",
    "        episode_profit = (final_equity / train_env.initial_balance - 1) * 100\n",
    "        \n",
    "        # Calculate Sharpe ratio\n",
    "        if len(episode_returns) > 1:\n",
    "            returns_std = np.std(episode_returns)\n",
    "            if returns_std > 0:\n",
    "                sharpe = np.sqrt(252 * 24) * np.mean(episode_returns) / returns_std  # Hourly to annual\n",
    "            else:\n",
    "                sharpe = 0\n",
    "        else:\n",
    "            sharpe = 0\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_profits.append(episode_profit)\n",
    "        episode_trades.append(info['trades'])\n",
    "        episode_win_rates.append(info['win_rate'])\n",
    "        episode_sharpe_ratios.append(sharpe)\n",
    "        \n",
    "        # Update exploration rate\n",
    "        explore_rate = max(min_explore, explore_rate * explore_decay)\n",
    "        \n",
    "        # Early stopping based on Sharpe ratio\n",
    "        if sharpe > best_sharpe:\n",
    "            best_sharpe = sharpe\n",
    "            patience = 0\n",
    "            \n",
    "            # Save best model\n",
    "            if save_models and episode > 50:\n",
    "                torch.save({\n",
    "                    'ppo_state_dict': ppo_agent.network.state_dict(),\n",
    "                    'episode': episode,\n",
    "                    'sharpe': best_sharpe,\n",
    "                    'profit': episode_profit\n",
    "                }, 'best_ppo_model_enhanced.pth')\n",
    "        else:\n",
    "            patience += 1\n",
    "        \n",
    "        if episode_profit > best_profit:\n",
    "            best_profit = episode_profit\n",
    "        \n",
    "        # Adaptive training phases\n",
    "        if episode < 100:\n",
    "            phase = \"Phase 1: Exploration\"\n",
    "        elif episode < 300:\n",
    "            phase = \"Phase 2: Exploitation\"\n",
    "        else:\n",
    "            phase = \"Phase 3: Refinement\"\n",
    "        \n",
    "        # Print progress\n",
    "        if episode % 10 == 0:\n",
    "            avg_profit = np.mean(episode_profits[-10:]) if len(episode_profits) >= 10 else episode_profit\n",
    "            avg_trades = np.mean(episode_trades[-10:]) if len(episode_trades) >= 10 else info['trades']\n",
    "            avg_win_rate = np.mean(episode_win_rates[-10:]) if len(episode_win_rates) >= 10 else info['win_rate']\n",
    "            avg_sharpe = np.mean(episode_sharpe_ratios[-10:]) if len(episode_sharpe_ratios) >= 10 else sharpe\n",
    "            \n",
    "            print(f\"Episode {episode} ({phase}) - Explore Rate: {explore_rate:.3f}\")\n",
    "            print(f\"  Profit: {episode_profit:.2f}% | Avg: {avg_profit:.2f}% | Best: {best_profit:.2f}%\")\n",
    "            print(f\"  Sharpe: {sharpe:.2f} | Avg: {avg_sharpe:.2f} | Best: {best_sharpe:.2f}\")\n",
    "            print(f\"  Trades: {info['trades']} | Win Rate: {info['win_rate']:.2%}\")\n",
    "        \n",
    "        # Check early stopping\n",
    "        if patience >= max_patience and episode > 200:\n",
    "            print(f\"\\nEarly stopping at episode {episode}\")\n",
    "            break\n",
    "    \n",
    "    # === Training Complete ===\n",
    "    print(\"\\n=== Training Complete ===\")\n",
    "    print(f\"Best Sharpe Ratio: {best_sharpe:.2f}\")\n",
    "    print(f\"Best Profit: {best_profit:.2f}%\")\n",
    "    print(f\"Final average profit (last 50): {np.mean(episode_profits[-50:]):.2f}%\")\n",
    "    print(f\"Final average Sharpe (last 50): {np.mean(episode_sharpe_ratios[-50:]):.2f}\")\n",
    "    print(f\"Final average win rate (last 50): {np.mean(episode_win_rates[-50:]):.2%}\")\n",
    "    \n",
    "    # Plot training progress\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Profits\n",
    "    ax1.plot(episode_profits)\n",
    "    ax1.axhline(y=0, color='r', linestyle='--')\n",
    "    ax1.set_title('Episode Profits (%)')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Profit %')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Sharpe Ratios\n",
    "    ax2.plot(episode_sharpe_ratios)\n",
    "    ax2.axhline(y=0, color='r', linestyle='--')\n",
    "    ax2.axhline(y=1, color='g', linestyle='--', alpha=0.5)\n",
    "    ax2.set_title('Episode Sharpe Ratios')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Sharpe Ratio')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Win rates\n",
    "    ax3.plot(episode_win_rates)\n",
    "    ax3.axhline(y=0.5, color='r', linestyle='--')\n",
    "    ax3.set_title('Win Rates')\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Win Rate')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    # Moving averages\n",
    "    window = 50\n",
    "    if len(episode_profits) >= window:\n",
    "        ma_profits = pd.Series(episode_profits).rolling(window).mean()\n",
    "        ma_sharpe = pd.Series(episode_sharpe_ratios).rolling(window).mean()\n",
    "        \n",
    "        ax4.plot(ma_profits, label='MA Profit %', color='green')\n",
    "        ax4.plot(ma_sharpe * 10, label='MA Sharpe x10', color='blue')\n",
    "        ax4.set_title(f'{window}-Episode Moving Averages')\n",
    "        ax4.set_xlabel('Episode')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ppo_agent, xgb_predictor, {\n",
    "        'episode_profits': episode_profits,\n",
    "        'episode_trades': episode_trades,\n",
    "        'episode_win_rates': episode_win_rates,\n",
    "        'episode_sharpe_ratios': episode_sharpe_ratios,\n",
    "        'best_profit': best_profit,\n",
    "        'best_sharpe': best_sharpe\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdac9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the enhanced system\n",
    "print(\"Starting enhanced multi-asset training...\")\n",
    "ppo_agent, xgb_predictor, training_results = train_enhanced_system(combined_data, episodes=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad8742",
   "metadata": {},
   "source": [
    "## 8. Backtesting on Individual Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b460eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_individual_asset(ppo_agent, xgb_predictor, asset_data, asset_name, plot_results=True):\n",
    "    \"\"\"Backtest on individual asset - FIXED for data handling\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== Backtesting on {asset_name} ===\")\n",
    "    \n",
    "    # Use only recent data for testing\n",
    "    test_data = asset_data[asset_data['datetime'] >= '2023-01-01'].copy()\n",
    "    \n",
    "    # Check if we have test data\n",
    "    if len(test_data) < 100:\n",
    "        print(f\"Warning: Only {len(test_data)} samples for {asset_name} testing. Need more recent data.\")\n",
    "        # Use last 30% of available data instead\n",
    "        test_size = int(0.3 * len(asset_data))\n",
    "        test_data = asset_data[-test_size:].copy()\n",
    "    \n",
    "    if len(test_data) < 50:\n",
    "        print(f\"Error: Not enough data for backtesting {asset_name}\")\n",
    "        return {\n",
    "            'asset': asset_name,\n",
    "            'total_return': 0,\n",
    "            'sharpe_ratio': 0,\n",
    "            'max_drawdown': 0,\n",
    "            'total_trades': 0,\n",
    "            'win_rate': 0,\n",
    "            'final_equity': 10000,\n",
    "            'equity_curve': [10000]\n",
    "        }\n",
    "    \n",
    "    print(f\"Test data range: {test_data['datetime'].min()} to {test_data['datetime'].max()}\")\n",
    "    print(f\"Test samples: {len(test_data)}\")\n",
    "    \n",
    "    # Create test environment\n",
    "    test_env = EnhancedCryptoFuturesEnv(test_data)\n",
    "    state = test_env.reset()\n",
    "    \n",
    "    # Prepare XGBoost features\n",
    "    xgb_features = []\n",
    "    for i in range(len(test_data)):\n",
    "        if i >= xgb_predictor.lookback:\n",
    "            feat = test_data[feature_columns].iloc[\n",
    "                i - xgb_predictor.lookback:i\n",
    "            ].values.flatten()\n",
    "            xgb_features.append(feat)\n",
    "    \n",
    "    xgb_features = np.array(xgb_features) if xgb_features else None\n",
    "    xgb_signals = xgb_predictor.predict(xgb_features) if xgb_features is not None else None\n",
    "    \n",
    "    # Trading loop\n",
    "    actions = []\n",
    "    prices = []\n",
    "    positions = []\n",
    "    equities = []\n",
    "    \n",
    "    # Detailed logging\n",
    "    action_logs = []\n",
    "    \n",
    "    step_count = 0\n",
    "    while True:\n",
    "        # Get XGBoost signal\n",
    "        if xgb_signals is not None and step_count < len(xgb_signals):\n",
    "            current_xgb_signal = xgb_signals[step_count]\n",
    "        else:\n",
    "            current_xgb_signal = None\n",
    "        \n",
    "        # Get valid actions\n",
    "        valid_actions = test_env.get_valid_actions()\n",
    "        \n",
    "        # Get action from PPO\n",
    "        with torch.no_grad():\n",
    "            action = ppo_agent.act(state, current_xgb_signal, valid_actions, explore_rate=0)\n",
    "        \n",
    "        # Execute action\n",
    "        next_state, reward, done, info = test_env.step(action)\n",
    "        \n",
    "        # Record data\n",
    "        actions.append(action)\n",
    "        prices.append(test_data['close'].iloc[test_env.current_step - 1])\n",
    "        positions.append(1 if test_env.position == 'long' else \n",
    "                        (-1 if test_env.position == 'short' else 0))\n",
    "        equities.append(info['equity'])\n",
    "        \n",
    "        # Print first few trades\n",
    "        if len(test_env.trades) > 0 and len(test_env.trades) <= 5:\n",
    "            trade = test_env.trades[-1]\n",
    "            print(f\"[Trade #{len(test_env.trades)}] {trade['position'].upper()} \"\n",
    "                  f\"Entry: ${trade['entry_price']:.2f} Exit: ${trade['exit_price']:.2f} \"\n",
    "                  f\"P&L: ${trade['pnl']:.2f} ({trade['pnl_pct']*100:.2f}%) \"\n",
    "                  f\"Leverage: {trade['leverage']}x Reason: {trade['exit_reason']}\")\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        step_count += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Calculate metrics\n",
    "    final_equity = equities[-1]\n",
    "    total_return = (final_equity / test_env.initial_balance - 1) * 100\n",
    "    \n",
    "    # Calculate Sharpe ratio\n",
    "    if len(equities) > 1:\n",
    "        equity_returns = pd.Series(equities).pct_change().dropna()\n",
    "        if len(equity_returns) > 0 and equity_returns.std() > 0:\n",
    "            sharpe_ratio = np.sqrt(252 * 24) * equity_returns.mean() / equity_returns.std()\n",
    "        else:\n",
    "            sharpe_ratio = 0\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "    \n",
    "    # Calculate max drawdown\n",
    "    if len(equities) > 0:\n",
    "        equity_series = pd.Series(equities)\n",
    "        running_max = equity_series.expanding().max()\n",
    "        drawdown = (equity_series - running_max) / running_max\n",
    "        max_drawdown = drawdown.min() * 100\n",
    "    else:\n",
    "        max_drawdown = 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n=== {asset_name} Results ===\")\n",
    "    print(f\"Total Return: {total_return:.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "    print(f\"Max Drawdown: {max_drawdown:.2f}%\")\n",
    "    print(f\"Total Trades: {len(test_env.trades)}\")\n",
    "    print(f\"Win Rate: {test_env._calculate_win_rate():.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'asset': asset_name,\n",
    "        'total_return': total_return,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'total_trades': len(test_env.trades),\n",
    "        'win_rate': test_env._calculate_win_rate(),\n",
    "        'final_equity': final_equity,\n",
    "        'equity_curve': equities\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest on each asset\n",
    "results = {}\n",
    "for symbol, data in multi_asset_data.items():\n",
    "    asset_name = symbol.split('/')[0]\n",
    "    results[asset_name] = backtest_individual_asset(\n",
    "        ppo_agent, xgb_predictor, data, asset_name, plot_results=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n=== PORTFOLIO SUMMARY ===\")\n",
    "print(f\"{'Asset':<10} {'Return %':<12} {'Sharpe':<10} {'Max DD %':<12} {'Win Rate':<10} {'Trades':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "total_return = 0\n",
    "for asset, res in results.items():\n",
    "    print(f\"{asset:<10} {res['total_return']:>10.2f}% {res['sharpe_ratio']:>8.2f} \"\n",
    "          f\"{res['max_drawdown']:>10.2f}% {res['win_rate']:>8.2%} {res['total_trades']:>8}\")\n",
    "    total_return += res['total_return']\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'AVERAGE':<10} {total_return/len(results):>10.2f}%\")\n",
    "\n",
    "print(\"\\nTraining completed successfully! The model is now ready for live trading simulation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
