{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8e6263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Downloading BTC/USDT 1h data...\n",
      "Downloaded 8760 candles...\n",
      "Downloaded 8760 candles\n",
      "Data shape: (8760, 7)\n",
      "Data with features: (8711, 42)\n",
      "Observation shape: (44,)\n",
      "Features: market(25) + position(7) + risk(7) + regime(5) = 44\n",
      "Train data: 6968 samples\n",
      "Test data: 1743 samples\n",
      "Training Advanced DQN Agent with Market-Adaptive Learning...\n",
      "Episode 0: Profit: 0.05%, Trades: 100, Avg Profit: 0.05%, Best: 0.05%, Epsilon: 0.010\n",
      "Episode 10: Profit: -0.24%, Trades: 100, Avg Profit: -20.62%, Best: 0.05%, Epsilon: 0.010\n",
      "Episode 20: Profit: -0.08%, Trades: 100, Avg Profit: -40.10%, Best: 1.50%, Epsilon: 0.010\n",
      "Episode 30: Profit: -0.08%, Trades: 100, Avg Profit: -20.70%, Best: 1.50%, Epsilon: 0.010\n",
      "Episode 40: Profit: -0.71%, Trades: 100, Avg Profit: -30.59%, Best: 1.50%, Epsilon: 0.010\n",
      "Episode 50: Profit: -1.12%, Trades: 100, Avg Profit: -0.57%, Best: 1.92%, Epsilon: 0.010\n",
      "Episode 60: Profit: -0.01%, Trades: 100, Avg Profit: -0.40%, Best: 1.92%, Epsilon: 0.010\n",
      "Episode 70: Profit: -0.77%, Trades: 100, Avg Profit: -0.82%, Best: 1.92%, Epsilon: 0.010\n",
      "Episode 80: Profit: 0.08%, Trades: 100, Avg Profit: -10.25%, Best: 1.92%, Epsilon: 0.010\n",
      "Episode 90: Profit: 0.99%, Trades: 100, Avg Profit: -20.16%, Best: 1.92%, Epsilon: 0.010\n",
      "\n",
      "Phase 2: Optimizing Entries/Exits\n",
      "Episode 100: Profit: 0.14%, Trades: 100, Avg Profit: -20.36%, Best: 1.92%, Epsilon: 0.010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1275\u001b[0m\n\u001b[0;32m   1272\u001b[0m agent \u001b[38;5;241m=\u001b[39m AdvancedDQNAgent(state_size, action_size)\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[1;32m-> 1275\u001b[0m training_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_advanced_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# More episodes\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;66;03m# ## 9. Training Analysis\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \n\u001b[0;32m   1280\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m fig, ((ax1, ax2), (ax3, ax4)) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "Cell \u001b[1;32mIn[1], line 1208\u001b[0m, in \u001b[0;36mtrain_advanced_agent\u001b[1;34m(agent, env, episodes, early_stop_patience)\u001b[0m\n\u001b[0;32m   1206\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state, env)\n\u001b[0;32m   1207\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m-> 1208\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1210\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m   1211\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[1], line 1099\u001b[0m, in \u001b[0;36mAdvancedDQNAgent.step\u001b[1;34m(self, state, action, next_state, reward, done, env)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m-> 1099\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 1138\u001b[0m, in \u001b[0;36mAdvancedDQNAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# Reset noise\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_network\u001b[38;5;241m.\u001b[39mreset_noise()\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# Soft update target network\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 892\u001b[0m, in \u001b[0;36mDuelingDQN.reset_noise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reset noise layers\"\"\"\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoisy:\n\u001b[1;32m--> 892\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_fc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_out\u001b[38;5;241m.\u001b[39mreset_noise()\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvantage_fc\u001b[38;5;241m.\u001b[39mreset_noise()\n",
      "Cell \u001b[1;32mIn[1], line 819\u001b[0m, in \u001b[0;36mNoisyLinear.reset_noise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m epsilon_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale_noise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features)\n\u001b[0;32m    818\u001b[0m epsilon_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale_noise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features)\n\u001b[1;32m--> 819\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_epsilon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepsilon_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepsilon_in\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_epsilon\u001b[38;5;241m.\u001b[39mcopy_(epsilon_out)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Advanced Cryptocurrency Futures Trading Bot with State-of-the-Art DQN\n",
    "# Improved implementation with proven techniques to prevent overtrading and liquidations\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Import Libraries & Setup\n",
    "\n",
    "# %%\n",
    "import ccxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Download Bitcoin Data\n",
    "\n",
    "# %%\n",
    "def download_btc_data(symbol='BTC/USDT', timeframe='1h', days=365):\n",
    "    \"\"\"Download Bitcoin data from Binance\"\"\"\n",
    "    exchange = ccxt.binance({\n",
    "        'rateLimit': 1200,\n",
    "        'enableRateLimit': True,\n",
    "    })\n",
    "    \n",
    "    end_time = exchange.milliseconds()\n",
    "    start_time = end_time - (days * 24 * 60 * 60 * 1000)\n",
    "    \n",
    "    print(f\"Downloading {symbol} {timeframe} data...\")\n",
    "    all_ohlcv = []\n",
    "    \n",
    "    while start_time < end_time:\n",
    "        try:\n",
    "            ohlcv = exchange.fetch_ohlcv(symbol, timeframe, start_time, 1500)\n",
    "            if not ohlcv:\n",
    "                break\n",
    "            all_ohlcv.extend(ohlcv)\n",
    "            start_time = ohlcv[-1][0] + 1\n",
    "            print(f\"Downloaded {len(all_ohlcv)} candles...\", end='\\r')\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "    \n",
    "    df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df = df.drop_duplicates(subset=['timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nDownloaded {len(df)} candles\")\n",
    "    return df\n",
    "\n",
    "# Download data\n",
    "data = download_btc_data(days=365)  # 1 year for better training\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Enhanced Feature Engineering\n",
    "\n",
    "# %%\n",
    "def add_advanced_features(df):\n",
    "    \"\"\"Add comprehensive technical indicators for futures trading\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Price features\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "    df['close_to_high'] = (df['high'] - df['close']) / df['high']\n",
    "    df['close_to_low'] = (df['close'] - df['low']) / df['low']\n",
    "    \n",
    "    # Volatility\n",
    "    df['volatility'] = df['returns'].rolling(20).std()\n",
    "    df['atr'] = calculate_atr(df, 14)\n",
    "    \n",
    "    # Moving averages\n",
    "    for period in [10, 20, 50]:\n",
    "        df[f'sma_{period}'] = df['close'].rolling(period).mean()\n",
    "        df[f'ema_{period}'] = df['close'].ewm(span=period).mean()\n",
    "    \n",
    "    df['sma_ratio_short'] = df['sma_10'] / df['sma_20']\n",
    "    df['sma_ratio_long'] = df['sma_20'] / df['sma_50']\n",
    "    \n",
    "    # TREND FOLLOWING FEATURES (NEW)\n",
    "    df['trend_20'] = (df['close'] - df['sma_20']) / df['sma_20']\n",
    "    df['trend_50'] = (df['close'] - df['sma_50']) / df['sma_50']\n",
    "    df['momentum_5'] = df['close'].pct_change(5)\n",
    "    df['momentum_10'] = df['close'].pct_change(10)\n",
    "    \n",
    "    # Higher highs/lows\n",
    "    df['hh'] = (df['high'] > df['high'].rolling(20).max().shift(1)).astype(float)\n",
    "    df['ll'] = (df['low'] < df['low'].rolling(20).min().shift(1)).astype(float)\n",
    "    \n",
    "    # RSI\n",
    "    df['rsi'] = calculate_rsi(df['close'], 14)\n",
    "    df['rsi_ma'] = df['rsi'].rolling(10).mean()\n",
    "    \n",
    "    # MACD\n",
    "    df['macd'], df['macd_signal'], df['macd_hist'] = calculate_macd(df['close'])\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['bb_middle'] = df['close'].rolling(20).mean()\n",
    "    bb_std = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_middle'] + (bb_std * 2)\n",
    "    df['bb_lower'] = df['bb_middle'] - (bb_std * 2)\n",
    "    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']\n",
    "    \n",
    "    # Volume features\n",
    "    df['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()\n",
    "    df['volume_trend'] = df['volume'].rolling(10).mean() / df['volume'].rolling(50).mean()\n",
    "    \n",
    "    # Market microstructure\n",
    "    df['spread'] = df['high'] - df['low']\n",
    "    df['spread_pct'] = df['spread'] / df['close']\n",
    "    \n",
    "    # Clean data\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def calculate_atr(df, period=14):\n",
    "    \"\"\"Calculate Average True Range\"\"\"\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    return tr.rolling(period).mean()\n",
    "\n",
    "def calculate_rsi(prices, period=14):\n",
    "    \"\"\"Calculate RSI\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Calculate MACD\"\"\"\n",
    "    ema_fast = prices.ewm(span=fast).mean()\n",
    "    ema_slow = prices.ewm(span=slow).mean()\n",
    "    macd = ema_fast - ema_slow\n",
    "    macd_signal = macd.ewm(span=signal).mean()\n",
    "    macd_hist = macd - macd_signal\n",
    "    return macd, macd_signal, macd_hist\n",
    "\n",
    "# Add features\n",
    "data = add_advanced_features(data)\n",
    "print(f\"Data with features: {data.shape}\")\n",
    "\n",
    "# Select features - Focus on TREND\n",
    "feature_columns = [\n",
    "    # Trend features (NEW)\n",
    "    'trend_20', 'trend_50', 'momentum_5', 'momentum_10', 'hh', 'll',\n",
    "    # Price action\n",
    "    'returns', 'log_returns', 'high_low_ratio', 'close_to_high', 'close_to_low',\n",
    "    # Technical indicators\n",
    "    'sma_ratio_short', 'sma_ratio_long', 'rsi', 'rsi_ma', \n",
    "    'macd', 'macd_signal', 'macd_hist', 'bb_position', 'bb_width',\n",
    "    # Volume and volatility\n",
    "    'volume_ratio', 'volume_trend', 'volatility', 'atr', 'spread_pct'\n",
    "]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Advanced Futures Trading Environment\n",
    "\n",
    "# %%\n",
    "class AdvancedFuturesTradingEnv:\n",
    "    \"\"\"Enhanced environment with anti-overtrading and risk management\"\"\"\n",
    "    \n",
    "    def __init__(self, data, initial_balance=10000, fee_rate=0.0005, \n",
    "                 liquidation_threshold=0.05, max_trades_per_episode=100):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.initial_balance = initial_balance\n",
    "        self.fee_rate = fee_rate  # Reduced to 0.05%\n",
    "        self.liquidation_threshold = liquidation_threshold\n",
    "        self.max_trades_per_episode = max_trades_per_episode  # Increased to 100\n",
    "        \n",
    "        # Anti-overtrading\n",
    "        self.min_holding_period = 3  # Minimum bars between trades\n",
    "        self.overtrading_penalty = 0.0001  # Very small penalty\n",
    "        \n",
    "        # Position sizing - MUCH SMALLER\n",
    "        self.max_position_pct = 0.1  # Max 10% of capital per trade\n",
    "        self.min_position_pct = 0.02  # Min 2% position\n",
    "        self.min_position_pct = 0.02  # Min 2% position\n",
    "        self.kelly_fraction = 0.1  # Very conservative Kelly\n",
    "        \n",
    "        # Stop loss\n",
    "        self.stop_loss_pct = 0.02  # 2% stop loss\n",
    "        \n",
    "        # Features\n",
    "        self.feature_columns = feature_columns\n",
    "        self.action_space = 4  # HOLD, LONG, SHORT, EXIT\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 100  # Need history for indicators\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = None\n",
    "        self.entry_price = 0\n",
    "        self.position_size = 0\n",
    "        self.entry_step = 0\n",
    "        \n",
    "        # Tracking\n",
    "        self.trades = []\n",
    "        self.num_trades = 0\n",
    "        self.consecutive_losses = 0\n",
    "        self.max_drawdown = 0\n",
    "        self.peak_equity = self.initial_balance\n",
    "        \n",
    "        # History\n",
    "        self.balance_history = [self.balance]\n",
    "        self.equity_history = [self.balance]\n",
    "        self.action_history = []\n",
    "        self.position_history = []\n",
    "        \n",
    "        return self._get_observation()\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Enhanced state representation\"\"\"\n",
    "        # Market features\n",
    "        features = self.data[self.feature_columns].iloc[self.current_step].values\n",
    "        \n",
    "        # Normalize features\n",
    "        features = np.clip(features, -5, 5)  # Clip extreme values\n",
    "        \n",
    "        # Position features\n",
    "        current_price = self.data['close'].iloc[self.current_step]\n",
    "        position_features = self._get_position_features(current_price)\n",
    "        \n",
    "        # Risk features\n",
    "        risk_features = self._get_risk_features(current_price)\n",
    "        \n",
    "        # Market regime features\n",
    "        regime_features = self._get_market_regime_features()\n",
    "        \n",
    "        return np.concatenate([features, position_features, risk_features, regime_features]).astype(np.float32)\n",
    "    \n",
    "    def _get_position_features(self, current_price):\n",
    "        \"\"\"Position-specific features\"\"\"\n",
    "        if self.position is None:\n",
    "            return np.array([0, 0, 0, 0, 0, 0, 0])\n",
    "        \n",
    "        # Basic position info\n",
    "        is_long = float(self.position == 'long')\n",
    "        is_short = float(self.position == 'short')\n",
    "        \n",
    "        # P&L calculation\n",
    "        if self.position == 'long':\n",
    "            pnl_pct = (current_price - self.entry_price) / self.entry_price\n",
    "        else:\n",
    "            pnl_pct = (self.entry_price - current_price) / self.entry_price\n",
    "        \n",
    "        # Time in position\n",
    "        bars_in_position = self.current_step - self.entry_step\n",
    "        \n",
    "        # Distance to liquidation\n",
    "        if self.position == 'long':\n",
    "            liquidation_price = self.entry_price * (1 - self.liquidation_threshold)\n",
    "            distance_to_liq = (current_price - liquidation_price) / current_price\n",
    "        else:\n",
    "            liquidation_price = self.entry_price * (1 + self.liquidation_threshold)\n",
    "            distance_to_liq = (liquidation_price - current_price) / current_price\n",
    "        \n",
    "        return np.array([\n",
    "            is_long,\n",
    "            is_short,\n",
    "            self.position_size / self.initial_balance,\n",
    "            pnl_pct,\n",
    "            np.tanh(pnl_pct * 10),  # Scaled P&L\n",
    "            bars_in_position / 100,  # Normalized time\n",
    "            distance_to_liq\n",
    "        ])\n",
    "    \n",
    "    def _get_risk_features(self, current_price):\n",
    "        \"\"\"Risk management features\"\"\"\n",
    "        equity = self._calculate_equity(current_price)\n",
    "        \n",
    "        # Drawdown\n",
    "        self.peak_equity = max(self.peak_equity, equity)\n",
    "        current_drawdown = (self.peak_equity - equity) / self.peak_equity\n",
    "        \n",
    "        # Trade statistics\n",
    "        win_rate = self._calculate_win_rate()\n",
    "        avg_win_loss_ratio = self._calculate_avg_win_loss_ratio()\n",
    "        trade_frequency = self.num_trades / max(1, self.current_step - 100)\n",
    "        \n",
    "        return np.array([\n",
    "            self.balance / self.initial_balance,\n",
    "            equity / self.initial_balance - 1,  # Total return\n",
    "            current_drawdown,\n",
    "            self.consecutive_losses / 5,  # Normalized\n",
    "            win_rate,\n",
    "            avg_win_loss_ratio,\n",
    "            trade_frequency\n",
    "        ])\n",
    "    \n",
    "    def _get_market_bias(self):\n",
    "        \"\"\"Detect overall market bias for trend following\"\"\"\n",
    "        if self.current_step < 100:\n",
    "            return 'neutral'\n",
    "        \n",
    "        lookback = 100\n",
    "        start_idx = max(0, self.current_step - lookback)\n",
    "        \n",
    "        # Long-term trend\n",
    "        start_price = self.data['close'].iloc[start_idx]\n",
    "        current_price = self.data['close'].iloc[self.current_step]\n",
    "        trend_pct = (current_price - start_price) / start_price\n",
    "        \n",
    "        # Moving average alignment\n",
    "        sma20 = self.data['sma_20'].iloc[self.current_step]\n",
    "        sma50 = self.data['sma_50'].iloc[self.current_step]\n",
    "        ma_aligned_up = sma20 > sma50\n",
    "        \n",
    "        # Strong uptrend\n",
    "        if trend_pct > 0.05 and ma_aligned_up:\n",
    "            return 'bullish'\n",
    "        # Strong downtrend\n",
    "        elif trend_pct < -0.05 and not ma_aligned_up:\n",
    "            return 'bearish'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    def _get_market_regime(self):\n",
    "        \"\"\"Detect market regime: trending, ranging, volatile\"\"\"\n",
    "        if self.current_step < 50:\n",
    "            return 'unknown'\n",
    "        \n",
    "        lookback = 50\n",
    "        start = max(0, self.current_step - lookback)\n",
    "        recent = self.data.iloc[start:self.current_step]\n",
    "        \n",
    "        # Trend detection\n",
    "        trend = (recent['close'].iloc[-1] - recent['close'].iloc[0]) / recent['close'].iloc[0]\n",
    "        \n",
    "        # Volatility\n",
    "        volatility = recent['returns'].std()\n",
    "        \n",
    "        # Range detection using ATR\n",
    "        atr_ratio = recent['atr'].mean() / recent['close'].mean()\n",
    "        \n",
    "        if abs(trend) > 0.05 and volatility < 0.03:\n",
    "            return 'trending'\n",
    "        elif abs(trend) < 0.02 and atr_ratio < 0.01:\n",
    "            return 'ranging'\n",
    "        else:\n",
    "            return 'volatile'\n",
    "    \n",
    "    def _get_trend_strength(self):\n",
    "        \"\"\"Get current trend strength\"\"\"\n",
    "        if self.current_step < 20:\n",
    "            return 0\n",
    "        \n",
    "        # Use trend_20 feature if available\n",
    "        return self.data['trend_20'].iloc[self.current_step]\n",
    "    \n",
    "    def _get_market_regime_features(self):\n",
    "        \"\"\"Market regime detection features\"\"\"\n",
    "        idx = self.current_step\n",
    "        \n",
    "        # Get market regime\n",
    "        regime = self._get_market_regime()\n",
    "        regime_encoded = [0, 0, 0]  # trending, ranging, volatile\n",
    "        if regime == 'trending':\n",
    "            regime_encoded[0] = 1\n",
    "        elif regime == 'ranging':\n",
    "            regime_encoded[1] = 1\n",
    "        elif regime == 'volatile':\n",
    "            regime_encoded[2] = 1\n",
    "        \n",
    "        # Trend strength from features\n",
    "        trend_strength = self._get_trend_strength()\n",
    "        \n",
    "        # Volatility percentile\n",
    "        if idx >= 100:\n",
    "            vol_percentile = self.data['volatility'].iloc[idx-100:idx].rank(pct=True).iloc[-1]\n",
    "        else:\n",
    "            vol_percentile = 0.5\n",
    "        \n",
    "        return np.array(regime_encoded + [trend_strength, vol_percentile])\n",
    "    \n",
    "    def _check_liquidation(self):\n",
    "        \"\"\"Check liquidation using high/low prices\"\"\"\n",
    "        if self.position is None:\n",
    "            return False\n",
    "        \n",
    "        high_price = self.data['high'].iloc[self.current_step]\n",
    "        low_price = self.data['low'].iloc[self.current_step]\n",
    "        \n",
    "        if self.position == 'long':\n",
    "            worst_price = low_price\n",
    "            loss_pct = (worst_price - self.entry_price) / self.entry_price\n",
    "        else:\n",
    "            worst_price = high_price\n",
    "            loss_pct = (self.entry_price - worst_price) / self.entry_price\n",
    "        \n",
    "        return loss_pct <= -self.liquidation_threshold\n",
    "    \n",
    "    def _calculate_position_size(self, confidence=1.0):\n",
    "        \"\"\"Dynamic position sizing based on winning streaks and market conditions\"\"\"\n",
    "        base_size_pct = 0.05  # 5% base\n",
    "        \n",
    "        # Check recent performance\n",
    "        recent_trades = [t for t in self.trades[-5:] if 'pnl' in t]\n",
    "        if len(recent_trades) >= 3:\n",
    "            wins = sum(1 for t in recent_trades if t['pnl'] > 0)\n",
    "            win_rate_recent = wins / len(recent_trades)\n",
    "            \n",
    "            if win_rate_recent >= 0.8:  # 80% win rate in last 5\n",
    "                perf_multiplier = 2.0\n",
    "            elif win_rate_recent >= 0.6:\n",
    "                perf_multiplier = 1.5\n",
    "            else:\n",
    "                perf_multiplier = 1.0\n",
    "        else:\n",
    "            perf_multiplier = 1.0\n",
    "        \n",
    "        # Market bias adjustment\n",
    "        market_bias = self._get_market_bias()\n",
    "        trend_strength = abs(self._get_trend_strength())\n",
    "        \n",
    "        if market_bias != 'neutral' and trend_strength > 0.02:\n",
    "            # Increase size in strong trends\n",
    "            trend_multiplier = 1.5\n",
    "        else:\n",
    "            trend_multiplier = 1.0\n",
    "        \n",
    "        # Volatility adjustment\n",
    "        volatility = self.data['volatility'].iloc[self.current_step]\n",
    "        if volatility > 0:\n",
    "            vol_multiplier = min(1.0, 0.02 / volatility)\n",
    "        else:\n",
    "            vol_multiplier = 1.0\n",
    "        \n",
    "        # Calculate final size\n",
    "        position_pct = base_size_pct * perf_multiplier * trend_multiplier * vol_multiplier * confidence\n",
    "        \n",
    "        # Clamp between min and max\n",
    "        position_pct = min(self.max_position_pct, max(self.min_position_pct, position_pct))\n",
    "        \n",
    "        return self.balance * position_pct\n",
    "    \n",
    "    def _should_allow_trade(self, action):\n",
    "        \"\"\"Filter trades against strong trends\"\"\"\n",
    "        market_bias = self._get_market_bias()\n",
    "        trend_strength = self._get_trend_strength()\n",
    "        \n",
    "        # Block counter-trend trades in strong trends\n",
    "        if market_bias == 'bullish' and trend_strength > 0.03:\n",
    "            if action == 2:  # Trying to short in bull market\n",
    "                return False\n",
    "        elif market_bias == 'bearish' and trend_strength < -0.03:\n",
    "            if action == 1:  # Trying to long in bear market\n",
    "                return False\n",
    "        \n",
    "        # Allow all trades in neutral markets\n",
    "        return True\n",
    "    \n",
    "    def _execute_action(self, action):\n",
    "        \"\"\"Execute action with enhanced risk management\"\"\"\n",
    "        current_price = self.data['close'].iloc[self.current_step]\n",
    "        \n",
    "        # CRITICAL: Check stop loss BEFORE liquidation\n",
    "        if self.position is not None:\n",
    "            if self.position == 'long':\n",
    "                loss_pct = (current_price - self.entry_price) / self.entry_price\n",
    "            else:\n",
    "                loss_pct = (self.entry_price - current_price) / self.entry_price\n",
    "            \n",
    "            # Force exit if stop loss hit\n",
    "            if loss_pct <= -self.stop_loss_pct:\n",
    "                action = 3  # Override to EXIT\n",
    "                # Record stop loss\n",
    "                self.trades.append({\n",
    "                    'step': self.current_step,\n",
    "                    'action': 'stop_loss',\n",
    "                    'position': self.position,\n",
    "                    'loss_pct': loss_pct\n",
    "                })\n",
    "        \n",
    "        # Check liquidation after stop loss\n",
    "        if self._check_liquidation():\n",
    "            loss = -self.position_size\n",
    "            self.balance = 0\n",
    "            \n",
    "            self.trades.append({\n",
    "                'step': self.current_step,\n",
    "                'action': 'liquidation',\n",
    "                'position': self.position,\n",
    "                'loss': loss\n",
    "            })\n",
    "            \n",
    "            self.consecutive_losses += 1\n",
    "            self.position = None\n",
    "            self.position_size = 0\n",
    "            \n",
    "            return -1.0  # Maximum penalty\n",
    "        \n",
    "        # Anti-overtrading: enforce minimum holding period\n",
    "        if self.position is not None and self.min_holding_period > 0:\n",
    "            bars_held = self.current_step - self.entry_step\n",
    "            if bars_held < self.min_holding_period and action == 3:\n",
    "                return -0.001  # Small penalty for trying to exit too early\n",
    "        \n",
    "        # Execute actions\n",
    "        reward = 0\n",
    "        \n",
    "        # TREND FILTER - Block counter-trend trades\n",
    "        if action in [1, 2] and not self._should_allow_trade(action):\n",
    "            return 0  # No reward, just skip the action\n",
    "        \n",
    "        if action == 0:  # HOLD\n",
    "            pass\n",
    "            \n",
    "        elif action == 1 and self.position is None:  # LONG\n",
    "            if self.num_trades < self.max_trades_per_episode:\n",
    "                self.position_size = self._calculate_position_size(confidence=1.0)\n",
    "                fee = self.position_size * self.fee_rate\n",
    "                \n",
    "                if self.position_size + fee <= self.balance:\n",
    "                    self.balance -= (self.position_size + fee)\n",
    "                    self.position = 'long'\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.num_trades += 1\n",
    "                    \n",
    "                    self.trades.append({\n",
    "                        'step': self.current_step,\n",
    "                        'action': 'open_long',\n",
    "                        'price': current_price,\n",
    "                        'size': self.position_size\n",
    "                    })\n",
    "            \n",
    "        elif action == 2 and self.position is None:  # SHORT\n",
    "            if self.num_trades < self.max_trades_per_episode:\n",
    "                self.position_size = self._calculate_position_size(confidence=1.0)\n",
    "                fee = self.position_size * self.fee_rate\n",
    "                \n",
    "                if self.position_size + fee <= self.balance:\n",
    "                    self.balance -= (self.position_size + fee)\n",
    "                    self.position = 'short'\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.num_trades += 1\n",
    "                    \n",
    "                    self.trades.append({\n",
    "                        'step': self.current_step,\n",
    "                        'action': 'open_short',\n",
    "                        'price': current_price,\n",
    "                        'size': self.position_size\n",
    "                    })\n",
    "            \n",
    "        elif action == 3 and self.position is not None:  # EXIT\n",
    "            # Calculate P&L\n",
    "            if self.position == 'long':\n",
    "                pnl_pct = (current_price - self.entry_price) / self.entry_price\n",
    "            else:\n",
    "                pnl_pct = (self.entry_price - current_price) / self.entry_price\n",
    "            \n",
    "            gross_pnl = self.position_size * pnl_pct\n",
    "            exit_fee = self.position_size * self.fee_rate\n",
    "            net_pnl = gross_pnl - exit_fee\n",
    "            \n",
    "            self.balance += self.position_size + net_pnl\n",
    "            \n",
    "            # Update consecutive losses\n",
    "            if net_pnl < 0:\n",
    "                self.consecutive_losses += 1\n",
    "            else:\n",
    "                self.consecutive_losses = 0\n",
    "            \n",
    "            self.trades.append({\n",
    "                'step': self.current_step,\n",
    "                'action': f'close_{self.position}',\n",
    "                'entry_price': self.entry_price,\n",
    "                'exit_price': current_price,\n",
    "                'pnl': net_pnl,\n",
    "                'pnl_pct': pnl_pct\n",
    "            })\n",
    "            \n",
    "            # ENHANCED Position-based reward\n",
    "            if pnl_pct > 0.005:  # 0.5% profit\n",
    "                # Big reward for profitable exits\n",
    "                reward = pnl_pct * 1.0  # 100% of profit\n",
    "            elif pnl_pct > 0:\n",
    "                # Small profit\n",
    "                reward = pnl_pct * 0.5\n",
    "            else:\n",
    "                # Loss - smaller penalty\n",
    "                reward = pnl_pct * 0.2  # Only 20% penalty\n",
    "            \n",
    "            self.position = None\n",
    "            self.position_size = 0\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _calculate_equity(self, current_price):\n",
    "        \"\"\"Calculate current equity\"\"\"\n",
    "        if self.position is None:\n",
    "            return self.balance\n",
    "        \n",
    "        if self.position == 'long':\n",
    "            unrealized_pnl = self.position_size * ((current_price - self.entry_price) / self.entry_price)\n",
    "        else:\n",
    "            unrealized_pnl = self.position_size * ((self.entry_price - current_price) / self.entry_price)\n",
    "        \n",
    "        return self.balance + self.position_size + unrealized_pnl\n",
    "    \n",
    "    def _calculate_win_rate(self):\n",
    "        \"\"\"Calculate historical win rate\"\"\"\n",
    "        if not self.trades:\n",
    "            return 0.5  # Default 50% win rate\n",
    "        \n",
    "        wins = sum(1 for t in self.trades if t.get('pnl', 0) > 0)\n",
    "        total = sum(1 for t in self.trades if 'pnl' in t)\n",
    "        \n",
    "        if total == 0:\n",
    "            return 0.5  # Default if no closed trades\n",
    "        \n",
    "        return wins / total\n",
    "    \n",
    "    def _calculate_avg_win_loss_ratio(self):\n",
    "        \"\"\"Calculate average win/loss ratio\"\"\"\n",
    "        if not self.trades:\n",
    "            return 1.5  # Default optimistic ratio\n",
    "        \n",
    "        wins = [t['pnl'] for t in self.trades if t.get('pnl', 0) > 0]\n",
    "        losses = [abs(t['pnl']) for t in self.trades if t.get('pnl', 0) < 0]\n",
    "        \n",
    "        avg_win = np.mean(wins) if wins else 100  # Default win size\n",
    "        avg_loss = np.mean(losses) if losses else 100  # Default loss size\n",
    "        \n",
    "        return avg_win / max(avg_loss, 1e-6)\n",
    "    \n",
    "    def _calculate_sharpe_ratio(self):\n",
    "        \"\"\"Calculate Sharpe ratio of returns\"\"\"\n",
    "        if len(self.equity_history) < 20:\n",
    "            return 0\n",
    "        \n",
    "        # Get last 20 equity values\n",
    "        equity_array = np.array(self.equity_history[-20:])\n",
    "        # Calculate returns: (price[t] - price[t-1]) / price[t-1]\n",
    "        returns = np.diff(equity_array) / equity_array[:-1]\n",
    "        \n",
    "        if len(returns) > 0 and returns.std() > 0:\n",
    "            return np.sqrt(252) * returns.mean() / returns.std()  # Annualized\n",
    "        return 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one step with comprehensive reward calculation\"\"\"\n",
    "        # Execute action\n",
    "        action_reward = self._execute_action(action)\n",
    "        \n",
    "        # Calculate comprehensive reward\n",
    "        current_price = self.data['close'].iloc[self.current_step]\n",
    "        equity = self._calculate_equity(current_price)\n",
    "        \n",
    "        # Update drawdown\n",
    "        self.peak_equity = max(self.peak_equity, equity)\n",
    "        drawdown = (self.peak_equity - equity) / self.peak_equity\n",
    "        self.max_drawdown = max(self.max_drawdown, drawdown)\n",
    "        \n",
    "        # Composite reward function\n",
    "        prev_equity = self.equity_history[-1] if self.equity_history else self.initial_balance\n",
    "        equity_change = (equity - prev_equity) / self.initial_balance\n",
    "        \n",
    "        # Sharpe ratio component\n",
    "        sharpe = self._calculate_sharpe_ratio()\n",
    "        \n",
    "        # Get trend strength and market bias\n",
    "        trend_strength = self._get_trend_strength()\n",
    "        market_bias = self._get_market_bias()\n",
    "        \n",
    "        # TREND FOLLOWING REWARDS with market bias\n",
    "        trend_reward = 0\n",
    "        if market_bias == 'bullish':\n",
    "            if self.position == 'long':\n",
    "                trend_reward = 0.01  # Reward holding long in bull market\n",
    "            elif self.position == 'short':\n",
    "                trend_reward = -0.02  # Penalize shorting in bull market\n",
    "            elif action == 1:  # Opening long\n",
    "                trend_reward = 0.005\n",
    "            elif action == 2:  # Opening short\n",
    "                trend_reward = -0.01\n",
    "        elif market_bias == 'bearish':\n",
    "            if self.position == 'short':\n",
    "                trend_reward = 0.01\n",
    "            elif self.position == 'long':\n",
    "                trend_reward = -0.02\n",
    "            elif action == 2:  # Opening short\n",
    "                trend_reward = 0.005\n",
    "            elif action == 1:  # Opening long\n",
    "                trend_reward = -0.01\n",
    "        \n",
    "        # Reward holding profitable positions longer\n",
    "        holding_reward = 0\n",
    "        if self.position is not None:\n",
    "            bars_held = self.current_step - self.entry_step\n",
    "            if self.position == 'long':\n",
    "                current_pnl = (current_price - self.entry_price) / self.entry_price\n",
    "            else:\n",
    "                current_pnl = (self.entry_price - current_price) / self.entry_price\n",
    "            \n",
    "            if current_pnl > 0:\n",
    "                # Exponential reward for holding winners\n",
    "                holding_reward = min(0.02, current_pnl * bars_held * 0.0001)\n",
    "            elif current_pnl < -0.01 and bars_held > 3:\n",
    "                # Quick penalty for holding losers\n",
    "                holding_reward = -0.01\n",
    "        \n",
    "        # No overtrading penalty\n",
    "        trade_penalty = 0\n",
    "        \n",
    "        # Exit bonus for profitable trades\n",
    "        exit_bonus = 0\n",
    "        if action == 3 and self.position is not None:\n",
    "            # Already calculated in execute_action\n",
    "            pass\n",
    "        \n",
    "        # PROFIT-FOCUSED reward calculation\n",
    "        reward = (\n",
    "            0.5 * action_reward +           # Direct P&L (highest weight)\n",
    "            0.3 * trend_reward +            # Trend following\n",
    "            0.1 * equity_change +           # Portfolio growth\n",
    "            0.05 * sharpe * 0.01 +          # Risk-adjusted returns\n",
    "            0.05 * holding_reward +         # Position holding\n",
    "            trade_penalty                   # No penalty\n",
    "        )\n",
    "        \n",
    "        # Clip reward\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "        \n",
    "        # Update history\n",
    "        self.balance_history.append(self.balance)\n",
    "        self.equity_history.append(equity)\n",
    "        self.action_history.append(action)\n",
    "        self.position_history.append(\n",
    "            0 if self.position is None else (1 if self.position == 'long' else 2)\n",
    "        )\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Episode ends if: out of data, no equity, or reached max steps\n",
    "        max_steps = min(len(self.data) - 1, self.current_step + 1000)  # At least 1000 steps\n",
    "        done = (self.current_step >= max_steps) or (equity <= 0)\n",
    "        \n",
    "        next_obs = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        \n",
    "        info = {\n",
    "            'equity': equity,\n",
    "            'drawdown': drawdown,\n",
    "            'num_trades': self.num_trades,\n",
    "            'position': self.position\n",
    "        }\n",
    "        \n",
    "        return next_obs, reward, done, info\n",
    "\n",
    "# Test environment\n",
    "env = AdvancedFuturesTradingEnv(data)\n",
    "obs = env.reset()\n",
    "print(f\"Observation shape: {obs.shape}\")\n",
    "print(f\"Features: market({len(feature_columns)}) + position(7) + risk(7) + regime(5) = {obs.shape[0]}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Dueling DQN with Noisy Networks\n",
    "\n",
    "# %%\n",
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"Noisy linear layer for exploration\"\"\"\n",
    "    def __init__(self, in_features, out_features, std_init=0.5):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        \n",
    "        # Factorized noise\n",
    "        self.register_buffer('weight_epsilon', torch.empty(out_features, in_features))\n",
    "        self.register_buffer('bias_epsilon', torch.empty(out_features))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / np.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / np.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.out_features))\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "    \n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        return x.sign().mul_(x.abs().sqrt_())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        \n",
    "        return F.linear(x, weight, bias)\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"Dueling DQN with Noisy Networks\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=512, noisy=True):\n",
    "        super().__init__()\n",
    "        self.noisy = noisy\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared_fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.shared_fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Value stream\n",
    "        if noisy:\n",
    "            self.value_fc = NoisyLinear(hidden_dim, hidden_dim // 2)\n",
    "            self.value_out = NoisyLinear(hidden_dim // 2, 1)\n",
    "        else:\n",
    "            self.value_fc = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "            self.value_out = nn.Linear(hidden_dim // 2, 1)\n",
    "        \n",
    "        # Advantage stream\n",
    "        if noisy:\n",
    "            self.advantage_fc = NoisyLinear(hidden_dim, hidden_dim // 2)\n",
    "            self.advantage_out = NoisyLinear(hidden_dim // 2, action_dim)\n",
    "        else:\n",
    "            self.advantage_fc = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "            self.advantage_out = nn.Linear(hidden_dim // 2, action_dim)\n",
    "        \n",
    "        # Normalization layers\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        # Shared network\n",
    "        x = F.relu(self.ln1(self.shared_fc1(state)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.ln2(self.shared_fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Value stream\n",
    "        value = F.relu(self.value_fc(x))\n",
    "        value = self.value_out(value)\n",
    "        \n",
    "        # Advantage stream\n",
    "        advantage = F.relu(self.advantage_fc(x))\n",
    "        advantage = self.advantage_out(advantage)\n",
    "        \n",
    "        # Combine streams\n",
    "        q_values = value + advantage - advantage.mean(dim=-1, keepdim=True)\n",
    "        \n",
    "        return q_values\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset noise layers\"\"\"\n",
    "        if self.noisy:\n",
    "            self.value_fc.reset_noise()\n",
    "            self.value_out.reset_noise()\n",
    "            self.advantage_fc.reset_noise()\n",
    "            self.advantage_out.reset_noise()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Prioritized Experience Replay\n",
    "\n",
    "# %%\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Prioritized Experience Replay Buffer\"\"\"\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_end=1.0, beta_steps=100000):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.beta_steps = beta_steps\n",
    "        self.frame = 0\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, next_state, reward, done))\n",
    "        else:\n",
    "            self.buffer[self.position] = (state, action, next_state, reward, done)\n",
    "        \n",
    "        self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.position]\n",
    "        \n",
    "        # Calculate sampling probabilities\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        beta = min(1.0, self.beta_start + (self.beta_end - self.beta_start) * self.frame / self.beta_steps)\n",
    "        self.frame += 1\n",
    "        \n",
    "        weights = (len(self.buffer) * probabilities[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        batch = list(zip(*samples))\n",
    "        states = torch.FloatTensor(batch[0])\n",
    "        actions = torch.LongTensor(batch[1])\n",
    "        next_states = torch.FloatTensor(batch[2])\n",
    "        rewards = torch.FloatTensor(batch[3])\n",
    "        dones = torch.FloatTensor(batch[4])\n",
    "        weights = torch.FloatTensor(weights)\n",
    "        \n",
    "        return states, actions, next_states, rewards, dones, weights, indices\n",
    "    \n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        \"\"\"Update priorities with bonus for profitable trades and market beating\"\"\"\n",
    "        for idx, td_error in zip(indices, td_errors):\n",
    "            # Base priority from TD error\n",
    "            base_priority = abs(td_error) + 1e-6\n",
    "            \n",
    "            # Get transition details if available\n",
    "            if idx < len(self.buffer):\n",
    "                transition = self.buffer[idx]\n",
    "                reward = transition.reward if hasattr(transition, 'reward') else 0\n",
    "                \n",
    "                # Big bonus for highly profitable transitions\n",
    "                if reward > 0.01:  # 1% profit\n",
    "                    base_priority *= 5.0\n",
    "                elif reward > 0:\n",
    "                    base_priority *= 2.0\n",
    "                \n",
    "                # Extra bonus for market-beating trades\n",
    "                if reward > 0.001:  # Positive reward\n",
    "                    base_priority *= 3.0\n",
    "            \n",
    "            self.priorities[idx] = base_priority\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Advanced DQN Agent\n",
    "\n",
    "# %%\n",
    "class AdvancedDQNAgent:\n",
    "    \"\"\"DQN Agent with all state-of-the-art improvements\"\"\"\n",
    "    def __init__(self, state_size, action_size, lr=5e-4):  # Increased learning rate\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = DuelingDQN(state_size, action_size, noisy=True).to(device)\n",
    "        self.target_network = DuelingDQN(state_size, action_size, noisy=True).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Optimizer with gradient clipping\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Memory\n",
    "        self.memory = PrioritizedReplayBuffer(100000)\n",
    "        \n",
    "        # Parameters\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.001  # Slower target updates\n",
    "        self.batch_size = 128  # Larger batch for stability\n",
    "        self.update_every = 1  # Update every step\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Epsilon for initial exploration (in addition to noisy nets)\n",
    "        self.epsilon = 0.5  # Start with medium exploration\n",
    "        self.epsilon_min = 0.01  # Very low minimum\n",
    "        self.epsilon_decay = 0.999  # Very slow decay\n",
    "        \n",
    "        # For tracking\n",
    "        self.losses = []\n",
    "        self.q_values = []\n",
    "    \n",
    "    def act(self, state, env=None, training=True):\n",
    "        \"\"\"Select action using noisy networks with epsilon-greedy backup\"\"\"\n",
    "        # Valid action masking\n",
    "        valid_actions = [0]  # HOLD always valid\n",
    "        if env and env.position is None:\n",
    "            valid_actions.extend([1, 2])  # Can open position\n",
    "        elif env and env.position is not None:\n",
    "            valid_actions.append(3)  # Can close position\n",
    "        \n",
    "        # Epsilon-greedy exploration with trend awareness\n",
    "        if training and random.random() < self.epsilon:\n",
    "            # Get market bias if available\n",
    "            market_bias = env._get_market_bias() if env and hasattr(env, '_get_market_bias') else 'neutral'\n",
    "            \n",
    "            if env and env.position is None:\n",
    "                # When no position, bias exploration toward trend\n",
    "                if market_bias == 'bullish':\n",
    "                    # 70% long, 20% hold, 10% short in bull market\n",
    "                    r = random.random()\n",
    "                    if r < 0.7:\n",
    "                        return 1  # LONG\n",
    "                    elif r < 0.9:\n",
    "                        return 0  # HOLD\n",
    "                    else:\n",
    "                        return 2  # SHORT (rarely)\n",
    "                elif market_bias == 'bearish':\n",
    "                    # 70% short, 20% hold, 10% long in bear market\n",
    "                    r = random.random()\n",
    "                    if r < 0.7:\n",
    "                        return 2  # SHORT\n",
    "                    elif r < 0.9:\n",
    "                        return 0  # HOLD\n",
    "                    else:\n",
    "                        return 1  # LONG (rarely)\n",
    "                else:\n",
    "                    # Neutral market - equal probability\n",
    "                    return random.choice([0, 1, 2])\n",
    "            else:\n",
    "                # When have position, encourage holding winners\n",
    "                bars_held = env.current_step - env.entry_step if hasattr(env, 'entry_step') else 0\n",
    "                \n",
    "                # Check if position is profitable\n",
    "                if env.position == 'long':\n",
    "                    pnl = (env.data['close'].iloc[env.current_step] - env.entry_price) / env.entry_price\n",
    "                else:\n",
    "                    pnl = (env.entry_price - env.data['close'].iloc[env.current_step]) / env.entry_price\n",
    "                \n",
    "                if pnl > 0.01:  # 1% profit\n",
    "                    # Hold winners longer\n",
    "                    return 0 if random.random() < 0.7 else 3\n",
    "                elif pnl < -0.01:  # 1% loss\n",
    "                    # Exit losers quickly\n",
    "                    return 3 if random.random() < 0.7 else 0\n",
    "                else:\n",
    "                    return random.choice([0, 3])\n",
    "            \n",
    "            return random.choice(valid_actions)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        self.q_network.eval()\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor).cpu().numpy()[0]\n",
    "            self.q_values.append(q_values)\n",
    "        self.q_network.train()\n",
    "        \n",
    "        # Mask invalid actions\n",
    "        masked_q_values = np.full(self.action_size, -np.inf)\n",
    "        for action in valid_actions:\n",
    "            masked_q_values[action] = q_values[action]\n",
    "        \n",
    "        return np.argmax(masked_q_values)\n",
    "    \n",
    "    def step(self, state, action, next_state, reward, done, env):\n",
    "        # Store experience\n",
    "        self.memory.push(state, action, next_state, reward, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY steps\n",
    "        self.steps += 1\n",
    "        if self.steps % self.update_every == 0 and len(self.memory) > self.batch_size:\n",
    "            self.learn()\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Update network with prioritized experience replay\"\"\"\n",
    "        # Sample from memory\n",
    "        states, actions, next_states, rewards, dones, weights, indices = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        dones = dones.to(device)\n",
    "        weights = weights.to(device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Double DQN\n",
    "        with torch.no_grad():\n",
    "            # Online network selects actions\n",
    "            next_actions = self.q_network(next_states).argmax(1)\n",
    "            # Target network evaluates\n",
    "            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "        \n",
    "        # TD errors for prioritized replay\n",
    "        td_errors = (current_q_values - target_q_values).detach().cpu().numpy()\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "        \n",
    "        # Weighted loss\n",
    "        loss = (weights * F.mse_loss(current_q_values, target_q_values, reduction='none')).mean()\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Reset noise\n",
    "        self.q_network.reset_noise()\n",
    "        self.target_network.reset_noise()\n",
    "        \n",
    "        # Soft update target network\n",
    "        self.soft_update()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "    \n",
    "    def soft_update(self):\n",
    "        \"\"\"Soft update of target network\"\"\"\n",
    "        for target_param, param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Training with Early Stopping\n",
    "\n",
    "# %%\n",
    "def train_advanced_agent(agent, env, episodes=1000, early_stop_patience=100):\n",
    "    \"\"\"Train agent with adaptive learning based on market conditions\"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_profits = []\n",
    "    episode_trades = []\n",
    "    episode_sharpes = []\n",
    "    \n",
    "    best_profit = -np.inf\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Training Advanced DQN Agent with Market-Adaptive Learning...\")\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Detect initial market condition\n",
    "        env.reset()\n",
    "        market_bias = env._get_market_bias()\n",
    "        \n",
    "        # Adaptive exploration based on market\n",
    "        if market_bias != 'neutral':\n",
    "            # Less exploration in trending markets\n",
    "            agent.epsilon = max(0.01, agent.epsilon * 0.99)\n",
    "        else:\n",
    "            # More exploration in choppy markets\n",
    "            agent.epsilon = max(0.05, agent.epsilon * 0.995)\n",
    "        \n",
    "        # Adjust learning for different phases\n",
    "        if episode < 100:\n",
    "            phase = \"Phase 1: Learning Market Bias\"\n",
    "            # Focus on trend following\n",
    "        elif episode < 300:\n",
    "            phase = \"Phase 2: Optimizing Entries/Exits\"\n",
    "            # Reduce epsilon faster\n",
    "            agent.epsilon = max(0.05, agent.epsilon * 0.99)\n",
    "        else:\n",
    "            phase = \"Phase 3: Fine Tuning\"\n",
    "            # Minimal exploration\n",
    "            agent.epsilon = max(0.01, agent.epsilon * 0.999)\n",
    "        \n",
    "        if episode % 100 == 0 and episode > 0:\n",
    "            print(f\"\\n{phase}\")\n",
    "        \n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Run episode\n",
    "        while True:\n",
    "            action = agent.act(state, env)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.step(state, action, next_state, reward, done, env)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Calculate metrics\n",
    "        final_equity = info['equity']\n",
    "        profit = (final_equity / env.initial_balance - 1) * 100\n",
    "        sharpe = env._calculate_sharpe_ratio()\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_profits.append(profit)\n",
    "        episode_trades.append(env.num_trades)\n",
    "        episode_sharpes.append(sharpe)\n",
    "        \n",
    "        # Track best profit\n",
    "        if profit > best_profit:\n",
    "            best_profit = profit\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping only after sufficient training\n",
    "        if patience_counter >= early_stop_patience and episode > 300:\n",
    "            print(f\"\\nEarly stopping at episode {episode} (Best profit: {best_profit:.2f}%)\")\n",
    "            break\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            avg_trades = np.mean(episode_trades[-10:])\n",
    "            avg_profit = np.mean(episode_profits[-10:])\n",
    "            avg_sharpe = np.mean(episode_sharpes[-10:]) if episode_sharpes[-10:] else 0\n",
    "            \n",
    "            print(f\"Episode {episode}: Profit: {profit:.2f}%, Trades: {env.num_trades}, \"\n",
    "                  f\"Avg Profit: {avg_profit:.2f}%, Best: {best_profit:.2f}%, \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    print(f\"\\nTraining completed! Best profit achieved: {best_profit:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_profits': episode_profits,\n",
    "        'episode_trades': episode_trades,\n",
    "        'episode_sharpes': episode_sharpes,\n",
    "        'best_profit': best_profit\n",
    "    }\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(data))\n",
    "train_data = data[:train_size].copy()\n",
    "test_data = data[train_size:].copy()\n",
    "\n",
    "print(f\"Train data: {len(train_data)} samples\")\n",
    "print(f\"Test data: {len(test_data)} samples\")\n",
    "\n",
    "# Create environments\n",
    "train_env = AdvancedFuturesTradingEnv(train_data)\n",
    "test_env = AdvancedFuturesTradingEnv(test_data)\n",
    "\n",
    "# Initialize agent\n",
    "state_size = train_env.reset().shape[0]\n",
    "action_size = 4\n",
    "agent = AdvancedDQNAgent(state_size, action_size)\n",
    "\n",
    "# Train agent\n",
    "training_results = train_advanced_agent(agent, train_env, episodes=1000)  # More episodes\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Training Analysis\n",
    "\n",
    "# %%\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Profits\n",
    "ax1.plot(training_results['episode_profits'])\n",
    "ax1.axhline(y=0, color='r', linestyle='--')\n",
    "ax1.set_title('Episode Profits (%)')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Profit %')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Number of trades (should decrease)\n",
    "ax2.plot(training_results['episode_trades'])\n",
    "ax2.set_title('Trades per Episode (Lower is Better)')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Number of Trades')\n",
    "ax2.grid(True)\n",
    "\n",
    "# Sharpe ratios\n",
    "ax3.plot(training_results['episode_sharpes'])\n",
    "ax3.axhline(y=0, color='r', linestyle='--')\n",
    "ax3.set_title('Sharpe Ratios')\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Sharpe Ratio')\n",
    "ax3.grid(True)\n",
    "\n",
    "# Moving averages\n",
    "window = 20\n",
    "if len(training_results['episode_profits']) >= window:\n",
    "    ma_profits = pd.Series(training_results['episode_profits']).rolling(window).mean()\n",
    "    ma_trades = pd.Series(training_results['episode_trades']).rolling(window).mean()\n",
    "    \n",
    "    ax4.plot(ma_profits, label='Avg Profit %', color='green')\n",
    "    ax4.plot(ma_trades, label='Avg Trades', color='orange')\n",
    "    ax4.set_title(f'{window}-Episode Moving Averages')\n",
    "    ax4.set_xlabel('Episode')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"Final 10-episode avg profit: {np.mean(training_results['episode_profits'][-10:]):.2f}%\")\n",
    "print(f\"Final 10-episode avg trades: {np.mean(training_results['episode_trades'][-10:]):.1f}\")\n",
    "print(f\"Final 10-episode avg Sharpe: {np.mean(training_results['episode_sharpes'][-10:]):.2f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Enhanced Backtesting\n",
    "\n",
    "# %%\n",
    "def backtest_advanced(agent, env, plot=True):\n",
    "    \"\"\"Backtest with comprehensive metrics\"\"\"\n",
    "    state = env.reset()\n",
    "    \n",
    "    actions = []\n",
    "    prices = []\n",
    "    highs = []\n",
    "    lows = []\n",
    "    equities = []\n",
    "    positions = []\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state, env)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        actions.append(action)\n",
    "        step_idx = env.current_step - 1\n",
    "        prices.append(env.data['close'].iloc[step_idx])\n",
    "        highs.append(env.data['high'].iloc[step_idx])\n",
    "        lows.append(env.data['low'].iloc[step_idx])\n",
    "        equities.append(info['equity'])\n",
    "        positions.append(env.position_history[-1] if env.position_history else 0)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    final_equity = equities[-1]\n",
    "    final_return = (final_equity / env.initial_balance - 1) * 100\n",
    "    \n",
    "    # Risk metrics\n",
    "    equity_array = np.array(equities)\n",
    "    returns = []\n",
    "    if len(equity_array) > 1:\n",
    "        returns = np.diff(equity_array) / equity_array[:-1]\n",
    "    \n",
    "    sharpe_ratio = 0\n",
    "    if len(returns) > 0 and returns.std() > 0:\n",
    "        sharpe_ratio = np.sqrt(252) * returns.mean() / returns.std()\n",
    "    \n",
    "    max_dd = env.max_drawdown * 100\n",
    "    \n",
    "    # Trade analysis\n",
    "    total_trades = env.num_trades\n",
    "    winning_trades = sum(1 for t in env.trades if t.get('pnl', 0) > 0)\n",
    "    win_rate = (winning_trades / max(1, total_trades)) * 100\n",
    "    \n",
    "    print(f\"\\n=== Advanced Backtest Results ===\")\n",
    "    print(f\"Initial Balance: ${env.initial_balance:,.2f}\")\n",
    "    print(f\"Final Equity: ${final_equity:,.2f}\")\n",
    "    print(f\"Total Return: {final_return:.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "    print(f\"Max Drawdown: {max_dd:.2f}%\")\n",
    "    print(f\"Total Trades: {total_trades}\")\n",
    "    print(f\"Win Rate: {win_rate:.1f}%\")\n",
    "    print(f\"Avg Trades per 100 bars: {total_trades / (len(prices) / 100):.1f}\")\n",
    "    \n",
    "    if plot:\n",
    "        plot_backtest_results(env, prices, highs, lows, actions, equities, positions)\n",
    "    \n",
    "    return {\n",
    "        'final_return': final_return,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_dd,\n",
    "        'total_trades': total_trades,\n",
    "        'win_rate': win_rate,\n",
    "        'equities': equities\n",
    "    }\n",
    "\n",
    "def plot_backtest_results(env, prices, highs, lows, actions, equities, positions):\n",
    "    \"\"\"Enhanced plotting with trade markers\"\"\"\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))\n",
    "    \n",
    "    x = range(len(prices))\n",
    "    \n",
    "    # Price chart with positions\n",
    "    for i in x:\n",
    "        ax1.plot([i, i], [lows[i], highs[i]], color='gray', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    ax1.plot(x, prices, label='Close Price', color='black', linewidth=1.5)\n",
    "    \n",
    "    # Color background by position\n",
    "    for i in range(1, len(positions)):\n",
    "        if positions[i] == 1:  # Long\n",
    "            ax1.axvspan(i-1, i, alpha=0.2, color='green')\n",
    "        elif positions[i] == 2:  # Short\n",
    "            ax1.axvspan(i-1, i, alpha=0.2, color='red')\n",
    "    \n",
    "    # Trade markers\n",
    "    for trade in env.trades:\n",
    "        idx = trade['step'] - 100  # Adjust for initial offset\n",
    "        if 0 <= idx < len(prices):\n",
    "            if trade['action'] == 'open_long':\n",
    "                ax1.scatter(idx, trade['price'], marker='^', color='green', s=100, zorder=5)\n",
    "            elif trade['action'] == 'open_short':\n",
    "                ax1.scatter(idx, trade['price'], marker='v', color='red', s=100, zorder=5)\n",
    "            elif trade['action'] in ['close_long', 'close_short']:\n",
    "                color = 'green' if trade.get('pnl', 0) > 0 else 'red'\n",
    "                ax1.scatter(idx, trade['exit_price'], marker='x', color=color, s=100, zorder=5)\n",
    "    \n",
    "    ax1.set_title('Price Action with Positions', fontsize=14)\n",
    "    ax1.set_ylabel('Price (USDT)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Actions\n",
    "    ax2.plot(actions, color='blue', linewidth=1)\n",
    "    ax2.fill_between(range(len(actions)), actions, alpha=0.3)\n",
    "    ax2.set_title('Actions (0=HOLD, 1=LONG, 2=SHORT, 3=EXIT)', fontsize=14)\n",
    "    ax2.set_ylabel('Action')\n",
    "    ax2.set_ylim(-0.5, 3.5)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Portfolio equity with drawdown shading\n",
    "    ax3.plot(equities, label='Portfolio Equity', color='green', linewidth=2)\n",
    "    ax3.axhline(y=env.initial_balance, color='red', linestyle='--', label='Initial Balance')\n",
    "    \n",
    "    # Shade drawdowns\n",
    "    peak = env.initial_balance\n",
    "    for i, equity in enumerate(equities):\n",
    "        if equity > peak:\n",
    "            peak = equity\n",
    "        if equity < peak:\n",
    "            ax3.fill_between([i-1, i], [peak, peak], [equities[i-1], equity], \n",
    "                           color='red', alpha=0.3)\n",
    "    \n",
    "    ax3.set_title('Portfolio Equity with Drawdowns', fontsize=14)\n",
    "    ax3.set_xlabel('Time Steps')\n",
    "    ax3.set_ylabel('Equity (USDT)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run backtest\n",
    "print(\"Running backtest on test data...\")\n",
    "test_results = backtest_advanced(agent, test_env)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 11. Compare with Baseline Strategies\n",
    "\n",
    "# %%\n",
    "def buy_and_hold_backtest(data, initial_balance=10000):\n",
    "    \"\"\"Buy and hold benchmark\"\"\"\n",
    "    start_price = data['close'].iloc[100]\n",
    "    end_price = data['close'].iloc[-1]\n",
    "    \n",
    "    final_value = (initial_balance / start_price) * end_price\n",
    "    total_return = (final_value / initial_balance - 1) * 100\n",
    "    \n",
    "    # Calculate max drawdown\n",
    "    prices = data['close'].iloc[100:]\n",
    "    cumulative_returns = (initial_balance / start_price) * prices\n",
    "    running_max = cumulative_returns.expanding().max()\n",
    "    drawdown = (cumulative_returns - running_max) / running_max\n",
    "    max_drawdown = drawdown.min() * 100\n",
    "    \n",
    "    print(f\"\\n=== Buy & Hold Results ===\")\n",
    "    print(f\"Total Return: {total_return:.2f}%\")\n",
    "    print(f\"Max Drawdown: {max_drawdown:.2f}%\")\n",
    "    \n",
    "    return total_return, max_drawdown\n",
    "\n",
    "def random_trading_backtest(env_data, n_simulations=100):\n",
    "    \"\"\"Random trading baseline\"\"\"\n",
    "    returns = []\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        env = AdvancedFuturesTradingEnv(env_data)\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            # Random valid action\n",
    "            if env.position is None:\n",
    "                action = np.random.choice([0, 1, 2])  # HOLD, LONG, SHORT\n",
    "            else:\n",
    "                action = np.random.choice([0, 3])  # HOLD, EXIT\n",
    "            \n",
    "            _, _, done, info = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        final_return = (info['equity'] / env.initial_balance - 1) * 100\n",
    "        returns.append(final_return)\n",
    "    \n",
    "    avg_return = np.mean(returns)\n",
    "    std_return = np.std(returns)\n",
    "    \n",
    "    print(f\"\\n=== Random Trading Results ===\")\n",
    "    print(f\"Average Return: {avg_return:.2f}% ± {std_return:.2f}%\")\n",
    "    \n",
    "    return avg_return, std_return\n",
    "\n",
    "# Run comparisons\n",
    "bh_return, bh_dd = buy_and_hold_backtest(test_data)\n",
    "random_return, random_std = random_trading_backtest(test_data, n_simulations=50)\n",
    "\n",
    "print(f\"\\n=== Strategy Comparison ===\")\n",
    "print(f\"Advanced DQN Return: {test_results['final_return']:.2f}%\")\n",
    "print(f\"Advanced DQN Sharpe: {test_results['sharpe_ratio']:.2f}\")\n",
    "print(f\"Buy & Hold Return: {bh_return:.2f}%\")\n",
    "print(f\"Random Trading Return: {random_return:.2f}%\")\n",
    "print(f\"\\nDQN vs Buy & Hold: {test_results['final_return'] - bh_return:+.2f}%\")\n",
    "print(f\"DQN vs Random: {test_results['final_return'] - random_return:+.2f}%\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 12. Save Model and Results\n",
    "\n",
    "# %%\n",
    "# Save model\n",
    "model_path = Path('advanced_futures_dqn.pth')\n",
    "torch.save({\n",
    "    'model_state_dict': agent.q_network.state_dict(),\n",
    "    'target_state_dict': agent.target_network.state_dict(),\n",
    "    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save comprehensive results\n",
    "results = {\n",
    "    'test_results': {\n",
    "        'final_return': test_results['final_return'],\n",
    "        'sharpe_ratio': test_results['sharpe_ratio'],\n",
    "        'max_drawdown': test_results['max_drawdown'],\n",
    "        'total_trades': test_results['total_trades'],\n",
    "        'win_rate': test_results['win_rate']\n",
    "    },\n",
    "    'training_results': {\n",
    "        'final_profit': training_results['episode_profits'][-1],\n",
    "        'final_trades': training_results['episode_trades'][-1],\n",
    "        'final_sharpe': training_results['episode_sharpes'][-1]\n",
    "    },\n",
    "    'comparison': {\n",
    "        'buy_hold_return': bh_return,\n",
    "        'random_return': random_return,\n",
    "        'outperformance_bh': test_results['final_return'] - bh_return,\n",
    "        'outperformance_random': test_results['final_return'] - random_return\n",
    "    },\n",
    "    'model_config': {\n",
    "        'architecture': 'Dueling DQN with Noisy Networks',\n",
    "        'replay': 'Prioritized Experience Replay',\n",
    "        'features': len(feature_columns),\n",
    "        'hidden_dim': 512,\n",
    "        'learning_rate': 5e-5\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('advanced_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to advanced_results.json\")\n",
    "print(\"\\n=== Advanced Futures Trading Bot Complete! ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
