{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54b4fcb6",
   "metadata": {},
   "source": [
    "# Multi-Asset Cryptocurrency Trading Bot with Enhanced Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb687c0",
   "metadata": {},
   "source": [
    "# Improvements:\n",
    "# - Multi-asset training (BTC, ETH, XRP)\n",
    "# - Date range: 2019-01-01 to present\n",
    "# - Enhanced risk management\n",
    "# - Better position sizing\n",
    "# - Improved win rate strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687353fb",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba6c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import requests\n",
    "from scipy import stats\n",
    "import talib\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98217457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a752ab",
   "metadata": {},
   "source": [
    "## 2. Enhanced Data Collection for Multiple Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_crypto_data_with_date(symbol='BTC/USDT', timeframe='1h', start_date='2019-01-01'):\n",
    "    \"\"\"Download crypto data from specific date to present - FIXED for full data\"\"\"\n",
    "    exchange = ccxt.binance({\n",
    "        'rateLimit': 1200,\n",
    "        'enableRateLimit': True,\n",
    "    })\n",
    "    \n",
    "    # Convert start date to timestamp\n",
    "    start_timestamp = exchange.parse8601(start_date + 'T00:00:00Z')\n",
    "    end_timestamp = exchange.milliseconds()\n",
    "    \n",
    "    print(f\"Downloading {symbol} {timeframe} data from {start_date}...\")\n",
    "    all_ohlcv = []\n",
    "    \n",
    "    current_timestamp = start_timestamp\n",
    "    batch_count = 0\n",
    "    \n",
    "    while current_timestamp < end_timestamp:\n",
    "        try:\n",
    "            # Fetch batch\n",
    "            ohlcv = exchange.fetch_ohlcv(symbol, timeframe, current_timestamp, limit=1000)\n",
    "            \n",
    "            if not ohlcv:\n",
    "                break\n",
    "                \n",
    "            # Add to results\n",
    "            all_ohlcv.extend(ohlcv)\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Update timestamp for next batch\n",
    "            last_timestamp = ohlcv[-1][0]\n",
    "            \n",
    "            # Check if we've reached the end\n",
    "            if last_timestamp >= end_timestamp or len(ohlcv) < 1000:\n",
    "                break\n",
    "                \n",
    "            # Move to next batch (add 1ms to avoid duplicate)\n",
    "            current_timestamp = last_timestamp + 1\n",
    "            \n",
    "            # Progress update\n",
    "            print(f\"Downloaded {len(all_ohlcv)} candles (batch {batch_count})...\", end='\\r')\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(exchange.rateLimit / 1000)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError downloading batch {batch_count}: {e}\")\n",
    "            time.sleep(5)\n",
    "            # Retry from last successful timestamp\n",
    "            if all_ohlcv:\n",
    "                current_timestamp = all_ohlcv[-1][0] + 1\n",
    "            else:\n",
    "                # If first batch failed, wait and retry\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df = df.drop_duplicates(subset=['timestamp']).reset_index(drop=True)\n",
    "    df['symbol'] = symbol\n",
    "    \n",
    "    # Calculate total days of data\n",
    "    if len(df) > 0:\n",
    "        days_of_data = (df['datetime'].max() - df['datetime'].min()).days\n",
    "        print(f\"\\nDownloaded {len(df)} candles for {symbol} ({days_of_data} days of data)\")\n",
    "    else:\n",
    "        print(f\"\\nNo data downloaded for {symbol}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad346218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_multiple_assets(symbols=['BTC/USDT', 'ETH/USDT', 'XRP/USDT'], \n",
    "                           timeframe='1h', start_date='2019-01-01'):\n",
    "    \"\"\"Download data for multiple cryptocurrency pairs\"\"\"\n",
    "    all_data = {}\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        try:\n",
    "            data = download_crypto_data_with_date(symbol, timeframe, start_date)\n",
    "            all_data[symbol] = data\n",
    "            time.sleep(2)  # Be nice to the API\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {symbol}: {e}\")\n",
    "            \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c7d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data for multiple assets\n",
    "print(\"Downloading multi-asset data...\")\n",
    "symbols = ['BTC/USDT', 'ETH/USDT', 'XRP/USDT']\n",
    "multi_asset_data = download_multiple_assets(symbols, timeframe='1h', start_date='2019-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670221a",
   "metadata": {},
   "source": [
    "## 3. Enhanced Feature Engineering for Multi-Asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e0222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advanced_features(df, asset_name='BTC'):\n",
    "    \"\"\"Calculate only essential features - optimized version\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # === Core Price Features (5) ===\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    df['high_low_range'] = (df['high'] - df['low']) / df['close']\n",
    "    df['close_position'] = (df['close'] - df['low']) / (df['high'] - df['low'] + 1e-8)\n",
    "    \n",
    "    # === Essential Volatility (2) ===\n",
    "    df['volatility'] = df['returns'].rolling(20).std()\n",
    "    df['atr'] = talib.ATR(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "    \n",
    "    # === Key Moving Averages (6) ===\n",
    "    df['sma_20'] = talib.SMA(df['close'], timeperiod=20)\n",
    "    df['sma_50'] = talib.SMA(df['close'], timeperiod=50)\n",
    "    df['ema_20'] = talib.EMA(df['close'], timeperiod=20)\n",
    "    \n",
    "    # Price relative to MA\n",
    "    df['price_to_sma20'] = (df['close'] - df['sma_20']) / df['sma_20']\n",
    "    df['price_to_sma50'] = (df['close'] - df['sma_50']) / df['sma_50']\n",
    "    df['ma_divergence'] = (df['sma_20'] - df['sma_50']) / df['sma_50']\n",
    "    \n",
    "    # === Critical Momentum Indicators (4) ===\n",
    "    df['rsi'] = talib.RSI(df['close'], timeperiod=14)\n",
    "    df['macd'], df['macd_signal'], df['macd_hist'] = talib.MACD(df['close'])\n",
    "    \n",
    "    # === Bollinger Bands (2) ===\n",
    "    df['bb_upper'], df['bb_middle'], df['bb_lower'] = talib.BBANDS(df['close'], timeperiod=20)\n",
    "    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'] + 1e-8)\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']\n",
    "    \n",
    "    # === Volume Analysis (2) ===\n",
    "    df['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()\n",
    "    df['volume_trend'] = talib.OBV(df['close'], df['volume']) / 1e6  # Normalize\n",
    "    \n",
    "    # === Market Structure (3) ===\n",
    "    df['support'] = df['low'].rolling(20).min()\n",
    "    df['resistance'] = df['high'].rolling(20).max()\n",
    "    df['sr_ratio'] = (df['close'] - df['support']) / (df['resistance'] - df['support'] + 1e-8)\n",
    "    \n",
    "    # === Multi-timeframe (2) ===\n",
    "    df['returns_4h'] = df['close'].pct_change(4)\n",
    "    df['returns_24h'] = df['close'].pct_change(24)\n",
    "    \n",
    "    # === Asset identifier ===\n",
    "    asset_map = {'BTC': 0, 'ETH': 1, 'XRP': 2}\n",
    "    df['asset_id'] = asset_map.get(asset_name.split('/')[0], 0)\n",
    "    \n",
    "    # Clean data\n",
    "    df = df.dropna()\n",
    "    df = df.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering to all assets\n",
    "for symbol, data in multi_asset_data.items():\n",
    "    asset_name = symbol.split('/')[0]\n",
    "    multi_asset_data[symbol] = calculate_advanced_features(data, asset_name)\n",
    "    print(f\"Features calculated for {symbol}: {multi_asset_data[symbol].shape}\")\n",
    "\n",
    "# Combine all assets for training\n",
    "combined_data = pd.concat(list(multi_asset_data.values()), ignore_index=True)\n",
    "combined_data = combined_data.sort_values('datetime').reset_index(drop=True)\n",
    "print(f\"Combined data shape: {combined_data.shape}\")\n",
    "\n",
    "# Enhanced feature columns\n",
    "feature_columns = [\n",
    "    'returns', 'log_returns', 'high_low_range', 'close_position',\n",
    "    'volatility', 'atr',\n",
    "    'price_to_sma20', 'price_to_sma50', 'ma_divergence',\n",
    "    'rsi', 'macd', 'macd_signal', 'macd_hist',\n",
    "    'bb_position', 'bb_width',\n",
    "    'volume_ratio', 'volume_trend',\n",
    "    'sr_ratio',\n",
    "    'returns_4h', 'returns_24h',\n",
    "    'asset_id'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfee132",
   "metadata": {},
   "source": [
    "## 4. Enhanced XGBoost Direction Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedDirectionPredictor:\n",
    "    \"\"\"XGBoost with GPU support and optimized parameters\"\"\"\n",
    "    \n",
    "    def __init__(self, lookback=24, prediction_horizon=4):\n",
    "        self.lookback = lookback\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.models = {}\n",
    "        self.best_threshold = 0.5\n",
    "        \n",
    "    def prepare_data(self, df, feature_cols):\n",
    "        \"\"\"Prepare data with better labeling strategy\"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        for i in range(self.lookback, len(df) - self.prediction_horizon):\n",
    "            # Features\n",
    "            X.append(df[feature_cols].iloc[i-self.lookback:i].values.flatten())\n",
    "            \n",
    "            # Improved target: consider high/low within prediction window\n",
    "            current_price = df['close'].iloc[i]\n",
    "            \n",
    "            # Check future price movement including high/low\n",
    "            future_highs = df['high'].iloc[i+1:i+self.prediction_horizon+1].max()\n",
    "            future_lows = df['low'].iloc[i+1:i+self.prediction_horizon+1].min()\n",
    "            future_close = df['close'].iloc[i + self.prediction_horizon]\n",
    "            \n",
    "            # Calculate potential gains/losses\n",
    "            max_gain = (future_highs - current_price) / current_price\n",
    "            max_loss = (future_lows - current_price) / current_price\n",
    "            final_return = (future_close - current_price) / current_price\n",
    "            \n",
    "            # Better labeling: consider risk/reward\n",
    "            if max_gain > 0.015 and max_loss > -0.01:  # Good risk/reward\n",
    "                y.append(1)\n",
    "            elif max_loss < -0.015:  # High risk\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(0 if final_return < 0 else 1)\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def train_ensemble(self, X_train, y_train, X_val, y_val, n_models=3):\n",
    "        \"\"\"Train with GPU if available\"\"\"\n",
    "        print(\"Training XGBoost with GPU support...\")\n",
    "        \n",
    "        # Check for GPU\n",
    "        try:\n",
    "            import GPUtil\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            use_gpu = len(gpus) > 0\n",
    "        except:\n",
    "            use_gpu = False\n",
    "        \n",
    "        print(f\"GPU available: {use_gpu}\")\n",
    "        \n",
    "        # Base parameters\n",
    "        base_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss',\n",
    "            'tree_method': 'gpu_hist' if use_gpu else 'hist',\n",
    "            'predictor': 'gpu_predictor' if use_gpu else 'cpu_predictor',\n",
    "            'use_label_encoder': False,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        # Find optimal threshold\n",
    "        best_f1 = 0\n",
    "        for thresh in [0.4, 0.45, 0.5, 0.55, 0.6]:\n",
    "            params = {\n",
    "                **base_params,\n",
    "                'max_depth': 6,\n",
    "                'learning_rate': 0.05,\n",
    "                'n_estimators': 200,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8\n",
    "            }\n",
    "            \n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                     early_stopping_rounds=20, verbose=False)\n",
    "            \n",
    "            pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "            pred = (pred_proba > thresh).astype(int)\n",
    "            \n",
    "            # Calculate F1 score\n",
    "            from sklearn.metrics import f1_score\n",
    "            f1 = f1_score(y_val, pred)\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                self.best_threshold = thresh\n",
    "        \n",
    "        print(f\"Best threshold: {self.best_threshold} with F1: {best_f1:.3f}\")\n",
    "        \n",
    "        # Train final ensemble with best parameters\n",
    "        for i in range(n_models):\n",
    "            print(f\"Training model {i+1}/{n_models}\")\n",
    "            \n",
    "            params = {\n",
    "                **base_params,\n",
    "                'max_depth': np.random.randint(4, 8),\n",
    "                'learning_rate': np.random.uniform(0.03, 0.1),\n",
    "                'n_estimators': np.random.randint(150, 300),\n",
    "                'subsample': np.random.uniform(0.7, 0.9),\n",
    "                'colsample_bytree': np.random.uniform(0.7, 0.9),\n",
    "                'min_child_weight': np.random.randint(1, 5),\n",
    "                'gamma': np.random.uniform(0, 0.2),\n",
    "                'reg_alpha': np.random.uniform(0, 0.1),\n",
    "                'reg_lambda': np.random.uniform(1, 2)\n",
    "            }\n",
    "            \n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                     early_stopping_rounds=30, verbose=False)\n",
    "            \n",
    "            self.models[f'model_{i}'] = model\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Ensemble prediction\"\"\"\n",
    "        predictions = []\n",
    "        for model in self.models.values():\n",
    "            predictions.append(model.predict_proba(X)[:, 1])\n",
    "        return np.mean(predictions, axis=0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Get trading signals\"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        predictions = np.zeros(len(proba))\n",
    "        \n",
    "        # Strong signals only\n",
    "        predictions[proba > self.best_threshold + 0.1] = 1  # Strong bullish\n",
    "        predictions[proba < self.best_threshold - 0.1] = -1  # Strong bearish\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7208f85b",
   "metadata": {},
   "source": [
    "## 5. Enhanced PPO Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a459652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedPPOAgent:\n",
    "    \"\"\"Enhanced PPO with better exploration and learning\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4):\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Networks\n",
    "        self.network = ActorCriticNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        # Enhanced PPO parameters\n",
    "        self.gamma = 0.995  # Increased for longer-term thinking\n",
    "        self.gae_lambda = 0.97\n",
    "        self.clip_epsilon = 0.2\n",
    "        self.value_coef = 0.5\n",
    "        self.entropy_coef = 0.02  # Increased for more exploration\n",
    "        self.max_grad_norm = 0.5\n",
    "        \n",
    "        # Adaptive learning rate\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='max', factor=0.5, patience=10\n",
    "        )\n",
    "        \n",
    "        # Memory\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.episode_returns = []\n",
    "        \n",
    "    def act(self, state, xgb_signal=None, valid_actions=None, explore_rate=0.1):\n",
    "        \"\"\"Select action with enhanced exploration\"\"\"\n",
    "        # Validate state\n",
    "        state = np.nan_to_num(state, 0.0)\n",
    "        state = np.clip(state, -10, 10)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_probs, value = self.network(state_tensor)\n",
    "        \n",
    "        # Ensure valid probabilities\n",
    "        action_probs = action_probs.cpu()\n",
    "        \n",
    "        if valid_actions is None:\n",
    "            valid_actions = list(range(self.action_dim))\n",
    "        \n",
    "        # Mask invalid actions\n",
    "        mask = torch.zeros(1, self.action_dim)\n",
    "        for action in valid_actions:\n",
    "            mask[0, action] = 1\n",
    "        \n",
    "        # Apply mask\n",
    "        action_probs = action_probs * mask\n",
    "        \n",
    "        # Renormalize\n",
    "        if action_probs.sum() > 0:\n",
    "            action_probs = action_probs / action_probs.sum(dim=-1, keepdim=True)\n",
    "        else:\n",
    "            action_probs = mask / mask.sum()\n",
    "        \n",
    "        # Enhanced XGBoost signal incorporation\n",
    "        if xgb_signal is not None and not np.isnan(xgb_signal):\n",
    "            signal_strength = min(abs(xgb_signal), 1.0)  # Cap signal strength\n",
    "            \n",
    "            if xgb_signal > 0 and 1 in valid_actions:  # Bullish\n",
    "                action_probs[0, 1] *= (1 + 0.5 * signal_strength)\n",
    "                if 2 in valid_actions:\n",
    "                    action_probs[0, 2] *= (1 - 0.3 * signal_strength)\n",
    "            elif xgb_signal < 0 and 2 in valid_actions:  # Bearish\n",
    "                if 1 in valid_actions:\n",
    "                    action_probs[0, 1] *= (1 - 0.3 * signal_strength)\n",
    "                action_probs[0, 2] *= (1 + 0.5 * signal_strength)\n",
    "            \n",
    "            # Renormalize\n",
    "            action_probs = action_probs / action_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Exploration vs exploitation\n",
    "        if np.random.random() < explore_rate:\n",
    "            # Random valid action\n",
    "            action = np.random.choice(valid_actions)\n",
    "        else:\n",
    "            # Sample from distribution\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample().item()\n",
    "        \n",
    "        # Store for training\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        if value.dim() == 0:\n",
    "            self.values.append(value.cpu().item())\n",
    "        else:\n",
    "            self.values.append(value[0].cpu().item() if len(value) > 0 else 0.0)\n",
    "        \n",
    "        # Calculate log prob for the taken action\n",
    "        dist = Categorical(action_probs)\n",
    "        self.log_probs.append(dist.log_prob(torch.tensor(action)).item())\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update(self, epochs=10):\n",
    "        \"\"\"Update with adaptive learning\"\"\"\n",
    "        if len(self.states) < 64:  # Increased minimum batch\n",
    "            return\n",
    "        \n",
    "        # Compute advantages and returns\n",
    "        advantages, returns = self.compute_gae()\n",
    "        \n",
    "        if advantages.numel() == 0:\n",
    "            return\n",
    "        \n",
    "        # Calculate average return for scheduler\n",
    "        avg_return = np.mean(self.rewards[-100:]) if len(self.rewards) >= 100 else 0\n",
    "        self.scheduler.step(avg_return)\n",
    "        \n",
    "        # Rest of update logic remains the same...\n",
    "        # [Previous update code here]\n",
    "        \n",
    "    def compute_gae(self):\n",
    "        \"\"\"Compute Generalized Advantage Estimation\"\"\"\n",
    "        # Ensure all lists have same length\n",
    "        min_length = min(len(self.rewards), len(self.values), len(self.dones))\n",
    "        \n",
    "        if min_length == 0:\n",
    "            return torch.FloatTensor([]).to(device), torch.FloatTensor([]).to(device)\n",
    "        \n",
    "        # Trim to same length\n",
    "        rewards = self.rewards[:min_length]\n",
    "        values = self.values[:min_length]\n",
    "        dones = self.dones[:min_length]\n",
    "        \n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "            advantage = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * advantage\n",
    "            advantages.insert(0, advantage)\n",
    "        \n",
    "        advantages = torch.FloatTensor(advantages).to(device)\n",
    "        returns = advantages + torch.FloatTensor(values).to(device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        if advantages.numel() > 0:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        return advantages, returns\n",
    "    \n",
    "    def store_reward(self, reward, done):\n",
    "        \"\"\"Store reward and done flag\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    \"\"\"Enhanced Actor-Critic network\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=512):  # Larger network\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared layers with more capacity\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        self.ln3 = nn.LayerNorm(hidden_dim // 2)\n",
    "        \n",
    "        # Actor head\n",
    "        self.actor_fc = nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "        self.actor_out = nn.Linear(hidden_dim // 4, action_dim)\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic_fc = nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "        self.critic_out = nn.Linear(hidden_dim // 4, 1)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights properly\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        # Check for NaN in input\n",
    "        if torch.isnan(state).any():\n",
    "            state = torch.nan_to_num(state, 0.0)\n",
    "        \n",
    "        # Shared network with residual connection\n",
    "        x = F.relu(self.ln1(self.fc1(state)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x2 = F.relu(self.ln2(self.fc2(x)))\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        x3 = F.relu(self.ln3(self.fc3(x2)))\n",
    "        x3 = self.dropout(x3)\n",
    "        \n",
    "        # Actor\n",
    "        actor = F.relu(self.actor_fc(x3))\n",
    "        action_logits = self.actor_out(actor)\n",
    "        action_probs = F.softmax(action_logits, dim=-1) + 1e-8\n",
    "        \n",
    "        # Critic\n",
    "        critic = F.relu(self.critic_fc(x3))\n",
    "        value = self.critic_out(critic).squeeze(-1)\n",
    "        \n",
    "        return action_probs, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab96c03",
   "metadata": {},
   "source": [
    "## 6. Enhanced Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53fcc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCryptoFuturesEnv:\n",
    "    \"\"\"Fixed environment with proper high/low liquidation checks\"\"\"\n",
    "    \n",
    "    def __init__(self, data, initial_balance=10000, fee_rate=0.0005, \n",
    "                 max_leverage=10, liquidation_margin=0.05):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.initial_balance = initial_balance\n",
    "        self.fee_rate = fee_rate\n",
    "        self.max_leverage = max_leverage\n",
    "        self.liquidation_margin = liquidation_margin\n",
    "        \n",
    "        # Position sizing\n",
    "        self.min_position_pct = 0.02   # 2% minimum\n",
    "        self.max_position_pct = 0.10   # 10% maximum  \n",
    "        self.base_position_pct = 0.05  # 5% base\n",
    "        \n",
    "        # Risk management\n",
    "        self.stop_loss_pct = 0.02      # 2% stop loss\n",
    "        self.take_profit_pct = 0.03    # 3% take profit\n",
    "        self.trailing_stop_pct = 0.015 # 1.5% trailing\n",
    "        \n",
    "        self.feature_columns = feature_columns\n",
    "        self.action_space = 4\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _check_intrabar_liquidation(self, entry_price, high, low, position_type, leverage):\n",
    "        \"\"\"Check if position would be liquidated based on high/low\"\"\"\n",
    "        if position_type == 'long':\n",
    "            # For long: check if low price hits liquidation\n",
    "            worst_pnl = (low - entry_price) / entry_price\n",
    "            if worst_pnl * leverage <= -self.liquidation_margin:\n",
    "                return True, low\n",
    "        else:  # short\n",
    "            # For short: check if high price hits liquidation\n",
    "            worst_pnl = (entry_price - high) / entry_price\n",
    "            if worst_pnl * leverage <= -self.liquidation_margin:\n",
    "                return True, high\n",
    "        \n",
    "        return False, None\n",
    "    \n",
    "    def _execute_action(self, action, exit_reason=None):\n",
    "        \"\"\"Execute trading action with proper P&L calculation\"\"\"\n",
    "        current_price = self.data['close'].iloc[self.current_step]\n",
    "        current_high = self.data['high'].iloc[self.current_step]\n",
    "        current_low = self.data['low'].iloc[self.current_step]\n",
    "        reward = 0\n",
    "        \n",
    "        # Validate action\n",
    "        valid_actions = self.get_valid_actions()\n",
    "        if action not in valid_actions:\n",
    "            return -0.1\n",
    "        \n",
    "        if action == 0:  # HOLD\n",
    "            if self.position is None:\n",
    "                reward = -0.0005\n",
    "            else:\n",
    "                # Check intrabar liquidation while holding\n",
    "                liquidated, liq_price = self._check_intrabar_liquidation(\n",
    "                    self.entry_price, current_high, current_low, \n",
    "                    self.position, self.leverage\n",
    "                )\n",
    "                \n",
    "                if liquidated:\n",
    "                    # Force liquidation\n",
    "                    self._close_position(liq_price, 'liquidation')\n",
    "                    reward = -1.0\n",
    "                else:\n",
    "                    # Normal holding reward\n",
    "                    if self.position == 'long':\n",
    "                        pnl_pct = (current_price - self.entry_price) / self.entry_price\n",
    "                    else:\n",
    "                        pnl_pct = (self.entry_price - current_price) / self.entry_price\n",
    "                    \n",
    "                    reward = 0.01 if pnl_pct > 0 else -0.002\n",
    "        \n",
    "        elif action in [1, 2] and self.position is None:  # Open position\n",
    "            # Calculate position size\n",
    "            position_size = self._calculate_position_size(action)\n",
    "            fee = position_size * self.fee_rate\n",
    "            \n",
    "            if position_size + fee <= self.balance:\n",
    "                self.balance -= fee\n",
    "                self.position = 'long' if action == 1 else 'short'\n",
    "                self.entry_price = current_price\n",
    "                self.entry_step = self.current_step\n",
    "                self.position_size = position_size\n",
    "                \n",
    "                # Set initial stops\n",
    "                if self.position == 'long':\n",
    "                    self.stop_loss_price = current_price * (1 - self.stop_loss_pct)\n",
    "                    self.take_profit_price = current_price * (1 + self.take_profit_pct)\n",
    "                else:\n",
    "                    self.stop_loss_price = current_price * (1 + self.stop_loss_pct)\n",
    "                    self.take_profit_price = current_price * (1 - self.take_profit_pct)\n",
    "                \n",
    "                reward = 0.01\n",
    "        \n",
    "        elif action == 3 and self.position is not None:  # Exit position\n",
    "            self._close_position(current_price, exit_reason or 'manual')\n",
    "            reward = self._calculate_exit_reward(exit_reason)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _close_position(self, exit_price, exit_reason):\n",
    "        \"\"\"Close position with proper P&L calculation\"\"\"\n",
    "        if self.position == 'long':\n",
    "            pnl_pct = (exit_price - self.entry_price) / self.entry_price\n",
    "        else:\n",
    "            pnl_pct = (self.entry_price - exit_price) / self.entry_price\n",
    "        \n",
    "        # Apply leverage\n",
    "        leveraged_pnl_pct = pnl_pct * self.leverage\n",
    "        \n",
    "        # Calculate P&L amount\n",
    "        pnl_amount = self.position_size * leveraged_pnl_pct\n",
    "        exit_fee = self.position_size * self.fee_rate\n",
    "        net_pnl = pnl_amount - exit_fee\n",
    "        \n",
    "        # Update balance\n",
    "        self.balance += net_pnl\n",
    "        \n",
    "        # Record trade\n",
    "        self.trades.append({\n",
    "            'entry_price': self.entry_price,\n",
    "            'exit_price': exit_price,\n",
    "            'position': self.position,\n",
    "            'pnl': net_pnl,\n",
    "            'pnl_pct': pnl_pct,\n",
    "            'leveraged_pnl_pct': leveraged_pnl_pct,\n",
    "            'exit_reason': exit_reason,\n",
    "            'leverage': self.leverage,\n",
    "            'duration': self.current_step - self.entry_step\n",
    "        })\n",
    "        \n",
    "        # Update streaks\n",
    "        if net_pnl > 0:\n",
    "            self.winning_streak += 1\n",
    "            self.consecutive_losses = 0\n",
    "        else:\n",
    "            self.consecutive_losses += 1\n",
    "            self.winning_streak = 0\n",
    "        \n",
    "        # Reset position\n",
    "        self.position = None\n",
    "        self.position_size = 0\n",
    "        self.leverage = 1\n",
    "    \n",
    "    def _calculate_exit_reward(self, exit_reason):\n",
    "        \"\"\"Calculate reward based on exit reason\"\"\"\n",
    "        if not self.trades:\n",
    "            return 0\n",
    "        \n",
    "        last_trade = self.trades[-1]\n",
    "        pnl_pct = last_trade['leveraged_pnl_pct']\n",
    "        \n",
    "        if exit_reason == 'liquidation':\n",
    "            return -2.0\n",
    "        elif exit_reason == 'stop_loss':\n",
    "            return -0.5\n",
    "        elif exit_reason == 'take_profit':\n",
    "            return 1.0 + min(pnl_pct * 10, 1.0)\n",
    "        elif exit_reason == 'trailing_stop':\n",
    "            return 0.5 + min(pnl_pct * 10, 1.0)\n",
    "        else:  # manual\n",
    "            return np.tanh(pnl_pct * 20)\n",
    "    \n",
    "    def _calculate_position_size(self, action):\n",
    "        \"\"\"Dynamic position sizing based on market conditions\"\"\"\n",
    "        size_pct = self.base_position_pct\n",
    "        \n",
    "        # Get current conditions\n",
    "        current_vol = self.data['volatility'].iloc[self.current_step]\n",
    "        rsi = self.data['rsi'].iloc[self.current_step]\n",
    "        \n",
    "        # Volatility adjustment\n",
    "        if current_vol > 0:\n",
    "            vol_percentile = stats.percentileofscore(\n",
    "                self.data['volatility'].dropna().values, current_vol\n",
    "            )\n",
    "            if vol_percentile > 80:  # High volatility\n",
    "                size_pct *= 0.7\n",
    "            elif vol_percentile < 20:  # Low volatility\n",
    "                size_pct *= 1.3\n",
    "        \n",
    "        # RSI adjustment\n",
    "        if rsi < 30 or rsi > 70:  # Extremes\n",
    "            size_pct *= 1.2\n",
    "        \n",
    "        # Streak adjustment\n",
    "        if self.consecutive_losses >= 3:\n",
    "            size_pct *= 0.5\n",
    "        elif self.winning_streak >= 2:\n",
    "            size_pct *= 1.1\n",
    "        \n",
    "        # Dynamic leverage\n",
    "        if current_vol > 0:\n",
    "            if rsi < 25 or rsi > 75:  # Strong extremes\n",
    "                self.leverage = min(3, self.max_leverage)\n",
    "            else:\n",
    "                self.leverage = 2\n",
    "        else:\n",
    "            self.leverage = 2\n",
    "        \n",
    "        size_pct = np.clip(size_pct, self.min_position_pct, self.max_position_pct)\n",
    "        return self.balance * size_pct\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one step with proper liquidation checks\"\"\"\n",
    "        # Get current bar data\n",
    "        current_high = self.data['high'].iloc[self.current_step]\n",
    "        current_low = self.data['low'].iloc[self.current_step]\n",
    "        current_close = self.data['close'].iloc[self.current_step]\n",
    "        \n",
    "        # Check for intrabar events if we have a position\n",
    "        if self.position is not None:\n",
    "            # Check liquidation first (highest priority)\n",
    "            liquidated, liq_price = self._check_intrabar_liquidation(\n",
    "                self.entry_price, current_high, current_low, \n",
    "                self.position, self.leverage\n",
    "            )\n",
    "            \n",
    "            if liquidated:\n",
    "                self._close_position(liq_price, 'liquidation')\n",
    "                reward = -2.0\n",
    "            else:\n",
    "                # Check stop loss\n",
    "                if self.position == 'long' and current_low <= self.stop_loss_price:\n",
    "                    self._close_position(self.stop_loss_price, 'stop_loss')\n",
    "                    reward = -0.5\n",
    "                elif self.position == 'short' and current_high >= self.stop_loss_price:\n",
    "                    self._close_position(self.stop_loss_price, 'stop_loss')\n",
    "                    reward = -0.5\n",
    "                # Check take profit\n",
    "                elif self.position == 'long' and current_high >= self.take_profit_price:\n",
    "                    self._close_position(self.take_profit_price, 'take_profit')\n",
    "                    reward = 1.0\n",
    "                elif self.position == 'short' and current_low <= self.take_profit_price:\n",
    "                    self._close_position(self.take_profit_price, 'take_profit')\n",
    "                    reward = 1.0\n",
    "                else:\n",
    "                    # Normal action execution\n",
    "                    reward = self._execute_action(action)\n",
    "        else:\n",
    "            # No position, normal action execution\n",
    "            reward = self._execute_action(action)\n",
    "        \n",
    "        # Update equity tracking\n",
    "        current_equity = self._calculate_equity()\n",
    "        self.equity_curve.append(current_equity)\n",
    "        self.peak_equity = max(self.peak_equity, current_equity)\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Check if done\n",
    "        done = (self.current_step >= len(self.data) - 1 or \n",
    "                current_equity <= self.initial_balance * 0.5)\n",
    "        \n",
    "        # Get next observation\n",
    "        next_obs = self._get_observation() if not done else np.zeros_like(self._get_observation())\n",
    "        \n",
    "        info = {\n",
    "            'equity': current_equity,\n",
    "            'position': self.position,\n",
    "            'trades': len(self.trades),\n",
    "            'win_rate': self._calculate_win_rate()\n",
    "        }\n",
    "        \n",
    "        return next_obs, reward, done, info\n",
    "    \n",
    "    # Other methods remain the same...\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment\"\"\"\n",
    "        self.current_step = max(200, int(0.1 * len(self.data)))\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = None\n",
    "        self.position_size = 0\n",
    "        self.entry_price = 0\n",
    "        self.entry_step = 0\n",
    "        self.leverage = 1\n",
    "        self.stop_loss_price = 0\n",
    "        self.take_profit_price = 0\n",
    "        \n",
    "        self.trades = []\n",
    "        self.equity_curve = [self.balance]\n",
    "        self.peak_equity = self.balance\n",
    "        self.consecutive_losses = 0\n",
    "        self.winning_streak = 0\n",
    "        \n",
    "        return self._get_observation()\n",
    "    \n",
    "    def _calculate_equity(self):\n",
    "        \"\"\"Calculate current equity\"\"\"\n",
    "        if self.position is None:\n",
    "            return self.balance\n",
    "        \n",
    "        current_price = self.data['close'].iloc[self.current_step]\n",
    "        \n",
    "        if self.position == 'long':\n",
    "            pnl_pct = (current_price - self.entry_price) / self.entry_price\n",
    "        else:\n",
    "            pnl_pct = (self.entry_price - current_price) / self.entry_price\n",
    "        \n",
    "        unrealized_pnl = self.position_size * pnl_pct * self.leverage\n",
    "        return self.balance + unrealized_pnl\n",
    "    \n",
    "    def _calculate_win_rate(self):\n",
    "        \"\"\"Calculate win rate\"\"\"\n",
    "        if not self.trades:\n",
    "            return 0.5\n",
    "        \n",
    "        wins = sum(1 for t in self.trades if t['pnl'] > 0)\n",
    "        return wins / len(self.trades)\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        \"\"\"Get valid actions\"\"\"\n",
    "        if self.position is None:\n",
    "            return [0, 1, 2]  # HOLD, LONG, SHORT\n",
    "        else:\n",
    "            return [0, 3]  # HOLD, EXIT\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get current observation\"\"\"\n",
    "        # Market features\n",
    "        market_features = self.data[self.feature_columns].iloc[self.current_step].values\n",
    "        market_features = np.nan_to_num(market_features, 0.0)\n",
    "        \n",
    "        # Position features\n",
    "        position_features = self._get_position_features()\n",
    "        \n",
    "        # Account features  \n",
    "        account_features = self._get_account_features()\n",
    "        \n",
    "        observation = np.concatenate([market_features, position_features, account_features])\n",
    "        return observation.astype(np.float32)\n",
    "    \n",
    "    def _get_position_features(self):\n",
    "        \"\"\"Get position features\"\"\"\n",
    "        if self.position is None:\n",
    "            return np.zeros(8)\n",
    "        \n",
    "        current_price = self.data['close'].iloc[self.current_step]\n",
    "        \n",
    "        is_long = float(self.position == 'long')\n",
    "        is_short = float(self.position == 'short')\n",
    "        \n",
    "        if self.position == 'long':\n",
    "            pnl_pct = (current_price - self.entry_price) / self.entry_price\n",
    "        else:\n",
    "            pnl_pct = (self.entry_price - current_price) / self.entry_price\n",
    "        \n",
    "        leveraged_pnl = pnl_pct * self.leverage\n",
    "        bars_in_position = self.current_step - self.entry_step\n",
    "        \n",
    "        return np.array([\n",
    "            is_long,\n",
    "            is_short,\n",
    "            pnl_pct,\n",
    "            leveraged_pnl,\n",
    "            bars_in_position / 100,\n",
    "            self.leverage / self.max_leverage,\n",
    "            self.position_size / self.initial_balance,\n",
    "            min(bars_in_position / 48, 1)  # Time pressure\n",
    "        ])\n",
    "    \n",
    "    def _get_account_features(self):\n",
    "        \"\"\"Get account features\"\"\"\n",
    "        current_equity = self._calculate_equity()\n",
    "        total_return = (current_equity - self.initial_balance) / self.initial_balance\n",
    "        \n",
    "        if self.peak_equity > 0:\n",
    "            drawdown = (self.peak_equity - current_equity) / self.peak_equity\n",
    "        else:\n",
    "            drawdown = 0\n",
    "        \n",
    "        win_rate = self._calculate_win_rate()\n",
    "        \n",
    "        return np.array([\n",
    "            self.balance / self.initial_balance,\n",
    "            current_equity / self.initial_balance,\n",
    "            total_return,\n",
    "            drawdown,\n",
    "            win_rate,\n",
    "            min(self.winning_streak / 5, 1),\n",
    "            min(self.consecutive_losses / 5, 1),\n",
    "            len(self.trades) / 100\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd93b80",
   "metadata": {},
   "source": [
    "## 7. Enhanced Training System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80390696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enhanced_system(combined_data, episodes=500, save_models=True):\n",
    "    \"\"\"Train the enhanced multi-asset system\"\"\"\n",
    "    \n",
    "    # Validate data\n",
    "    print(\"Validating data...\")\n",
    "    if combined_data.isnull().any().any():\n",
    "        print(\"Warning: Found NaN values in data, cleaning...\")\n",
    "        combined_data = combined_data.ffill().bfill().fillna(0)\n",
    "    \n",
    "    # Check data range\n",
    "    print(f\"Data range: {combined_data['datetime'].min()} to {combined_data['datetime'].max()}\")\n",
    "    print(f\"Total samples: {len(combined_data)}\")\n",
    "    \n",
    "    # Split by time - ensure we have enough data\n",
    "    total_days = (combined_data['datetime'].max() - combined_data['datetime'].min()).days\n",
    "    \n",
    "    if total_days < 90:\n",
    "        print(f\"Warning: Only {total_days} days of data available. Need more data for proper training.\")\n",
    "        # Use 70/30 split for limited data\n",
    "        train_size = int(0.7 * len(combined_data))\n",
    "        train_data = combined_data[:train_size].copy()\n",
    "        test_data = combined_data[train_size:].copy()\n",
    "    else:\n",
    "        # Use date-based split for sufficient data\n",
    "        split_date = combined_data['datetime'].max() - pd.Timedelta(days=30)\n",
    "        train_data = combined_data[combined_data['datetime'] < split_date].copy()\n",
    "        test_data = combined_data[combined_data['datetime'] >= split_date].copy()\n",
    "    \n",
    "    # Ensure we have data\n",
    "    if len(train_data) < 1000:\n",
    "        print(\"Error: Not enough training data. Please download more historical data.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"Train data: {len(train_data)} samples ({train_data['datetime'].min()} to {train_data['datetime'].max()})\")\n",
    "    print(f\"Test data: {len(test_data)} samples ({test_data['datetime'].min() if len(test_data) > 0 else 'None'} to {test_data['datetime'].max() if len(test_data) > 0 else 'None'})\")\n",
    "    \n",
    "    # === Step 1: Train Enhanced XGBoost ===\n",
    "    print(\"\\n=== Training Enhanced XGBoost Direction Predictor ===\")\n",
    "    \n",
    "    xgb_predictor = EnhancedDirectionPredictor(lookback=24, prediction_horizon=4)\n",
    "    \n",
    "    # Prepare XGBoost data\n",
    "    X, y = xgb_predictor.prepare_data(train_data, feature_columns)\n",
    "    \n",
    "    if len(X) < 100:\n",
    "        print(\"Error: Not enough samples for XGBoost training\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Train-validation split\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(f\"XGBoost train samples: {len(X_train)}\")\n",
    "    print(f\"XGBoost validation samples: {len(X_val)}\")\n",
    "    \n",
    "    # Train ensemble with validation\n",
    "    xgb_predictor.train_ensemble(X_train, y_train, X_val, y_val, n_models=7)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_predictions = xgb_predictor.predict(X_val)\n",
    "    val_accuracy = np.mean((val_predictions != 0))  # Non-zero predictions\n",
    "    print(f\"XGBoost validation signal rate: {val_accuracy:.2%}\")\n",
    "    \n",
    "    # === Step 2: Initialize Enhanced PPO Agent ===\n",
    "    print(\"\\n=== Initializing Enhanced PPO Agent ===\")\n",
    "    \n",
    "    # Create environment\n",
    "    train_env = EnhancedCryptoFuturesEnv(train_data)\n",
    "    \n",
    "    # Get state dimensions\n",
    "    initial_state = train_env.reset()\n",
    "    state_dim = initial_state.shape[0]\n",
    "    action_dim = train_env.action_space\n",
    "    \n",
    "    print(f\"State dimension: {state_dim}\")\n",
    "    print(f\"Action dimension: {action_dim}\")\n",
    "    \n",
    "    # Initialize PPO agent\n",
    "    ppo_agent = EnhancedPPOAgent(state_dim, action_dim, lr=3e-4)\n",
    "    \n",
    "    # === Step 3: Train PPO with Curriculum Learning ===\n",
    "    print(\"\\n=== Training Enhanced PPO Agent ===\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_profits = []\n",
    "    episode_trades = []\n",
    "    episode_win_rates = []\n",
    "    episode_sharpe_ratios = []\n",
    "    \n",
    "    best_sharpe = -np.inf\n",
    "    best_profit = -np.inf\n",
    "    patience = 0\n",
    "    max_patience = 75\n",
    "    \n",
    "    # Curriculum learning parameters\n",
    "    explore_rate = 0.3  # Start with high exploration\n",
    "    explore_decay = 0.995\n",
    "    min_explore = 0.05\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = train_env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Prepare XGBoost features for the episode\n",
    "        if train_env.current_step >= xgb_predictor.lookback:\n",
    "            xgb_features = []\n",
    "            for i in range(len(train_data) - train_env.current_step):\n",
    "                if train_env.current_step + i >= xgb_predictor.lookback:\n",
    "                    feat = train_data[feature_columns].iloc[\n",
    "                        train_env.current_step + i - xgb_predictor.lookback:\n",
    "                        train_env.current_step + i\n",
    "                    ].values.flatten()\n",
    "                    xgb_features.append(feat)\n",
    "            \n",
    "            if xgb_features:\n",
    "                xgb_features = np.array(xgb_features)\n",
    "                xgb_signals = xgb_predictor.predict(xgb_features)\n",
    "            else:\n",
    "                xgb_signals = None\n",
    "        else:\n",
    "            xgb_signals = None\n",
    "        \n",
    "        # Episode loop\n",
    "        step_count = 0\n",
    "        done = False\n",
    "        episode_returns = []\n",
    "        \n",
    "        while not done:\n",
    "            # Get XGBoost signal for current step\n",
    "            if xgb_signals is not None and step_count < len(xgb_signals):\n",
    "                current_xgb_signal = xgb_signals[step_count]\n",
    "            else:\n",
    "                current_xgb_signal = None\n",
    "            \n",
    "            # Get valid actions\n",
    "            valid_actions = train_env.get_valid_actions()\n",
    "            \n",
    "            # Select action with exploration\n",
    "            action = ppo_agent.act(state, current_xgb_signal, valid_actions, explore_rate)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done, info = train_env.step(action)\n",
    "            \n",
    "            # Store reward and done\n",
    "            ppo_agent.store_reward(reward, done)\n",
    "            \n",
    "            # Track returns for Sharpe ratio\n",
    "            if len(train_env.equity_curve) > 1:\n",
    "                ret = (train_env.equity_curve[-1] - train_env.equity_curve[-2]) / train_env.equity_curve[-2]\n",
    "                episode_returns.append(ret)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update PPO\n",
    "        ppo_agent.update(epochs=10)\n",
    "        \n",
    "        # Calculate episode metrics\n",
    "        final_equity = info['equity']\n",
    "        episode_profit = (final_equity / train_env.initial_balance - 1) * 100\n",
    "        \n",
    "        # Calculate Sharpe ratio\n",
    "        if len(episode_returns) > 1:\n",
    "            returns_std = np.std(episode_returns)\n",
    "            if returns_std > 0:\n",
    "                sharpe = np.sqrt(252 * 24) * np.mean(episode_returns) / returns_std  # Hourly to annual\n",
    "            else:\n",
    "                sharpe = 0\n",
    "        else:\n",
    "            sharpe = 0\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_profits.append(episode_profit)\n",
    "        episode_trades.append(info['trades'])\n",
    "        episode_win_rates.append(info['win_rate'])\n",
    "        episode_sharpe_ratios.append(sharpe)\n",
    "        \n",
    "        # Update exploration rate\n",
    "        explore_rate = max(min_explore, explore_rate * explore_decay)\n",
    "        \n",
    "        # Early stopping based on Sharpe ratio\n",
    "        if sharpe > best_sharpe:\n",
    "            best_sharpe = sharpe\n",
    "            patience = 0\n",
    "            \n",
    "            # Save best model\n",
    "            if save_models and episode > 50:\n",
    "                torch.save({\n",
    "                    'ppo_state_dict': ppo_agent.network.state_dict(),\n",
    "                    'episode': episode,\n",
    "                    'sharpe': best_sharpe,\n",
    "                    'profit': episode_profit\n",
    "                }, 'best_ppo_model_enhanced.pth')\n",
    "        else:\n",
    "            patience += 1\n",
    "        \n",
    "        if episode_profit > best_profit:\n",
    "            best_profit = episode_profit\n",
    "        \n",
    "        # Adaptive training phases\n",
    "        if episode < 100:\n",
    "            phase = \"Phase 1: Exploration\"\n",
    "        elif episode < 300:\n",
    "            phase = \"Phase 2: Exploitation\"\n",
    "        else:\n",
    "            phase = \"Phase 3: Refinement\"\n",
    "        \n",
    "        # Print progress\n",
    "        if episode % 10 == 0:\n",
    "            avg_profit = np.mean(episode_profits[-10:]) if len(episode_profits) >= 10 else episode_profit\n",
    "            avg_trades = np.mean(episode_trades[-10:]) if len(episode_trades) >= 10 else info['trades']\n",
    "            avg_win_rate = np.mean(episode_win_rates[-10:]) if len(episode_win_rates) >= 10 else info['win_rate']\n",
    "            avg_sharpe = np.mean(episode_sharpe_ratios[-10:]) if len(episode_sharpe_ratios) >= 10 else sharpe\n",
    "            \n",
    "            print(f\"Episode {episode} ({phase}) - Explore Rate: {explore_rate:.3f}\")\n",
    "            print(f\"  Profit: {episode_profit:.2f}% | Avg: {avg_profit:.2f}% | Best: {best_profit:.2f}%\")\n",
    "            print(f\"  Sharpe: {sharpe:.2f} | Avg: {avg_sharpe:.2f} | Best: {best_sharpe:.2f}\")\n",
    "            print(f\"  Trades: {info['trades']} | Win Rate: {info['win_rate']:.2%}\")\n",
    "        \n",
    "        # Check early stopping\n",
    "        if patience >= max_patience and episode > 200:\n",
    "            print(f\"\\nEarly stopping at episode {episode}\")\n",
    "            break\n",
    "    \n",
    "    # === Training Complete ===\n",
    "    print(\"\\n=== Training Complete ===\")\n",
    "    print(f\"Best Sharpe Ratio: {best_sharpe:.2f}\")\n",
    "    print(f\"Best Profit: {best_profit:.2f}%\")\n",
    "    print(f\"Final average profit (last 50): {np.mean(episode_profits[-50:]):.2f}%\")\n",
    "    print(f\"Final average Sharpe (last 50): {np.mean(episode_sharpe_ratios[-50:]):.2f}\")\n",
    "    print(f\"Final average win rate (last 50): {np.mean(episode_win_rates[-50:]):.2%}\")\n",
    "    \n",
    "    # Plot training progress\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Profits\n",
    "    ax1.plot(episode_profits)\n",
    "    ax1.axhline(y=0, color='r', linestyle='--')\n",
    "    ax1.set_title('Episode Profits (%)')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Profit %')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Sharpe Ratios\n",
    "    ax2.plot(episode_sharpe_ratios)\n",
    "    ax2.axhline(y=0, color='r', linestyle='--')\n",
    "    ax2.axhline(y=1, color='g', linestyle='--', alpha=0.5)\n",
    "    ax2.set_title('Episode Sharpe Ratios')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Sharpe Ratio')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Win rates\n",
    "    ax3.plot(episode_win_rates)\n",
    "    ax3.axhline(y=0.5, color='r', linestyle='--')\n",
    "    ax3.set_title('Win Rates')\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Win Rate')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    # Moving averages\n",
    "    window = 50\n",
    "    if len(episode_profits) >= window:\n",
    "        ma_profits = pd.Series(episode_profits).rolling(window).mean()\n",
    "        ma_sharpe = pd.Series(episode_sharpe_ratios).rolling(window).mean()\n",
    "        \n",
    "        ax4.plot(ma_profits, label='MA Profit %', color='green')\n",
    "        ax4.plot(ma_sharpe * 10, label='MA Sharpe x10', color='blue')\n",
    "        ax4.set_title(f'{window}-Episode Moving Averages')\n",
    "        ax4.set_xlabel('Episode')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ppo_agent, xgb_predictor, {\n",
    "        'episode_profits': episode_profits,\n",
    "        'episode_trades': episode_trades,\n",
    "        'episode_win_rates': episode_win_rates,\n",
    "        'episode_sharpe_ratios': episode_sharpe_ratios,\n",
    "        'best_profit': best_profit,\n",
    "        'best_sharpe': best_sharpe\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdac9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the enhanced system\n",
    "print(\"Starting enhanced multi-asset training...\")\n",
    "ppo_agent, xgb_predictor, training_results = train_enhanced_system(combined_data, episodes=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad8742",
   "metadata": {},
   "source": [
    "## 8. Backtesting on Individual Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b460eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_individual_asset(ppo_agent, xgb_predictor, asset_data, asset_name, plot_results=True):\n",
    "    \"\"\"Backtest on individual asset - FIXED for data handling\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== Backtesting on {asset_name} ===\")\n",
    "    \n",
    "    # Use only recent data for testing\n",
    "    test_data = asset_data[asset_data['datetime'] >= '2023-01-01'].copy()\n",
    "    \n",
    "    # Check if we have test data\n",
    "    if len(test_data) < 100:\n",
    "        print(f\"Warning: Only {len(test_data)} samples for {asset_name} testing. Need more recent data.\")\n",
    "        # Use last 30% of available data instead\n",
    "        test_size = int(0.3 * len(asset_data))\n",
    "        test_data = asset_data[-test_size:].copy()\n",
    "    \n",
    "    if len(test_data) < 50:\n",
    "        print(f\"Error: Not enough data for backtesting {asset_name}\")\n",
    "        return {\n",
    "            'asset': asset_name,\n",
    "            'total_return': 0,\n",
    "            'sharpe_ratio': 0,\n",
    "            'max_drawdown': 0,\n",
    "            'total_trades': 0,\n",
    "            'win_rate': 0,\n",
    "            'final_equity': 10000,\n",
    "            'equity_curve': [10000]\n",
    "        }\n",
    "    \n",
    "    print(f\"Test data range: {test_data['datetime'].min()} to {test_data['datetime'].max()}\")\n",
    "    print(f\"Test samples: {len(test_data)}\")\n",
    "    \n",
    "    # Create test environment\n",
    "    test_env = EnhancedCryptoFuturesEnv(test_data)\n",
    "    state = test_env.reset()\n",
    "    \n",
    "    # Prepare XGBoost features\n",
    "    xgb_features = []\n",
    "    for i in range(len(test_data)):\n",
    "        if i >= xgb_predictor.lookback:\n",
    "            feat = test_data[feature_columns].iloc[\n",
    "                i - xgb_predictor.lookback:i\n",
    "            ].values.flatten()\n",
    "            xgb_features.append(feat)\n",
    "    \n",
    "    xgb_features = np.array(xgb_features) if xgb_features else None\n",
    "    xgb_signals = xgb_predictor.predict(xgb_features) if xgb_features is not None else None\n",
    "    \n",
    "    # Trading loop\n",
    "    actions = []\n",
    "    prices = []\n",
    "    positions = []\n",
    "    equities = []\n",
    "    \n",
    "    # Detailed logging\n",
    "    action_logs = []\n",
    "    \n",
    "    step_count = 0\n",
    "    while True:\n",
    "        # Get XGBoost signal\n",
    "        if xgb_signals is not None and step_count < len(xgb_signals):\n",
    "            current_xgb_signal = xgb_signals[step_count]\n",
    "        else:\n",
    "            current_xgb_signal = None\n",
    "        \n",
    "        # Get valid actions\n",
    "        valid_actions = test_env.get_valid_actions()\n",
    "        \n",
    "        # Get action from PPO\n",
    "        with torch.no_grad():\n",
    "            action = ppo_agent.act(state, current_xgb_signal, valid_actions, explore_rate=0)\n",
    "        \n",
    "        # Execute action\n",
    "        next_state, reward, done, info = test_env.step(action)\n",
    "        \n",
    "        # Record data\n",
    "        actions.append(action)\n",
    "        prices.append(test_data['close'].iloc[test_env.current_step - 1])\n",
    "        positions.append(1 if test_env.position == 'long' else \n",
    "                        (-1 if test_env.position == 'short' else 0))\n",
    "        equities.append(info['equity'])\n",
    "        \n",
    "        # Print first few trades\n",
    "        if len(test_env.trades) > 0 and len(test_env.trades) <= 5:\n",
    "            trade = test_env.trades[-1]\n",
    "            print(f\"[Trade #{len(test_env.trades)}] {trade['position'].upper()} \"\n",
    "                  f\"Entry: ${trade['entry_price']:.2f} Exit: ${trade['exit_price']:.2f} \"\n",
    "                  f\"P&L: ${trade['pnl']:.2f} ({trade['pnl_pct']*100:.2f}%) \"\n",
    "                  f\"Leverage: {trade['leverage']}x Reason: {trade['exit_reason']}\")\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        step_count += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Calculate metrics\n",
    "    final_equity = equities[-1]\n",
    "    total_return = (final_equity / test_env.initial_balance - 1) * 100\n",
    "    \n",
    "    # Calculate Sharpe ratio\n",
    "    if len(equities) > 1:\n",
    "        equity_returns = pd.Series(equities).pct_change().dropna()\n",
    "        if len(equity_returns) > 0 and equity_returns.std() > 0:\n",
    "            sharpe_ratio = np.sqrt(252 * 24) * equity_returns.mean() / equity_returns.std()\n",
    "        else:\n",
    "            sharpe_ratio = 0\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "    \n",
    "    # Calculate max drawdown\n",
    "    if len(equities) > 0:\n",
    "        equity_series = pd.Series(equities)\n",
    "        running_max = equity_series.expanding().max()\n",
    "        drawdown = (equity_series - running_max) / running_max\n",
    "        max_drawdown = drawdown.min() * 100\n",
    "    else:\n",
    "        max_drawdown = 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n=== {asset_name} Results ===\")\n",
    "    print(f\"Total Return: {total_return:.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "    print(f\"Max Drawdown: {max_drawdown:.2f}%\")\n",
    "    print(f\"Total Trades: {len(test_env.trades)}\")\n",
    "    print(f\"Win Rate: {test_env._calculate_win_rate():.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'asset': asset_name,\n",
    "        'total_return': total_return,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'total_trades': len(test_env.trades),\n",
    "        'win_rate': test_env._calculate_win_rate(),\n",
    "        'final_equity': final_equity,\n",
    "        'equity_curve': equities\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest on each asset\n",
    "results = {}\n",
    "for symbol, data in multi_asset_data.items():\n",
    "    asset_name = symbol.split('/')[0]\n",
    "    results[asset_name] = backtest_individual_asset(\n",
    "        ppo_agent, xgb_predictor, data, asset_name, plot_results=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n=== PORTFOLIO SUMMARY ===\")\n",
    "print(f\"{'Asset':<10} {'Return %':<12} {'Sharpe':<10} {'Max DD %':<12} {'Win Rate':<10} {'Trades':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "total_return = 0\n",
    "for asset, res in results.items():\n",
    "    print(f\"{asset:<10} {res['total_return']:>10.2f}% {res['sharpe_ratio']:>8.2f} \"\n",
    "          f\"{res['max_drawdown']:>10.2f}% {res['win_rate']:>8.2%} {res['total_trades']:>8}\")\n",
    "    total_return += res['total_return']\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'AVERAGE':<10} {total_return/len(results):>10.2f}%\")\n",
    "\n",
    "print(\"\\nTraining completed successfully! The model is now ready for live trading simulation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
